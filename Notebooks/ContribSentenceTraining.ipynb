{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMQE0y5JdlF"
      },
      "source": [
        "# **Problem Statement**\n",
        "Given a research paper in PDF (use this paper: https://aclanthology.org/P19-1106/), how would\n",
        "you find the “contributing statements” of the paper? For definition of contributing statement\n",
        "and the associated task please refer to here: https://ncg-task.github.io\n",
        "1. Train a model to extract the contributing statements from a paper. Combine the\n",
        "contributing statements smartly to form a paper summary.\n",
        "Read about contributing statements from research papers here:\n",
        "https://ceur-ws.org/Vol-2658/paper2.pdf\n",
        "The dataset for training/fine-tuning and testing your model is here:\n",
        "https://zenodo.org/record/4737071#.Y7UiQy0RpQI\n",
        "2. Evaluate your summary against the abstract of the paper (use this paper:\n",
        "https://aclanthology.org/P19-1106/) using ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, BARTScore (taking the abstract of the paper as the reference summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGr7PbUUJxe6"
      },
      "source": [
        "#**Dataset**\n",
        "\n",
        "NLPContributionGraph was introduced as Task 11 at SemEval 2021 for the first time. The task is defined on a dataset of Natural Language Processing (NLP) scholarly articles with their contributions structured to be integrable within Knowledge Graph infrastructures such as the Open Research Knowledge Graph. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxGhExzZKUq1"
      },
      "source": [
        "# **Proposed Solution**\n",
        "\n",
        "In the Training Dataset for every Research paper, The Raw Text has been extracted from the PDF using [Grobid](https://github.com/kermitt2/grobid) and passed to [Stanza](https://github.com/stanfordnlp/stanza) which provides formatted text in the text file format and contribution sentences from the paper has been annoted and stored as a seperate text file\n",
        "\n",
        "\n",
        "Our Task is to build model to classify the contribution sentences from the paper and generate a summary using the contribution sentences\n",
        "\n",
        "\n",
        "**Required Dataset**\n",
        "\n",
        "├── [articlename].pdf                      # scholarly article pdf\n",
        "        │   \n",
        "├── [articlename]-Grobid-out.txt           # plaintext output from the [Grobid parser](https://github.com/kermitt2/grobid)\n",
        "        │   \n",
        "├── [articlename]-Stanza-out.txt           # plaintext preprocessed output from [Stanza](https://github.com/stanfordnlp/stanza)\n",
        "        │   \n",
        "├── sentences.txt                          # annotated Contribution sentences in the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z84AuXQNyOg"
      },
      "source": [
        "### **Download and unzip the Training dataset from [SemEval-2021 Task 11: NLPContributionGraph](https://zenodo.org/record/4737071#.ZALbCtJBw3F)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko-VDGNpo0Uw",
        "outputId": "0f59f16e-2608-4951-df1f-224622db492a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-04 09:21:31--  https://zenodo.org/record/4737071/files/training-set.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 159144523 (152M) [application/octet-stream]\n",
            "Saving to: ‘training-set.zip’\n",
            "\n",
            "training-set.zip    100%[===================>] 151.77M  8.26MB/s    in 1m 41s  \n",
            "\n",
            "2023-03-04 09:23:14 (1.50 MB/s) - ‘training-set.zip’ saved [159144523/159144523]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/record/4737071/files/training-set.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBN5kcSSr9Rq",
        "outputId": "59ce555d-ca46-4b2e-e95f-c827b44d4391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  training-set.zip\n",
            "   creating: training-set/\n",
            " extracting: training-set/desktop.ini  \n",
            "   creating: training-set/natural_language_inference/\n",
            "   creating: training-set/natural_language_inference/0/\n",
            "  inflating: training-set/natural_language_inference/0/1606.01549v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/0/1606.01549v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/0/1606.01549v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/0/entities.txt  \n",
            "   creating: training-set/natural_language_inference/0/info-units/\n",
            "  inflating: training-set/natural_language_inference/0/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/0/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/0/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/0/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/0/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/0/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/0/triples/\n",
            "  inflating: training-set/natural_language_inference/0/triples/ablation-analysis.txt  \n",
            " extracting: training-set/natural_language_inference/0/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/0/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/0/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/0/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/1/\n",
            "   creating: training-set/natural_language_inference/10/\n",
            "   creating: training-set/natural_language_inference/100/\n",
            "  inflating: training-set/natural_language_inference/100/1801.01641v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/100/1801.01641v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/100/1801.01641v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/100/entities.txt  \n",
            "   creating: training-set/natural_language_inference/100/info-units/\n",
            "  inflating: training-set/natural_language_inference/100/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/100/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/100/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/100/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/100/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/100/triples/\n",
            "  inflating: training-set/natural_language_inference/100/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/100/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/100/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/100/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/10/1707.02786v4-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/10/1707.02786v4-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/10/1707.02786v4.pdf  \n",
            "  inflating: training-set/natural_language_inference/10/entities.txt  \n",
            "   creating: training-set/natural_language_inference/10/info-units/\n",
            "  inflating: training-set/natural_language_inference/10/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/10/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/10/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/10/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/10/triples/\n",
            "  inflating: training-set/natural_language_inference/10/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/10/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/10/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/11/\n",
            "  inflating: training-set/natural_language_inference/11/1706.02596v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/11/1706.02596v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/11/1706.02596v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/11/entities.txt  \n",
            "   creating: training-set/natural_language_inference/11/info-units/\n",
            "  inflating: training-set/natural_language_inference/11/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/11/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/11/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/11/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/11/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/11/triples/\n",
            "  inflating: training-set/natural_language_inference/11/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/11/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/11/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/11/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/12/\n",
            "  inflating: training-set/natural_language_inference/12/1708.02312v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/12/1708.02312v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/12/1708.02312v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/12/entities.txt  \n",
            "   creating: training-set/natural_language_inference/12/info-units/\n",
            "  inflating: training-set/natural_language_inference/12/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/12/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/12/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/12/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/12/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/12/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/12/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/12/triples/\n",
            "  inflating: training-set/natural_language_inference/12/triples/ablation-analysis.txt  \n",
            " extracting: training-set/natural_language_inference/12/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/12/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/12/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/12/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/12/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/13/\n",
            "  inflating: training-set/natural_language_inference/13/1803.09074v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/13/1803.09074v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/13/1803.09074v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/13/entities.txt  \n",
            "   creating: training-set/natural_language_inference/13/info-units/\n",
            "  inflating: training-set/natural_language_inference/13/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/13/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/13/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/13/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/13/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/13/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/13/triples/\n",
            "  inflating: training-set/natural_language_inference/13/triples/baselines.txt  \n",
            "  inflating: training-set/natural_language_inference/13/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/13/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/13/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/13/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/14/\n",
            "  inflating: training-set/natural_language_inference/14/1904.04365v4-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/14/1904.04365v4-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/14/1904.04365v4.pdf  \n",
            "  inflating: training-set/natural_language_inference/14/entities.txt  \n",
            "   creating: training-set/natural_language_inference/14/info-units/\n",
            "  inflating: training-set/natural_language_inference/14/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/14/info-units/dataset.json  \n",
            "  inflating: training-set/natural_language_inference/14/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/14/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/14/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/14/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/14/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/14/triples/\n",
            " extracting: training-set/natural_language_inference/14/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/14/triples/dataset.txt  \n",
            "  inflating: training-set/natural_language_inference/14/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/14/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/14/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/14/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/15/\n",
            "  inflating: training-set/natural_language_inference/15/1805.11360v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/15/1805.11360v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/15/1805.11360v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/15/entities.txt  \n",
            "   creating: training-set/natural_language_inference/15/info-units/\n",
            "  inflating: training-set/natural_language_inference/15/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/15/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/15/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/15/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/15/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/15/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/15/triples/\n",
            "  inflating: training-set/natural_language_inference/15/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/15/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/15/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/15/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/15/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/16/\n",
            "  inflating: training-set/natural_language_inference/16/1702.03814v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/16/1702.03814v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/16/1702.03814v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/16/entities.txt  \n",
            "   creating: training-set/natural_language_inference/16/info-units/\n",
            "  inflating: training-set/natural_language_inference/16/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/16/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/16/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/16/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/16/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/16/triples/\n",
            "  inflating: training-set/natural_language_inference/16/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/16/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/16/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/16/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/17/\n",
            "  inflating: training-set/natural_language_inference/17/1705.02798v6-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/17/1705.02798v6-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/17/1705.02798v6.pdf  \n",
            "  inflating: training-set/natural_language_inference/17/entities.txt  \n",
            "   creating: training-set/natural_language_inference/17/info-units/\n",
            "  inflating: training-set/natural_language_inference/17/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/17/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/17/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/17/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/17/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/17/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/17/triples/\n",
            "  inflating: training-set/natural_language_inference/17/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/17/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/17/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/17/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/17/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/18/\n",
            "  inflating: training-set/natural_language_inference/18/1511.06038v4-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/18/1511.06038v4-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/18/1511.06038v4.pdf  \n",
            "  inflating: training-set/natural_language_inference/18/entities.txt  \n",
            "   creating: training-set/natural_language_inference/18/info-units/\n",
            "  inflating: training-set/natural_language_inference/18/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/18/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/18/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/18/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/18/triples/\n",
            "  inflating: training-set/natural_language_inference/18/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/18/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/18/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/19/\n",
            "  inflating: training-set/natural_language_inference/19/1712.02047v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/19/1712.02047v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/19/1712.02047v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/19/entities.txt  \n",
            "   creating: training-set/natural_language_inference/19/info-units/\n",
            "  inflating: training-set/natural_language_inference/19/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/19/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/19/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/19/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/19/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/19/triples/\n",
            "  inflating: training-set/natural_language_inference/19/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/19/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/19/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/19/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/1/1506.02075v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/1/1506.02075v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/1/1506.02075v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/1/entities.txt  \n",
            "   creating: training-set/natural_language_inference/1/info-units/\n",
            "  inflating: training-set/natural_language_inference/1/info-units/dataset.json  \n",
            "  inflating: training-set/natural_language_inference/1/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/1/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/1/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/1/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/1/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/1/triples/\n",
            "  inflating: training-set/natural_language_inference/1/triples/dataset.txt  \n",
            "  inflating: training-set/natural_language_inference/1/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/1/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/1/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/1/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/2/\n",
            "   creating: training-set/natural_language_inference/20/\n",
            "  inflating: training-set/natural_language_inference/20/1808.08762v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/20/1808.08762v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/20/1808.08762v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/20/entities.txt  \n",
            "   creating: training-set/natural_language_inference/20/info-units/\n",
            "  inflating: training-set/natural_language_inference/20/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/20/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/20/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/20/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/20/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/20/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/20/triples/\n",
            " extracting: training-set/natural_language_inference/20/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/20/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/20/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/20/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/20/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/21/\n",
            "  inflating: training-set/natural_language_inference/21/1709.04696v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/21/1709.04696v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/21/1709.04696v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/21/entities.txt  \n",
            "   creating: training-set/natural_language_inference/21/info-units/\n",
            "  inflating: training-set/natural_language_inference/21/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/21/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/21/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/21/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/21/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/21/triples/\n",
            "  inflating: training-set/natural_language_inference/21/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/21/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/21/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/21/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/22/\n",
            "  inflating: training-set/natural_language_inference/22/1704.07415v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/22/1704.07415v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/22/1704.07415v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/22/entities.txt  \n",
            "   creating: training-set/natural_language_inference/22/info-units/\n",
            "  inflating: training-set/natural_language_inference/22/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/22/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/22/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/22/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/22/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/22/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/22/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/22/triples/\n",
            "  inflating: training-set/natural_language_inference/22/triples/ablation-analysis.txt  \n",
            " extracting: training-set/natural_language_inference/22/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/22/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/22/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/22/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/22/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/23/\n",
            "  inflating: training-set/natural_language_inference/23/1711.07341v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/23/1711.07341v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/23/1711.07341v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/23/entities.txt  \n",
            "   creating: training-set/natural_language_inference/23/info-units/\n",
            "  inflating: training-set/natural_language_inference/23/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/23/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/23/info-units/results.json  \n",
            " extracting: training-set/natural_language_inference/23/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/23/triples/\n",
            "  inflating: training-set/natural_language_inference/23/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/23/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/23/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/24/\n",
            "  inflating: training-set/natural_language_inference/24/1712.03556v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/24/1712.03556v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/24/1712.03556v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/24/entities.txt  \n",
            "   creating: training-set/natural_language_inference/24/info-units/\n",
            "  inflating: training-set/natural_language_inference/24/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/24/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/24/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/24/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/24/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/24/triples/\n",
            "  inflating: training-set/natural_language_inference/24/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/24/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/24/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/24/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/25/\n",
            "  inflating: training-set/natural_language_inference/25/1712.03609v4-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/25/1712.03609v4-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/25/1712.03609v4.pdf  \n",
            "  inflating: training-set/natural_language_inference/25/entities.txt  \n",
            "   creating: training-set/natural_language_inference/25/info-units/\n",
            "  inflating: training-set/natural_language_inference/25/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/25/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/25/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/25/info-units/results.json  \n",
            " extracting: training-set/natural_language_inference/25/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/25/triples/\n",
            "  inflating: training-set/natural_language_inference/25/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/25/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/25/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/25/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/26/\n",
            "  inflating: training-set/natural_language_inference/26/1909.02209v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/26/1909.02209v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/26/1909.02209v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/26/entities.txt  \n",
            "   creating: training-set/natural_language_inference/26/info-units/\n",
            "  inflating: training-set/natural_language_inference/26/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/26/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/26/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/26/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/26/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/26/triples/\n",
            "  inflating: training-set/natural_language_inference/26/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/26/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/26/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/26/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/27/\n",
            "  inflating: training-set/natural_language_inference/27/1804.09541v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/27/1804.09541v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/27/1804.09541v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/27/entities.txt  \n",
            "   creating: training-set/natural_language_inference/27/info-units/\n",
            "  inflating: training-set/natural_language_inference/27/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/27/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/27/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/27/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/27/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/27/triples/\n",
            "  inflating: training-set/natural_language_inference/27/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/27/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/27/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/27/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/28/\n",
            "  inflating: training-set/natural_language_inference/28/1710.09537v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/28/1710.09537v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/28/1710.09537v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/28/entities.txt  \n",
            "   creating: training-set/natural_language_inference/28/info-units/\n",
            "  inflating: training-set/natural_language_inference/28/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/28/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/28/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/28/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/28/triples/\n",
            "  inflating: training-set/natural_language_inference/28/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/28/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/28/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/29/\n",
            "  inflating: training-set/natural_language_inference/29/1901.07696v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/29/1901.07696v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/29/1901.07696v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/29/entities.txt  \n",
            "   creating: training-set/natural_language_inference/29/info-units/\n",
            "  inflating: training-set/natural_language_inference/29/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/29/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/29/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/29/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/29/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/29/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/29/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/29/triples/\n",
            "  inflating: training-set/natural_language_inference/29/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/29/triples/baselines.txt  \n",
            "  inflating: training-set/natural_language_inference/29/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/29/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/29/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/29/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/2/1801.08290v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/2/1801.08290v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/2/1801.08290v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/2/entities.txt  \n",
            "   creating: training-set/natural_language_inference/2/info-units/\n",
            "  inflating: training-set/natural_language_inference/2/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/2/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/2/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/2/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/2/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/2/triples/\n",
            "  inflating: training-set/natural_language_inference/2/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/2/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/2/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/2/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/3/\n",
            "   creating: training-set/natural_language_inference/30/\n",
            "  inflating: training-set/natural_language_inference/30/1404.4326v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/30/1404.4326v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/30/1404.4326v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/30/entities.txt  \n",
            "   creating: training-set/natural_language_inference/30/info-units/\n",
            "  inflating: training-set/natural_language_inference/30/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/30/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/30/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/30/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/30/triples/\n",
            "  inflating: training-set/natural_language_inference/30/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/30/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/30/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/31/\n",
            "  inflating: training-set/natural_language_inference/31/entities.txt  \n",
            "   creating: training-set/natural_language_inference/31/info-units/\n",
            "  inflating: training-set/natural_language_inference/31/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/31/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/31/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/31/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/31/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/31/P19-1465-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/31/P19-1465-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/31/P19-1465.pdf  \n",
            "  inflating: training-set/natural_language_inference/31/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/31/triples/\n",
            "  inflating: training-set/natural_language_inference/31/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/31/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/31/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/31/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/31/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/32/\n",
            "  inflating: training-set/natural_language_inference/32/1810.06683v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/32/1810.06683v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/32/1810.06683v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/32/entities.txt  \n",
            "   creating: training-set/natural_language_inference/32/info-units/\n",
            "  inflating: training-set/natural_language_inference/32/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/32/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/32/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/32/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/32/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/32/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/32/triples/\n",
            "  inflating: training-set/natural_language_inference/32/triples/baselines.txt  \n",
            " extracting: training-set/natural_language_inference/32/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/32/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/32/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/32/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/33/\n",
            "  inflating: training-set/natural_language_inference/33/1804.00079v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/33/1804.00079v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/33/1804.00079v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/33/entities.txt  \n",
            "   creating: training-set/natural_language_inference/33/info-units/\n",
            "  inflating: training-set/natural_language_inference/33/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/33/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/33/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/33/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/33/triples/\n",
            "  inflating: training-set/natural_language_inference/33/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/33/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/33/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/34/\n",
            "  inflating: training-set/natural_language_inference/34/1905.06933v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/34/1905.06933v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/34/1905.06933v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/34/entities.txt  \n",
            "   creating: training-set/natural_language_inference/34/info-units/\n",
            "  inflating: training-set/natural_language_inference/34/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/34/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/34/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/34/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/34/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/34/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/34/triples/\n",
            "  inflating: training-set/natural_language_inference/34/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/34/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/34/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/34/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/34/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/35/\n",
            "  inflating: training-set/natural_language_inference/35/1901.02262v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/35/1901.02262v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/35/1901.02262v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/35/entities.txt  \n",
            "   creating: training-set/natural_language_inference/35/info-units/\n",
            "  inflating: training-set/natural_language_inference/35/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/35/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/35/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/35/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/35/triples/\n",
            "  inflating: training-set/natural_language_inference/35/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/35/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/35/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/36/\n",
            "  inflating: training-set/natural_language_inference/36/1809.02794v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/36/1809.02794v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/36/1809.02794v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/36/entities.txt  \n",
            "   creating: training-set/natural_language_inference/36/info-units/\n",
            "  inflating: training-set/natural_language_inference/36/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/36/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/36/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/36/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/36/triples/\n",
            "  inflating: training-set/natural_language_inference/36/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/36/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/36/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/37/\n",
            "  inflating: training-set/natural_language_inference/37/1710.10723v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/37/1710.10723v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/37/1710.10723v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/37/entities.txt  \n",
            "   creating: training-set/natural_language_inference/37/info-units/\n",
            "  inflating: training-set/natural_language_inference/37/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/37/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/37/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/37/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/37/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/37/triples/\n",
            "  inflating: training-set/natural_language_inference/37/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/37/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/37/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/37/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/38/\n",
            "  inflating: training-set/natural_language_inference/38/1707.09098v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/38/1707.09098v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/38/1707.09098v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/38/entities.txt  \n",
            "   creating: training-set/natural_language_inference/38/info-units/\n",
            "  inflating: training-set/natural_language_inference/38/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/38/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/38/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/38/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/38/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/38/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/38/triples/\n",
            "  inflating: training-set/natural_language_inference/38/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/38/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/38/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/38/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/38/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/39/\n",
            "  inflating: training-set/natural_language_inference/39/entities.txt  \n",
            "   creating: training-set/natural_language_inference/39/info-units/\n",
            "  inflating: training-set/natural_language_inference/39/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/39/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/39/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/39/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/39/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/39/N16-1108-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/39/N16-1108-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/39/N16-1108.pdf  \n",
            "  inflating: training-set/natural_language_inference/39/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/39/triples/\n",
            "  inflating: training-set/natural_language_inference/39/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/39/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/39/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/39/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/39/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/3/1605.05573v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/3/1605.05573v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/3/1605.05573v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/3/entities.txt  \n",
            "   creating: training-set/natural_language_inference/3/info-units/\n",
            "  inflating: training-set/natural_language_inference/3/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/3/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/3/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/3/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/3/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/3/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/3/triples/\n",
            "  inflating: training-set/natural_language_inference/3/triples/baselines.txt  \n",
            "  inflating: training-set/natural_language_inference/3/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/3/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/3/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/3/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/4/\n",
            "   creating: training-set/natural_language_inference/40/\n",
            "  inflating: training-set/natural_language_inference/40/1703.02620v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/40/1703.02620v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/40/1703.02620v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/40/entities.txt  \n",
            "   creating: training-set/natural_language_inference/40/info-units/\n",
            "  inflating: training-set/natural_language_inference/40/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/40/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/40/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/40/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/40/triples/\n",
            "  inflating: training-set/natural_language_inference/40/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/40/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/40/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/41/\n",
            "  inflating: training-set/natural_language_inference/41/1910.13461v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/41/1910.13461v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/41/1910.13461v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/41/entities.txt  \n",
            "   creating: training-set/natural_language_inference/41/info-units/\n",
            "  inflating: training-set/natural_language_inference/41/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/41/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/41/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/41/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/41/triples/\n",
            "  inflating: training-set/natural_language_inference/41/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/41/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/41/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/42/\n",
            "  inflating: training-set/natural_language_inference/42/1810.09580v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/42/1810.09580v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/42/1810.09580v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/42/entities.txt  \n",
            "   creating: training-set/natural_language_inference/42/info-units/\n",
            "  inflating: training-set/natural_language_inference/42/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/42/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/42/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/42/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/42/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/42/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/42/triples/\n",
            "  inflating: training-set/natural_language_inference/42/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/42/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/42/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/42/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/42/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/43/\n",
            "  inflating: training-set/natural_language_inference/43/1707.04412v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/43/1707.04412v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/43/1707.04412v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/43/entities.txt  \n",
            "   creating: training-set/natural_language_inference/43/info-units/\n",
            "  inflating: training-set/natural_language_inference/43/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/43/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/43/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/43/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/43/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/43/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/43/triples/\n",
            "  inflating: training-set/natural_language_inference/43/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/43/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/43/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/43/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/43/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/44/\n",
            "  inflating: training-set/natural_language_inference/44/1805.08092v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/44/1805.08092v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/44/1805.08092v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/44/entities.txt  \n",
            "   creating: training-set/natural_language_inference/44/info-units/\n",
            "  inflating: training-set/natural_language_inference/44/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/44/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/44/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/44/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/44/triples/\n",
            "  inflating: training-set/natural_language_inference/44/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/44/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/44/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/45/\n",
            "  inflating: training-set/natural_language_inference/45/1611.01724v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/45/1611.01724v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/45/1611.01724v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/45/entities.txt  \n",
            "   creating: training-set/natural_language_inference/45/info-units/\n",
            "  inflating: training-set/natural_language_inference/45/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/45/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/45/info-units/research-problem.json  \n",
            " extracting: training-set/natural_language_inference/45/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/45/triples/\n",
            " extracting: training-set/natural_language_inference/45/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/45/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/45/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/46/\n",
            "  inflating: training-set/natural_language_inference/46/1712.07040v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/46/1712.07040v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/46/1712.07040v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/46/entities.txt  \n",
            "   creating: training-set/natural_language_inference/46/info-units/\n",
            "  inflating: training-set/natural_language_inference/46/info-units/dataset.json  \n",
            "  inflating: training-set/natural_language_inference/46/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/46/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/46/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/46/triples/\n",
            "  inflating: training-set/natural_language_inference/46/triples/dataset.txt  \n",
            "  inflating: training-set/natural_language_inference/46/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/46/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/47/\n",
            "  inflating: training-set/natural_language_inference/47/1508.05326v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/47/1508.05326v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/47/1508.05326v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/47/entities.txt  \n",
            "   creating: training-set/natural_language_inference/47/info-units/\n",
            "  inflating: training-set/natural_language_inference/47/info-units/dataset.json  \n",
            "  inflating: training-set/natural_language_inference/47/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/47/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/47/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/47/triples/\n",
            "  inflating: training-set/natural_language_inference/47/triples/dataset.txt  \n",
            "  inflating: training-set/natural_language_inference/47/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/47/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/48/\n",
            "  inflating: training-set/natural_language_inference/48/1606.02245v4-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/48/1606.02245v4-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/48/1606.02245v4.pdf  \n",
            "  inflating: training-set/natural_language_inference/48/entities.txt  \n",
            "   creating: training-set/natural_language_inference/48/info-units/\n",
            "  inflating: training-set/natural_language_inference/48/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/48/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/48/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/48/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/48/triples/\n",
            "  inflating: training-set/natural_language_inference/48/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/48/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/48/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/49/\n",
            "  inflating: training-set/natural_language_inference/49/1709.04348v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/49/1709.04348v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/49/1709.04348v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/49/entities.txt  \n",
            "   creating: training-set/natural_language_inference/49/info-units/\n",
            "  inflating: training-set/natural_language_inference/49/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/49/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/49/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/49/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/49/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/49/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/49/triples/\n",
            "  inflating: training-set/natural_language_inference/49/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/49/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/49/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/49/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/49/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/4/1711.00106v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/4/1711.00106v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/4/1711.00106v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/4/entities.txt  \n",
            "   creating: training-set/natural_language_inference/4/info-units/\n",
            "  inflating: training-set/natural_language_inference/4/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/4/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/4/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/4/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/4/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/4/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/4/triples/\n",
            "  inflating: training-set/natural_language_inference/4/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/4/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/4/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/4/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/4/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/5/\n",
            "   creating: training-set/natural_language_inference/50/\n",
            "  inflating: training-set/natural_language_inference/50/1707.07847v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/50/1707.07847v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/50/1707.07847v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/50/entities.txt  \n",
            "   creating: training-set/natural_language_inference/50/info-units/\n",
            "  inflating: training-set/natural_language_inference/50/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/50/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/50/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/50/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/50/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/50/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/50/triples/\n",
            "  inflating: training-set/natural_language_inference/50/triples/baselines.txt  \n",
            "  inflating: training-set/natural_language_inference/50/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/50/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/50/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/50/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/51/\n",
            "  inflating: training-set/natural_language_inference/51/1906.08862v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/51/1906.08862v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/51/1906.08862v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/51/entities.txt  \n",
            "   creating: training-set/natural_language_inference/51/info-units/\n",
            "  inflating: training-set/natural_language_inference/51/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/51/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/51/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/51/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/51/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/51/triples/\n",
            "  inflating: training-set/natural_language_inference/51/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/51/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/51/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/51/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/52/\n",
            "  inflating: training-set/natural_language_inference/52/entities.txt  \n",
            "   creating: training-set/natural_language_inference/52/info-units/\n",
            "  inflating: training-set/natural_language_inference/52/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/52/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/52/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/52/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/52/K17-1009-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/52/K17-1009-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/52/K17-1009.pdf  \n",
            "  inflating: training-set/natural_language_inference/52/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/52/triples/\n",
            "  inflating: training-set/natural_language_inference/52/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/52/triples/baselines.txt  \n",
            "  inflating: training-set/natural_language_inference/52/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/52/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/53/\n",
            "  inflating: training-set/natural_language_inference/53/1705.02364v5-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/53/1705.02364v5-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/53/1705.02364v5.pdf  \n",
            "  inflating: training-set/natural_language_inference/53/entities.txt  \n",
            "   creating: training-set/natural_language_inference/53/info-units/\n",
            "  inflating: training-set/natural_language_inference/53/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/53/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/53/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/53/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/53/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/53/triples/\n",
            "  inflating: training-set/natural_language_inference/53/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/53/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/53/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/53/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/54/\n",
            "  inflating: training-set/natural_language_inference/54/1703.00572v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/54/1703.00572v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/54/1703.00572v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/54/entities.txt  \n",
            "   creating: training-set/natural_language_inference/54/info-units/\n",
            "  inflating: training-set/natural_language_inference/54/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/54/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/54/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/54/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/54/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/54/triples/\n",
            "  inflating: training-set/natural_language_inference/54/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/54/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/54/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/54/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/55/\n",
            "  inflating: training-set/natural_language_inference/55/1406.3676v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/55/1406.3676v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/55/1406.3676v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/55/entities.txt  \n",
            "   creating: training-set/natural_language_inference/55/info-units/\n",
            "  inflating: training-set/natural_language_inference/55/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/55/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/55/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/55/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/55/triples/\n",
            "  inflating: training-set/natural_language_inference/55/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/55/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/55/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/56/\n",
            "  inflating: training-set/natural_language_inference/56/1711.08028v4-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/56/1711.08028v4-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/56/1711.08028v4.pdf  \n",
            "  inflating: training-set/natural_language_inference/56/entities.txt  \n",
            "   creating: training-set/natural_language_inference/56/info-units/\n",
            "  inflating: training-set/natural_language_inference/56/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/56/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/56/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/56/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/56/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/56/triples/\n",
            "  inflating: training-set/natural_language_inference/56/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/56/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/56/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/56/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/57/\n",
            "  inflating: training-set/natural_language_inference/57/1511.06361v6-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/57/1511.06361v6-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/57/1511.06361v6.pdf  \n",
            "  inflating: training-set/natural_language_inference/57/entities.txt  \n",
            "   creating: training-set/natural_language_inference/57/info-units/\n",
            "  inflating: training-set/natural_language_inference/57/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/57/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/57/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/57/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/57/triples/\n",
            "  inflating: training-set/natural_language_inference/57/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/57/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/57/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/58/\n",
            "  inflating: training-set/natural_language_inference/58/1609.05284v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/58/1609.05284v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/58/1609.05284v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/58/entities.txt  \n",
            "   creating: training-set/natural_language_inference/58/info-units/\n",
            "  inflating: training-set/natural_language_inference/58/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/58/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/58/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/58/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/58/triples/\n",
            "  inflating: training-set/natural_language_inference/58/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/58/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/58/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/59/\n",
            "  inflating: training-set/natural_language_inference/59/1704.04565v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/59/1704.04565v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/59/1704.04565v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/59/entities.txt  \n",
            "   creating: training-set/natural_language_inference/59/info-units/\n",
            "  inflating: training-set/natural_language_inference/59/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/59/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/59/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/59/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/59/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/59/triples/\n",
            "  inflating: training-set/natural_language_inference/59/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/59/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/59/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/59/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/5/1905.12897v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/5/1905.12897v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/5/1905.12897v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/5/entities.txt  \n",
            "   creating: training-set/natural_language_inference/5/info-units/\n",
            "  inflating: training-set/natural_language_inference/5/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/5/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/5/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/5/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/5/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/5/triples/\n",
            "  inflating: training-set/natural_language_inference/5/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/5/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/5/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/5/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/6/\n",
            "   creating: training-set/natural_language_inference/60/\n",
            "  inflating: training-set/natural_language_inference/60/1804.07983v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/60/1804.07983v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/60/1804.07983v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/60/entities.txt  \n",
            "   creating: training-set/natural_language_inference/60/info-units/\n",
            "  inflating: training-set/natural_language_inference/60/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/60/info-units/research-problem.json  \n",
            " extracting: training-set/natural_language_inference/60/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/60/triples/\n",
            "  inflating: training-set/natural_language_inference/60/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/60/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/61/\n",
            "  inflating: training-set/natural_language_inference/61/1812.01840v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/61/1812.01840v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/61/1812.01840v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/61/entities.txt  \n",
            "   creating: training-set/natural_language_inference/61/info-units/\n",
            "  inflating: training-set/natural_language_inference/61/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/61/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/61/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/61/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/61/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/61/triples/\n",
            "  inflating: training-set/natural_language_inference/61/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/61/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/61/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/61/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/62/\n",
            "  inflating: training-set/natural_language_inference/62/1809.03449v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/62/1809.03449v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/62/1809.03449v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/62/entities.txt  \n",
            "   creating: training-set/natural_language_inference/62/info-units/\n",
            "  inflating: training-set/natural_language_inference/62/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/62/info-units/approach.json  \n",
            "  inflating: training-set/natural_language_inference/62/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/62/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/62/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/62/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/62/triples/\n",
            "  inflating: training-set/natural_language_inference/62/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/62/triples/approach.txt  \n",
            "  inflating: training-set/natural_language_inference/62/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/62/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/62/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/63/\n",
            "  inflating: training-set/natural_language_inference/63/entities.txt  \n",
            "   creating: training-set/natural_language_inference/63/info-units/\n",
            "  inflating: training-set/natural_language_inference/63/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/63/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/63/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/63/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/63/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/63/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/63/triples/\n",
            "  inflating: training-set/natural_language_inference/63/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/63/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/63/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/63/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/63/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/63/W18-2603-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/63/W18-2603-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/63/W18-2603.pdf  \n",
            "   creating: training-set/natural_language_inference/64/\n",
            "  inflating: training-set/natural_language_inference/64/1911.04118-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/64/1911.04118-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/64/1911.04118.pdf  \n",
            "  inflating: training-set/natural_language_inference/64/entities.txt  \n",
            "   creating: training-set/natural_language_inference/64/info-units/\n",
            "  inflating: training-set/natural_language_inference/64/info-units/dataset.json  \n",
            "  inflating: training-set/natural_language_inference/64/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/64/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/64/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/64/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/64/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/64/triples/\n",
            "  inflating: training-set/natural_language_inference/64/triples/dataset.txt  \n",
            "  inflating: training-set/natural_language_inference/64/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/64/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/64/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/64/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/65/\n",
            "  inflating: training-set/natural_language_inference/65/1602.03609v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/65/1602.03609v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/65/1602.03609v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/65/entities.txt  \n",
            "   creating: training-set/natural_language_inference/65/info-units/\n",
            "  inflating: training-set/natural_language_inference/65/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/65/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/65/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/65/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/65/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/65/triples/\n",
            "  inflating: training-set/natural_language_inference/65/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/65/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/65/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/65/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/66/\n",
            "  inflating: training-set/natural_language_inference/66/1512.08849v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/66/1512.08849v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/66/1512.08849v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/66/entities.txt  \n",
            "   creating: training-set/natural_language_inference/66/info-units/\n",
            "  inflating: training-set/natural_language_inference/66/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/66/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/66/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/66/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/66/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/66/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/66/triples/\n",
            " extracting: training-set/natural_language_inference/66/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/66/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/66/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/66/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/66/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/67/\n",
            "  inflating: training-set/natural_language_inference/67/1610.09996v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/67/1610.09996v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/67/1610.09996v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/67/entities.txt  \n",
            "   creating: training-set/natural_language_inference/67/info-units/\n",
            "  inflating: training-set/natural_language_inference/67/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/67/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/67/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/67/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/67/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/67/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/67/triples/\n",
            "  inflating: training-set/natural_language_inference/67/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/67/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/67/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/67/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/67/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/68/\n",
            "  inflating: training-set/natural_language_inference/68/1608.07905v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/68/1608.07905v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/68/1608.07905v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/68/entities.txt  \n",
            "   creating: training-set/natural_language_inference/68/info-units/\n",
            "  inflating: training-set/natural_language_inference/68/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/68/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/68/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/68/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/68/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/68/triples/\n",
            "  inflating: training-set/natural_language_inference/68/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/68/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/68/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/68/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/69/\n",
            "  inflating: training-set/natural_language_inference/69/1710.06481v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/69/1710.06481v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/69/1710.06481v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/69/entities.txt  \n",
            "   creating: training-set/natural_language_inference/69/info-units/\n",
            "  inflating: training-set/natural_language_inference/69/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/69/info-units/dataset.json  \n",
            "  inflating: training-set/natural_language_inference/69/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/69/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/69/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/69/triples/\n",
            "  inflating: training-set/natural_language_inference/69/triples/baselines.txt  \n",
            "  inflating: training-set/natural_language_inference/69/triples/dataset.txt  \n",
            "  inflating: training-set/natural_language_inference/69/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/69/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/6/1812.10464v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/6/1812.10464v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/6/1812.10464v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/6/entities.txt  \n",
            "   creating: training-set/natural_language_inference/6/info-units/\n",
            "  inflating: training-set/natural_language_inference/6/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/6/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/6/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/6/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/6/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/6/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/6/triples/\n",
            "  inflating: training-set/natural_language_inference/6/triples/ablation-analysis.txt  \n",
            " extracting: training-set/natural_language_inference/6/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/6/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/6/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/6/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/7/\n",
            "   creating: training-set/natural_language_inference/70/\n",
            "  inflating: training-set/natural_language_inference/70/entities.txt  \n",
            "   creating: training-set/natural_language_inference/70/info-units/\n",
            "  inflating: training-set/natural_language_inference/70/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/70/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/70/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/70/S16-1172-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/70/S16-1172-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/70/S16-1172.pdf  \n",
            "  inflating: training-set/natural_language_inference/70/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/70/triples/\n",
            "  inflating: training-set/natural_language_inference/70/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/70/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/70/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/71/\n",
            "  inflating: training-set/natural_language_inference/71/1706.02761v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/71/1706.02761v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/71/1706.02761v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/71/entities.txt  \n",
            "   creating: training-set/natural_language_inference/71/info-units/\n",
            "  inflating: training-set/natural_language_inference/71/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/71/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/71/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/71/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/71/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/71/triples/\n",
            " extracting: training-set/natural_language_inference/71/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/71/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/71/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/71/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/72/\n",
            "  inflating: training-set/natural_language_inference/72/entities.txt  \n",
            "   creating: training-set/natural_language_inference/72/info-units/\n",
            "  inflating: training-set/natural_language_inference/72/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/72/info-units/dataset.json  \n",
            "  inflating: training-set/natural_language_inference/72/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/72/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/72/N18-1140-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/72/N18-1140-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/72/N18-1140.pdf  \n",
            "  inflating: training-set/natural_language_inference/72/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/72/triples/\n",
            "  inflating: training-set/natural_language_inference/72/triples/baselines.txt  \n",
            "  inflating: training-set/natural_language_inference/72/triples/dataset.txt  \n",
            "  inflating: training-set/natural_language_inference/72/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/72/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/73/\n",
            "  inflating: training-set/natural_language_inference/73/1801.10296v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/73/1801.10296v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/73/1801.10296v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/73/entities.txt  \n",
            "   creating: training-set/natural_language_inference/73/info-units/\n",
            "  inflating: training-set/natural_language_inference/73/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/73/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/73/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/73/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/73/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/73/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/73/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/73/triples/\n",
            "  inflating: training-set/natural_language_inference/73/triples/ablation-analysis.txt  \n",
            " extracting: training-set/natural_language_inference/73/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/73/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/73/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/73/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/73/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/74/\n",
            "  inflating: training-set/natural_language_inference/74/entities.txt  \n",
            "   creating: training-set/natural_language_inference/74/info-units/\n",
            "  inflating: training-set/natural_language_inference/74/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/74/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/74/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/74/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/74/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/74/P18-1091-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/74/P18-1091-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/74/P18-1091.pdf  \n",
            "  inflating: training-set/natural_language_inference/74/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/74/triples/\n",
            "  inflating: training-set/natural_language_inference/74/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/74/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/74/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/74/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/74/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/75/\n",
            "  inflating: training-set/natural_language_inference/75/1606.03126v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/75/1606.03126v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/75/1606.03126v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/75/entities.txt  \n",
            "   creating: training-set/natural_language_inference/75/info-units/\n",
            "  inflating: training-set/natural_language_inference/75/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/75/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/75/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/75/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/75/triples/\n",
            "  inflating: training-set/natural_language_inference/75/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/75/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/75/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/76/\n",
            "  inflating: training-set/natural_language_inference/76/1509.06664v4-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/76/1509.06664v4-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/76/1509.06664v4.pdf  \n",
            "  inflating: training-set/natural_language_inference/76/entities.txt  \n",
            "   creating: training-set/natural_language_inference/76/info-units/\n",
            "  inflating: training-set/natural_language_inference/76/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/76/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/76/info-units/results.json  \n",
            " extracting: training-set/natural_language_inference/76/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/76/triples/\n",
            "  inflating: training-set/natural_language_inference/76/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/76/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/76/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/77/\n",
            "  inflating: training-set/natural_language_inference/77/1703.04816v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/77/1703.04816v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/77/1703.04816v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/77/entities.txt  \n",
            "   creating: training-set/natural_language_inference/77/info-units/\n",
            "  inflating: training-set/natural_language_inference/77/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/77/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/77/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/77/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/77/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/77/triples/\n",
            "  inflating: training-set/natural_language_inference/77/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/77/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/77/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/77/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/78/\n",
            "  inflating: training-set/natural_language_inference/78/1801.00102v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/78/1801.00102v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/78/1801.00102v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/78/entities.txt  \n",
            "   creating: training-set/natural_language_inference/78/info-units/\n",
            "  inflating: training-set/natural_language_inference/78/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/78/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/78/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/78/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/78/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/78/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/78/triples/\n",
            "  inflating: training-set/natural_language_inference/78/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/78/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/78/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/78/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/78/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/79/\n",
            "  inflating: training-set/natural_language_inference/79/1405.4053v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/79/1405.4053v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/79/1405.4053v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/79/entities.txt  \n",
            "   creating: training-set/natural_language_inference/79/info-units/\n",
            "  inflating: training-set/natural_language_inference/79/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/79/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/79/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/79/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/79/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/79/triples/\n",
            "  inflating: training-set/natural_language_inference/79/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/79/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/79/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/79/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/7/1611.01436v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/7/1611.01436v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/7/1611.01436v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/7/entities.txt  \n",
            "   creating: training-set/natural_language_inference/7/info-units/\n",
            "  inflating: training-set/natural_language_inference/7/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/7/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/7/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/7/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/7/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/7/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/7/triples/\n",
            "  inflating: training-set/natural_language_inference/7/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/7/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/7/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/7/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/7/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/8/\n",
            "   creating: training-set/natural_language_inference/80/\n",
            "  inflating: training-set/natural_language_inference/80/1802.05577v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/80/1802.05577v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/80/1802.05577v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/80/entities.txt  \n",
            "   creating: training-set/natural_language_inference/80/info-units/\n",
            "  inflating: training-set/natural_language_inference/80/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/80/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/80/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/80/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/80/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/80/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/80/triples/\n",
            "  inflating: training-set/natural_language_inference/80/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/80/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/80/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/80/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/80/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/81/\n",
            "  inflating: training-set/natural_language_inference/81/1901.00603v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/81/1901.00603v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/81/1901.00603v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/81/entities.txt  \n",
            "   creating: training-set/natural_language_inference/81/info-units/\n",
            "  inflating: training-set/natural_language_inference/81/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/81/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/81/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/81/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/81/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/81/triples/\n",
            "  inflating: training-set/natural_language_inference/81/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/81/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/81/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/81/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/82/\n",
            "  inflating: training-set/natural_language_inference/82/entities.txt  \n",
            "   creating: training-set/natural_language_inference/82/info-units/\n",
            "  inflating: training-set/natural_language_inference/82/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/82/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/82/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/82/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/82/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/82/N16-1099-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/82/N16-1099-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/82/N16-1099.pdf  \n",
            " extracting: training-set/natural_language_inference/82/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/82/triples/\n",
            " extracting: training-set/natural_language_inference/82/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/82/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/82/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/82/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/82/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/83/\n",
            "  inflating: training-set/natural_language_inference/83/D17-1168-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/83/D17-1168-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/83/D17-1168.pdf  \n",
            "  inflating: training-set/natural_language_inference/83/entities.txt  \n",
            "   creating: training-set/natural_language_inference/83/info-units/\n",
            "  inflating: training-set/natural_language_inference/83/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/83/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/83/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/83/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/83/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/83/triples/\n",
            "  inflating: training-set/natural_language_inference/83/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/83/triples/baselines.txt  \n",
            "  inflating: training-set/natural_language_inference/83/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/83/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/84/\n",
            "  inflating: training-set/natural_language_inference/84/1706.00286v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/84/1706.00286v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/84/1706.00286v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/84/entities.txt  \n",
            "   creating: training-set/natural_language_inference/84/info-units/\n",
            "  inflating: training-set/natural_language_inference/84/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/84/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/84/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/84/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/84/triples/\n",
            "  inflating: training-set/natural_language_inference/84/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/84/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/84/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/85/\n",
            "  inflating: training-set/natural_language_inference/85/1609.06038v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/85/1609.06038v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/85/1609.06038v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/85/entities.txt  \n",
            "   creating: training-set/natural_language_inference/85/info-units/\n",
            "  inflating: training-set/natural_language_inference/85/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/85/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/85/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/85/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/85/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/85/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/85/triples/\n",
            "  inflating: training-set/natural_language_inference/85/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/85/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/85/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/85/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/85/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/86/\n",
            "  inflating: training-set/natural_language_inference/86/1612.04211v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/86/1612.04211v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/86/1612.04211v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/86/entities.txt  \n",
            "   creating: training-set/natural_language_inference/86/info-units/\n",
            "  inflating: training-set/natural_language_inference/86/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/86/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/86/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/86/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/86/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/86/triples/\n",
            "  inflating: training-set/natural_language_inference/86/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/86/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/86/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/86/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/87/\n",
            "  inflating: training-set/natural_language_inference/87/1908.05147v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/87/1908.05147v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/87/1908.05147v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/87/entities.txt  \n",
            "   creating: training-set/natural_language_inference/87/info-units/\n",
            "  inflating: training-set/natural_language_inference/87/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/87/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/87/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/87/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/87/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/87/triples/\n",
            "  inflating: training-set/natural_language_inference/87/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/87/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/87/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/87/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/88/\n",
            "  inflating: training-set/natural_language_inference/88/1601.06733v7-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/88/1601.06733v7-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/88/1601.06733v7.pdf  \n",
            "  inflating: training-set/natural_language_inference/88/entities.txt  \n",
            "   creating: training-set/natural_language_inference/88/info-units/\n",
            "  inflating: training-set/natural_language_inference/88/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/88/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/88/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/88/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/88/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/88/triples/\n",
            " extracting: training-set/natural_language_inference/88/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/88/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/88/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/88/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/89/\n",
            "  inflating: training-set/natural_language_inference/89/1808.05759v5-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/89/1808.05759v5-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/89/1808.05759v5.pdf  \n",
            "  inflating: training-set/natural_language_inference/89/entities.txt  \n",
            "   creating: training-set/natural_language_inference/89/info-units/\n",
            "  inflating: training-set/natural_language_inference/89/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/89/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/89/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/89/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/89/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/89/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/89/triples/\n",
            "  inflating: training-set/natural_language_inference/89/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/89/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/89/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/89/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/89/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/8/1412.1632v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/8/1412.1632v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/8/1412.1632v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/8/entities.txt  \n",
            "   creating: training-set/natural_language_inference/8/info-units/\n",
            "  inflating: training-set/natural_language_inference/8/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/8/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/8/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/8/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/8/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/8/triples/\n",
            "  inflating: training-set/natural_language_inference/8/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/8/triples/model.txt  \n",
            " extracting: training-set/natural_language_inference/8/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/8/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/9/\n",
            "   creating: training-set/natural_language_inference/90/\n",
            "  inflating: training-set/natural_language_inference/90/1606.01933v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/90/1606.01933v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/90/1606.01933v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/90/entities.txt  \n",
            "   creating: training-set/natural_language_inference/90/info-units/\n",
            "  inflating: training-set/natural_language_inference/90/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/90/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/90/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/90/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/90/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/90/triples/\n",
            "  inflating: training-set/natural_language_inference/90/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/90/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/90/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/90/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/91/\n",
            "  inflating: training-set/natural_language_inference/91/1603.06021v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/91/1603.06021v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/91/1603.06021v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/91/entities.txt  \n",
            "   creating: training-set/natural_language_inference/91/info-units/\n",
            "  inflating: training-set/natural_language_inference/91/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/91/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/91/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/91/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/91/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/91/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/91/triples/\n",
            " extracting: training-set/natural_language_inference/91/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/91/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/91/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/91/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/91/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/92/\n",
            "  inflating: training-set/natural_language_inference/92/1909.04849v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/92/1909.04849v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/92/1909.04849v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/92/entities.txt  \n",
            "   creating: training-set/natural_language_inference/92/info-units/\n",
            "  inflating: training-set/natural_language_inference/92/info-units/experiments.json  \n",
            "  inflating: training-set/natural_language_inference/92/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/92/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/92/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/92/triples/\n",
            "  inflating: training-set/natural_language_inference/92/triples/experiments.txt  \n",
            "  inflating: training-set/natural_language_inference/92/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/92/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/93/\n",
            "  inflating: training-set/natural_language_inference/93/entities.txt  \n",
            "   creating: training-set/natural_language_inference/93/info-units/\n",
            "  inflating: training-set/natural_language_inference/93/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/93/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/93/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/93/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/93/P17-1018-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/93/P17-1018-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/93/P17-1018.pdf  \n",
            "  inflating: training-set/natural_language_inference/93/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/93/triples/\n",
            "  inflating: training-set/natural_language_inference/93/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/93/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/93/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/93/triples/research-problem.txt  \n",
            "   creating: training-set/natural_language_inference/94/\n",
            "  inflating: training-set/natural_language_inference/94/1809.06309v3-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/94/1809.06309v3-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/94/1809.06309v3.pdf  \n",
            "  inflating: training-set/natural_language_inference/94/entities.txt  \n",
            "   creating: training-set/natural_language_inference/94/info-units/\n",
            "  inflating: training-set/natural_language_inference/94/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/94/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/94/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/94/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/94/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/94/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/94/triples/\n",
            "  inflating: training-set/natural_language_inference/94/triples/ablation-analysis.txt  \n",
            " extracting: training-set/natural_language_inference/94/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/94/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/94/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/94/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/95/\n",
            "  inflating: training-set/natural_language_inference/95/1805.02220v2-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/95/1805.02220v2-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/95/1805.02220v2.pdf  \n",
            "  inflating: training-set/natural_language_inference/95/entities.txt  \n",
            "   creating: training-set/natural_language_inference/95/info-units/\n",
            "  inflating: training-set/natural_language_inference/95/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/95/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/95/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/95/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/95/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/95/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/95/triples/\n",
            "  inflating: training-set/natural_language_inference/95/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/95/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/95/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/95/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/95/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/96/\n",
            "  inflating: training-set/natural_language_inference/96/1808.05326v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/96/1808.05326v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/96/1808.05326v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/96/entities.txt  \n",
            "   creating: training-set/natural_language_inference/96/info-units/\n",
            "  inflating: training-set/natural_language_inference/96/info-units/dataset.json  \n",
            "  inflating: training-set/natural_language_inference/96/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/96/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/96/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/96/triples/\n",
            "  inflating: training-set/natural_language_inference/96/triples/dataset.txt  \n",
            "  inflating: training-set/natural_language_inference/96/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/96/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/97/\n",
            "  inflating: training-set/natural_language_inference/97/1603.08884v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/97/1603.08884v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/97/1603.08884v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/97/entities.txt  \n",
            "   creating: training-set/natural_language_inference/97/info-units/\n",
            "  inflating: training-set/natural_language_inference/97/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/97/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/97/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/97/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/97/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/97/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/97/triples/\n",
            "  inflating: training-set/natural_language_inference/97/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/97/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/97/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/97/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/97/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/98/\n",
            "  inflating: training-set/natural_language_inference/98/1708.01353v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/98/1708.01353v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/98/1708.01353v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/98/entities.txt  \n",
            "   creating: training-set/natural_language_inference/98/info-units/\n",
            "  inflating: training-set/natural_language_inference/98/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/98/info-units/code.json  \n",
            "  inflating: training-set/natural_language_inference/98/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/98/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/98/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/98/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/98/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/98/triples/\n",
            "  inflating: training-set/natural_language_inference/98/triples/ablation-analysis.txt  \n",
            " extracting: training-set/natural_language_inference/98/triples/code.txt  \n",
            "  inflating: training-set/natural_language_inference/98/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/98/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/98/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/98/triples/results.txt  \n",
            "   creating: training-set/natural_language_inference/99/\n",
            "  inflating: training-set/natural_language_inference/99/1710.02772v1-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/99/1710.02772v1-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/99/1710.02772v1.pdf  \n",
            "  inflating: training-set/natural_language_inference/99/entities.txt  \n",
            "   creating: training-set/natural_language_inference/99/info-units/\n",
            "  inflating: training-set/natural_language_inference/99/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/99/info-units/experimental-setup.json  \n",
            "  inflating: training-set/natural_language_inference/99/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/99/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/99/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/99/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/99/triples/\n",
            "  inflating: training-set/natural_language_inference/99/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/99/triples/experimental-setup.txt  \n",
            "  inflating: training-set/natural_language_inference/99/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/99/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/99/triples/results.txt  \n",
            "  inflating: training-set/natural_language_inference/9/1606.04582v6-Grobid-out.txt  \n",
            "  inflating: training-set/natural_language_inference/9/1606.04582v6-Stanza-out.txt  \n",
            "  inflating: training-set/natural_language_inference/9/1606.04582v6.pdf  \n",
            "  inflating: training-set/natural_language_inference/9/entities.txt  \n",
            "   creating: training-set/natural_language_inference/9/info-units/\n",
            "  inflating: training-set/natural_language_inference/9/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/natural_language_inference/9/info-units/baselines.json  \n",
            "  inflating: training-set/natural_language_inference/9/info-units/hyperparameters.json  \n",
            "  inflating: training-set/natural_language_inference/9/info-units/model.json  \n",
            "  inflating: training-set/natural_language_inference/9/info-units/research-problem.json  \n",
            "  inflating: training-set/natural_language_inference/9/info-units/results.json  \n",
            "  inflating: training-set/natural_language_inference/9/sentences.txt  \n",
            "   creating: training-set/natural_language_inference/9/triples/\n",
            "  inflating: training-set/natural_language_inference/9/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/natural_language_inference/9/triples/baselines.txt  \n",
            "  inflating: training-set/natural_language_inference/9/triples/hyperparameters.txt  \n",
            "  inflating: training-set/natural_language_inference/9/triples/model.txt  \n",
            "  inflating: training-set/natural_language_inference/9/triples/research-problem.txt  \n",
            "  inflating: training-set/natural_language_inference/9/triples/results.txt  \n",
            "   creating: training-set/negation_scope_resolution/\n",
            "   creating: training-set/negation_scope_resolution/0/\n",
            "  inflating: training-set/negation_scope_resolution/0/1911.04211v3-Grobid-out.txt  \n",
            "  inflating: training-set/negation_scope_resolution/0/1911.04211v3-Stanza-out.txt  \n",
            "  inflating: training-set/negation_scope_resolution/0/1911.04211v3.pdf  \n",
            "  inflating: training-set/negation_scope_resolution/0/entities.txt  \n",
            "   creating: training-set/negation_scope_resolution/0/info-units/\n",
            "  inflating: training-set/negation_scope_resolution/0/info-units/approach.json  \n",
            "  inflating: training-set/negation_scope_resolution/0/info-units/experimental-setup.json  \n",
            "  inflating: training-set/negation_scope_resolution/0/info-units/research-problem.json  \n",
            "  inflating: training-set/negation_scope_resolution/0/info-units/results.json  \n",
            "  inflating: training-set/negation_scope_resolution/0/sentences.txt  \n",
            "   creating: training-set/negation_scope_resolution/0/triples/\n",
            "  inflating: training-set/negation_scope_resolution/0/triples/approach.txt  \n",
            "  inflating: training-set/negation_scope_resolution/0/triples/experimental-setup.txt  \n",
            "  inflating: training-set/negation_scope_resolution/0/triples/research-problem.txt  \n",
            "  inflating: training-set/negation_scope_resolution/0/triples/results.txt  \n",
            "   creating: training-set/paraphrase_generation/\n",
            "   creating: training-set/paraphrase_generation/0/\n",
            "  inflating: training-set/paraphrase_generation/0/1806.00807v5-Grobid-out.txt  \n",
            "  inflating: training-set/paraphrase_generation/0/1806.00807v5-Stanza-out.txt  \n",
            "  inflating: training-set/paraphrase_generation/0/1806.00807v5.pdf  \n",
            "  inflating: training-set/paraphrase_generation/0/entities.txt  \n",
            "   creating: training-set/paraphrase_generation/0/info-units/\n",
            "  inflating: training-set/paraphrase_generation/0/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/paraphrase_generation/0/info-units/baselines.json  \n",
            "  inflating: training-set/paraphrase_generation/0/info-units/model.json  \n",
            "  inflating: training-set/paraphrase_generation/0/info-units/research-problem.json  \n",
            "  inflating: training-set/paraphrase_generation/0/sentences.txt  \n",
            "   creating: training-set/paraphrase_generation/0/triples/\n",
            "  inflating: training-set/paraphrase_generation/0/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/paraphrase_generation/0/triples/baselines.txt  \n",
            "  inflating: training-set/paraphrase_generation/0/triples/model.txt  \n",
            "  inflating: training-set/paraphrase_generation/0/triples/research-problem.txt  \n",
            "   creating: training-set/paraphrase_generation/1/\n",
            "  inflating: training-set/paraphrase_generation/1/1709.05074v1-Grobid-out.txt  \n",
            "  inflating: training-set/paraphrase_generation/1/1709.05074v1-Stanza-out.txt  \n",
            "  inflating: training-set/paraphrase_generation/1/1709.05074v1.pdf  \n",
            "  inflating: training-set/paraphrase_generation/1/entities.txt  \n",
            "   creating: training-set/paraphrase_generation/1/info-units/\n",
            "  inflating: training-set/paraphrase_generation/1/info-units/baselines.json  \n",
            "  inflating: training-set/paraphrase_generation/1/info-units/hyperparameters.json  \n",
            "  inflating: training-set/paraphrase_generation/1/info-units/model.json  \n",
            "  inflating: training-set/paraphrase_generation/1/info-units/research-problem.json  \n",
            "  inflating: training-set/paraphrase_generation/1/info-units/results.json  \n",
            "  inflating: training-set/paraphrase_generation/1/sentences.txt  \n",
            "   creating: training-set/paraphrase_generation/1/triples/\n",
            "  inflating: training-set/paraphrase_generation/1/triples/baselines.txt  \n",
            "  inflating: training-set/paraphrase_generation/1/triples/hyperparameters.txt  \n",
            "  inflating: training-set/paraphrase_generation/1/triples/model.txt  \n",
            "  inflating: training-set/paraphrase_generation/1/triples/research-problem.txt  \n",
            "  inflating: training-set/paraphrase_generation/1/triples/results.txt  \n",
            "   creating: training-set/part-of-speech_tagging/\n",
            "   creating: training-set/part-of-speech_tagging/0/\n",
            "  inflating: training-set/part-of-speech_tagging/0/1711.04903v2-Grobid-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/0/1711.04903v2-Stanza-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/0/1711.04903v2.pdf  \n",
            "  inflating: training-set/part-of-speech_tagging/0/entities.txt  \n",
            "   creating: training-set/part-of-speech_tagging/0/info-units/\n",
            "  inflating: training-set/part-of-speech_tagging/0/info-units/hyperparameters.json  \n",
            "  inflating: training-set/part-of-speech_tagging/0/info-units/model.json  \n",
            "  inflating: training-set/part-of-speech_tagging/0/info-units/research-problem.json  \n",
            "  inflating: training-set/part-of-speech_tagging/0/info-units/results.json  \n",
            "  inflating: training-set/part-of-speech_tagging/0/sentences.txt  \n",
            "   creating: training-set/part-of-speech_tagging/0/triples/\n",
            "  inflating: training-set/part-of-speech_tagging/0/triples/hyperparameters.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/0/triples/model.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/0/triples/research-problem.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/0/triples/results.txt  \n",
            "   creating: training-set/part-of-speech_tagging/1/\n",
            "  inflating: training-set/part-of-speech_tagging/1/1810.12443v1-Grobid-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/1/1810.12443v1-Stanza-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/1/1810.12443v1.pdf  \n",
            "  inflating: training-set/part-of-speech_tagging/1/entities.txt  \n",
            "   creating: training-set/part-of-speech_tagging/1/info-units/\n",
            "  inflating: training-set/part-of-speech_tagging/1/info-units/baselines.json  \n",
            "  inflating: training-set/part-of-speech_tagging/1/info-units/hyperparameters.json  \n",
            "  inflating: training-set/part-of-speech_tagging/1/info-units/model.json  \n",
            "  inflating: training-set/part-of-speech_tagging/1/info-units/research-problem.json  \n",
            "  inflating: training-set/part-of-speech_tagging/1/info-units/results.json  \n",
            "  inflating: training-set/part-of-speech_tagging/1/sentences.txt  \n",
            "   creating: training-set/part-of-speech_tagging/1/triples/\n",
            "  inflating: training-set/part-of-speech_tagging/1/triples/baselines.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/1/triples/hyperparameters.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/1/triples/model.txt  \n",
            " extracting: training-set/part-of-speech_tagging/1/triples/research-problem.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/1/triples/results.txt  \n",
            "   creating: training-set/part-of-speech_tagging/2/\n",
            "  inflating: training-set/part-of-speech_tagging/2/1703.06345v1-Grobid-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/2/1703.06345v1-Stanza-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/2/1703.06345v1.pdf  \n",
            "  inflating: training-set/part-of-speech_tagging/2/entities.txt  \n",
            "   creating: training-set/part-of-speech_tagging/2/info-units/\n",
            "  inflating: training-set/part-of-speech_tagging/2/info-units/approach.json  \n",
            "  inflating: training-set/part-of-speech_tagging/2/info-units/code.json  \n",
            "  inflating: training-set/part-of-speech_tagging/2/info-units/experiments.json  \n",
            "  inflating: training-set/part-of-speech_tagging/2/info-units/research-problem.json  \n",
            "  inflating: training-set/part-of-speech_tagging/2/sentences.txt  \n",
            "   creating: training-set/part-of-speech_tagging/2/triples/\n",
            "  inflating: training-set/part-of-speech_tagging/2/triples/approach.txt  \n",
            " extracting: training-set/part-of-speech_tagging/2/triples/code.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/2/triples/experiments.txt  \n",
            " extracting: training-set/part-of-speech_tagging/2/triples/research-problem.txt  \n",
            "   creating: training-set/part-of-speech_tagging/3/\n",
            "  inflating: training-set/part-of-speech_tagging/3/1603.01354v5-Grobid-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/3/1603.01354v5-Stanza-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/3/1603.01354v5.pdf  \n",
            "  inflating: training-set/part-of-speech_tagging/3/entities.txt  \n",
            "   creating: training-set/part-of-speech_tagging/3/info-units/\n",
            "  inflating: training-set/part-of-speech_tagging/3/info-units/baselines.json  \n",
            "  inflating: training-set/part-of-speech_tagging/3/info-units/hyperparameters.json  \n",
            "  inflating: training-set/part-of-speech_tagging/3/info-units/model.json  \n",
            "  inflating: training-set/part-of-speech_tagging/3/info-units/research-problem.json  \n",
            "  inflating: training-set/part-of-speech_tagging/3/info-units/results.json  \n",
            "  inflating: training-set/part-of-speech_tagging/3/sentences.txt  \n",
            "   creating: training-set/part-of-speech_tagging/3/triples/\n",
            "  inflating: training-set/part-of-speech_tagging/3/triples/baselines.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/3/triples/hyperparameters.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/3/triples/model.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/3/triples/research-problem.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/3/triples/results.txt  \n",
            "   creating: training-set/part-of-speech_tagging/4/\n",
            "  inflating: training-set/part-of-speech_tagging/4/1908.08676v3-Grobid-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/4/1908.08676v3-Stanza-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/4/1908.08676v3.pdf  \n",
            "  inflating: training-set/part-of-speech_tagging/4/entities.txt  \n",
            "   creating: training-set/part-of-speech_tagging/4/info-units/\n",
            "  inflating: training-set/part-of-speech_tagging/4/info-units/model.json  \n",
            "  inflating: training-set/part-of-speech_tagging/4/info-units/research-problem.json  \n",
            "  inflating: training-set/part-of-speech_tagging/4/info-units/results.json  \n",
            "  inflating: training-set/part-of-speech_tagging/4/sentences.txt  \n",
            "   creating: training-set/part-of-speech_tagging/4/triples/\n",
            "  inflating: training-set/part-of-speech_tagging/4/triples/model.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/4/triples/research-problem.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/4/triples/results.txt  \n",
            "   creating: training-set/part-of-speech_tagging/5/\n",
            "  inflating: training-set/part-of-speech_tagging/5/1805.08237v1-Grobid-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/5/1805.08237v1-Stanza-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/5/1805.08237v1.pdf  \n",
            "  inflating: training-set/part-of-speech_tagging/5/entities.txt  \n",
            "   creating: training-set/part-of-speech_tagging/5/info-units/\n",
            "  inflating: training-set/part-of-speech_tagging/5/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/part-of-speech_tagging/5/info-units/hyperparameters.json  \n",
            "  inflating: training-set/part-of-speech_tagging/5/info-units/model.json  \n",
            "  inflating: training-set/part-of-speech_tagging/5/info-units/research-problem.json  \n",
            "  inflating: training-set/part-of-speech_tagging/5/info-units/results.json  \n",
            "  inflating: training-set/part-of-speech_tagging/5/sentences.txt  \n",
            "   creating: training-set/part-of-speech_tagging/5/triples/\n",
            "  inflating: training-set/part-of-speech_tagging/5/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/5/triples/hyperparameters.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/5/triples/model.txt  \n",
            " extracting: training-set/part-of-speech_tagging/5/triples/research-problem.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/5/triples/results.txt  \n",
            "   creating: training-set/part-of-speech_tagging/6/\n",
            "  inflating: training-set/part-of-speech_tagging/6/1705.05952v2-Grobid-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/6/1705.05952v2-Stanza-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/6/1705.05952v2.pdf  \n",
            "  inflating: training-set/part-of-speech_tagging/6/entities.txt  \n",
            "   creating: training-set/part-of-speech_tagging/6/info-units/\n",
            "  inflating: training-set/part-of-speech_tagging/6/info-units/code.json  \n",
            "  inflating: training-set/part-of-speech_tagging/6/info-units/experimental-setup.json  \n",
            "  inflating: training-set/part-of-speech_tagging/6/info-units/model.json  \n",
            "  inflating: training-set/part-of-speech_tagging/6/info-units/research-problem.json  \n",
            "  inflating: training-set/part-of-speech_tagging/6/info-units/results.json  \n",
            "  inflating: training-set/part-of-speech_tagging/6/sentences.txt  \n",
            "   creating: training-set/part-of-speech_tagging/6/triples/\n",
            " extracting: training-set/part-of-speech_tagging/6/triples/code.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/6/triples/experimental-setup.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/6/triples/model.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/6/triples/research-problem.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/6/triples/results.txt  \n",
            "   creating: training-set/part-of-speech_tagging/7/\n",
            "  inflating: training-set/part-of-speech_tagging/7/1604.05529v3-Grobid-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/7/1604.05529v3-Stanza-out.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/7/1604.05529v3.pdf  \n",
            "  inflating: training-set/part-of-speech_tagging/7/entities.txt  \n",
            "   creating: training-set/part-of-speech_tagging/7/info-units/\n",
            "  inflating: training-set/part-of-speech_tagging/7/info-units/code.json  \n",
            "  inflating: training-set/part-of-speech_tagging/7/info-units/hyperparameters.json  \n",
            "  inflating: training-set/part-of-speech_tagging/7/info-units/model.json  \n",
            "  inflating: training-set/part-of-speech_tagging/7/info-units/research-problem.json  \n",
            "  inflating: training-set/part-of-speech_tagging/7/info-units/results.json  \n",
            "  inflating: training-set/part-of-speech_tagging/7/sentences.txt  \n",
            "   creating: training-set/part-of-speech_tagging/7/triples/\n",
            " extracting: training-set/part-of-speech_tagging/7/triples/code.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/7/triples/hyperparameters.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/7/triples/model.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/7/triples/research-problem.txt  \n",
            "  inflating: training-set/part-of-speech_tagging/7/triples/results.txt  \n",
            "   creating: training-set/passage_re-ranking/\n",
            "   creating: training-set/passage_re-ranking/0/\n",
            "  inflating: training-set/passage_re-ranking/0/1904.08375v2-Grobid-out.txt  \n",
            "  inflating: training-set/passage_re-ranking/0/1904.08375v2-Stanza-out.txt  \n",
            "  inflating: training-set/passage_re-ranking/0/1904.08375v2.pdf  \n",
            "  inflating: training-set/passage_re-ranking/0/entities.txt  \n",
            "   creating: training-set/passage_re-ranking/0/info-units/\n",
            "  inflating: training-set/passage_re-ranking/0/info-units/approach.json  \n",
            "  inflating: training-set/passage_re-ranking/0/info-units/baselines.json  \n",
            "  inflating: training-set/passage_re-ranking/0/info-units/research-problem.json  \n",
            "  inflating: training-set/passage_re-ranking/0/info-units/results.json  \n",
            "  inflating: training-set/passage_re-ranking/0/sentences.txt  \n",
            "   creating: training-set/passage_re-ranking/0/triples/\n",
            "  inflating: training-set/passage_re-ranking/0/triples/approach.txt  \n",
            "  inflating: training-set/passage_re-ranking/0/triples/baselines.txt  \n",
            " extracting: training-set/passage_re-ranking/0/triples/research-problem.txt  \n",
            "  inflating: training-set/passage_re-ranking/0/triples/results.txt  \n",
            "   creating: training-set/passage_re-ranking/1/\n",
            "  inflating: training-set/passage_re-ranking/1/1901.04085v4-Grobid-out.txt  \n",
            "  inflating: training-set/passage_re-ranking/1/1901.04085v4-Stanza-out.txt  \n",
            "  inflating: training-set/passage_re-ranking/1/1901.04085v4.pdf  \n",
            "  inflating: training-set/passage_re-ranking/1/entities.txt  \n",
            "   creating: training-set/passage_re-ranking/1/info-units/\n",
            "  inflating: training-set/passage_re-ranking/1/info-units/approach.json  \n",
            "  inflating: training-set/passage_re-ranking/1/info-units/experiments.json  \n",
            "  inflating: training-set/passage_re-ranking/1/info-units/research-problem.json  \n",
            "  inflating: training-set/passage_re-ranking/1/info-units/results.json  \n",
            "  inflating: training-set/passage_re-ranking/1/sentences.txt  \n",
            "   creating: training-set/passage_re-ranking/1/triples/\n",
            "  inflating: training-set/passage_re-ranking/1/triples/approach.txt  \n",
            "  inflating: training-set/passage_re-ranking/1/triples/experiments.txt  \n",
            "  inflating: training-set/passage_re-ranking/1/triples/research-problem.txt  \n",
            "  inflating: training-set/passage_re-ranking/1/triples/results.txt  \n",
            "   creating: training-set/phrase_grounding/\n",
            "   creating: training-set/phrase_grounding/0/\n",
            "  inflating: training-set/phrase_grounding/0/1811.11683v2-Grobid-out.txt  \n",
            "  inflating: training-set/phrase_grounding/0/1811.11683v2-Stanza-out.txt  \n",
            "  inflating: training-set/phrase_grounding/0/1811.11683v2.pdf  \n",
            "  inflating: training-set/phrase_grounding/0/entities.txt  \n",
            "   creating: training-set/phrase_grounding/0/info-units/\n",
            "  inflating: training-set/phrase_grounding/0/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/phrase_grounding/0/info-units/hyperparameters.json  \n",
            "  inflating: training-set/phrase_grounding/0/info-units/model.json  \n",
            "  inflating: training-set/phrase_grounding/0/info-units/research-problem.json  \n",
            "  inflating: training-set/phrase_grounding/0/info-units/results.json  \n",
            "  inflating: training-set/phrase_grounding/0/sentences.txt  \n",
            "   creating: training-set/phrase_grounding/0/triples/\n",
            "  inflating: training-set/phrase_grounding/0/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/phrase_grounding/0/triples/hyperparameters.txt  \n",
            "  inflating: training-set/phrase_grounding/0/triples/model.txt  \n",
            "  inflating: training-set/phrase_grounding/0/triples/research-problem.txt  \n",
            "  inflating: training-set/phrase_grounding/0/triples/results.txt  \n",
            "   creating: training-set/prosody_prediction/\n",
            "   creating: training-set/prosody_prediction/0/\n",
            "  inflating: training-set/prosody_prediction/0/1908.02262v1-Grobid-out.txt  \n",
            "  inflating: training-set/prosody_prediction/0/1908.02262v1-Stanza-out.txt  \n",
            "  inflating: training-set/prosody_prediction/0/1908.02262v1.pdf  \n",
            "  inflating: training-set/prosody_prediction/0/entities.txt  \n",
            "   creating: training-set/prosody_prediction/0/info-units/\n",
            "  inflating: training-set/prosody_prediction/0/info-units/code.json  \n",
            "  inflating: training-set/prosody_prediction/0/info-units/dataset.json  \n",
            "  inflating: training-set/prosody_prediction/0/info-units/experimental-setup.json  \n",
            "  inflating: training-set/prosody_prediction/0/info-units/research-problem.json  \n",
            "  inflating: training-set/prosody_prediction/0/info-units/results.json  \n",
            "  inflating: training-set/prosody_prediction/0/sentences.txt  \n",
            "   creating: training-set/prosody_prediction/0/triples/\n",
            " extracting: training-set/prosody_prediction/0/triples/code.txt  \n",
            "  inflating: training-set/prosody_prediction/0/triples/dataset.txt  \n",
            "  inflating: training-set/prosody_prediction/0/triples/experimental-setup.txt  \n",
            "  inflating: training-set/prosody_prediction/0/triples/research-problem.txt  \n",
            "  inflating: training-set/prosody_prediction/0/triples/results.txt  \n",
            "   creating: training-set/query_wellformedness/\n",
            "   creating: training-set/query_wellformedness/0/\n",
            "  inflating: training-set/query_wellformedness/0/1808.09419v1-Grobid-out.txt  \n",
            "  inflating: training-set/query_wellformedness/0/1808.09419v1-Stanza-out.txt  \n",
            "  inflating: training-set/query_wellformedness/0/1808.09419v1.pdf  \n",
            "  inflating: training-set/query_wellformedness/0/entities.txt  \n",
            "   creating: training-set/query_wellformedness/0/info-units/\n",
            "  inflating: training-set/query_wellformedness/0/info-units/baselines.json  \n",
            "  inflating: training-set/query_wellformedness/0/info-units/dataset.json  \n",
            "  inflating: training-set/query_wellformedness/0/info-units/model.json  \n",
            "  inflating: training-set/query_wellformedness/0/info-units/research-problem.json  \n",
            "  inflating: training-set/query_wellformedness/0/info-units/results.json  \n",
            " extracting: training-set/query_wellformedness/0/sentences.txt  \n",
            "   creating: training-set/query_wellformedness/0/triples/\n",
            "  inflating: training-set/query_wellformedness/0/triples/baselines.txt  \n",
            "  inflating: training-set/query_wellformedness/0/triples/dataset.txt  \n",
            "  inflating: training-set/query_wellformedness/0/triples/model.txt  \n",
            "  inflating: training-set/query_wellformedness/0/triples/research-problem.txt  \n",
            "  inflating: training-set/query_wellformedness/0/triples/results.txt  \n",
            "   creating: training-set/question_answering/\n",
            "   creating: training-set/question_answering/0/\n",
            "  inflating: training-set/question_answering/0/C18-1280-Grobid-out.txt  \n",
            "  inflating: training-set/question_answering/0/C18-1280-Stanza-out.txt  \n",
            "  inflating: training-set/question_answering/0/C18-1280.pdf  \n",
            "  inflating: training-set/question_answering/0/entities.txt  \n",
            "   creating: training-set/question_answering/0/info-units/\n",
            "  inflating: training-set/question_answering/0/info-units/approach.json  \n",
            "  inflating: training-set/question_answering/0/info-units/baselines.json  \n",
            "  inflating: training-set/question_answering/0/info-units/code.json  \n",
            "  inflating: training-set/question_answering/0/info-units/research-problem.json  \n",
            "  inflating: training-set/question_answering/0/info-units/results.json  \n",
            "  inflating: training-set/question_answering/0/sentences.txt  \n",
            "   creating: training-set/question_answering/0/triples/\n",
            "  inflating: training-set/question_answering/0/triples/approach.txt  \n",
            "  inflating: training-set/question_answering/0/triples/baselines.txt  \n",
            "  inflating: training-set/question_answering/0/triples/code.txt  \n",
            "  inflating: training-set/question_answering/0/triples/research-problem.txt  \n",
            "  inflating: training-set/question_answering/0/triples/results.txt  \n",
            "   creating: training-set/question_answering/1/\n",
            "  inflating: training-set/question_answering/1/1611.01603v6-Grobid-out.txt  \n",
            "  inflating: training-set/question_answering/1/1611.01603v6-Stanza-out.txt  \n",
            "  inflating: training-set/question_answering/1/1611.01603v6.pdf  \n",
            "  inflating: training-set/question_answering/1/entities.txt  \n",
            "   creating: training-set/question_answering/1/info-units/\n",
            "  inflating: training-set/question_answering/1/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/question_answering/1/info-units/experimental-setup.json  \n",
            "  inflating: training-set/question_answering/1/info-units/model.json  \n",
            "  inflating: training-set/question_answering/1/info-units/research-problem.json  \n",
            "  inflating: training-set/question_answering/1/info-units/results.json  \n",
            "  inflating: training-set/question_answering/1/sentences.txt  \n",
            "   creating: training-set/question_answering/1/triples/\n",
            "  inflating: training-set/question_answering/1/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/question_answering/1/triples/experimental-setup.txt  \n",
            "  inflating: training-set/question_answering/1/triples/model.txt  \n",
            "  inflating: training-set/question_answering/1/triples/research-problem.txt  \n",
            "  inflating: training-set/question_answering/1/triples/results.txt  \n",
            "   creating: training-set/question_answering/2/\n",
            "  inflating: training-set/question_answering/2/1806.01873v2-Grobid-out.txt  \n",
            "  inflating: training-set/question_answering/2/1806.01873v2-Stanza-out.txt  \n",
            "  inflating: training-set/question_answering/2/1806.01873v2.pdf  \n",
            "  inflating: training-set/question_answering/2/entities.txt  \n",
            "   creating: training-set/question_answering/2/info-units/\n",
            "  inflating: training-set/question_answering/2/info-units/experiments.json  \n",
            "  inflating: training-set/question_answering/2/info-units/model.json  \n",
            "  inflating: training-set/question_answering/2/info-units/research-problem.json  \n",
            "  inflating: training-set/question_answering/2/sentences.txt  \n",
            "   creating: training-set/question_answering/2/triples/\n",
            "  inflating: training-set/question_answering/2/triples/experiments.txt  \n",
            "  inflating: training-set/question_answering/2/triples/model.txt  \n",
            "  inflating: training-set/question_answering/2/triples/research-problem.txt  \n",
            "   creating: training-set/question_answering/3/\n",
            "  inflating: training-set/question_answering/3/D18-1238-Grobid-out.txt  \n",
            "  inflating: training-set/question_answering/3/D18-1238-Stanza-out.txt  \n",
            "  inflating: training-set/question_answering/3/D18-1238.pdf  \n",
            "  inflating: training-set/question_answering/3/entities.txt  \n",
            "   creating: training-set/question_answering/3/info-units/\n",
            "  inflating: training-set/question_answering/3/info-units/baselines.json  \n",
            "  inflating: training-set/question_answering/3/info-units/experimental-setup.json  \n",
            "  inflating: training-set/question_answering/3/info-units/model.json  \n",
            "  inflating: training-set/question_answering/3/info-units/research-problem.json  \n",
            "  inflating: training-set/question_answering/3/info-units/results.json  \n",
            "  inflating: training-set/question_answering/3/sentences.txt  \n",
            "   creating: training-set/question_answering/3/triples/\n",
            "  inflating: training-set/question_answering/3/triples/baselines.txt  \n",
            "  inflating: training-set/question_answering/3/triples/experimental-setup.txt  \n",
            "  inflating: training-set/question_answering/3/triples/model.txt  \n",
            "  inflating: training-set/question_answering/3/triples/research-problem.txt  \n",
            "  inflating: training-set/question_answering/3/triples/results.txt  \n",
            "   creating: training-set/question_answering/4/\n",
            "  inflating: training-set/question_answering/4/1811.04210v2-Grobid-out.txt  \n",
            "  inflating: training-set/question_answering/4/1811.04210v2-Stanza-out.txt  \n",
            "  inflating: training-set/question_answering/4/1811.04210v2.pdf  \n",
            "  inflating: training-set/question_answering/4/entities.txt  \n",
            "   creating: training-set/question_answering/4/info-units/\n",
            "  inflating: training-set/question_answering/4/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/question_answering/4/info-units/baselines.json  \n",
            "  inflating: training-set/question_answering/4/info-units/experimental-setup.json  \n",
            "  inflating: training-set/question_answering/4/info-units/model.json  \n",
            "  inflating: training-set/question_answering/4/info-units/research-problem.json  \n",
            "  inflating: training-set/question_answering/4/info-units/results.json  \n",
            "  inflating: training-set/question_answering/4/sentences.txt  \n",
            "   creating: training-set/question_answering/4/triples/\n",
            "  inflating: training-set/question_answering/4/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/question_answering/4/triples/baselines.txt  \n",
            "  inflating: training-set/question_answering/4/triples/experimental-setup.txt  \n",
            "  inflating: training-set/question_answering/4/triples/model.txt  \n",
            "  inflating: training-set/question_answering/4/triples/research-problem.txt  \n",
            "  inflating: training-set/question_answering/4/triples/results.txt  \n",
            "   creating: training-set/question_answering/5/\n",
            "  inflating: training-set/question_answering/5/1711.05116v2-Grobid-out.txt  \n",
            "  inflating: training-set/question_answering/5/1711.05116v2-Stanza-out.txt  \n",
            "  inflating: training-set/question_answering/5/1711.05116v2.pdf  \n",
            "  inflating: training-set/question_answering/5/entities.txt  \n",
            "   creating: training-set/question_answering/5/info-units/\n",
            "  inflating: training-set/question_answering/5/info-units/baselines.json  \n",
            "  inflating: training-set/question_answering/5/info-units/code.json  \n",
            "  inflating: training-set/question_answering/5/info-units/hyperparameters.json  \n",
            "  inflating: training-set/question_answering/5/info-units/model.json  \n",
            "  inflating: training-set/question_answering/5/info-units/research-problem.json  \n",
            "  inflating: training-set/question_answering/5/info-units/results.json  \n",
            "  inflating: training-set/question_answering/5/sentences.txt  \n",
            "   creating: training-set/question_answering/5/triples/\n",
            "  inflating: training-set/question_answering/5/triples/baselines.txt  \n",
            " extracting: training-set/question_answering/5/triples/code.txt  \n",
            "  inflating: training-set/question_answering/5/triples/hyperparameters.txt  \n",
            "  inflating: training-set/question_answering/5/triples/model.txt  \n",
            "  inflating: training-set/question_answering/5/triples/research-problem.txt  \n",
            "  inflating: training-set/question_answering/5/triples/results.txt  \n",
            "   creating: training-set/question_generation/\n",
            "   creating: training-set/question_generation/0/\n",
            "  inflating: training-set/question_generation/0/1704.01792v3-Grobid-out.txt  \n",
            "  inflating: training-set/question_generation/0/1704.01792v3-Stanza-out.txt  \n",
            "  inflating: training-set/question_generation/0/1704.01792v3.pdf  \n",
            "  inflating: training-set/question_generation/0/entities.txt  \n",
            "   creating: training-set/question_generation/0/info-units/\n",
            "  inflating: training-set/question_generation/0/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/question_generation/0/info-units/baselines.json  \n",
            "  inflating: training-set/question_generation/0/info-units/model.json  \n",
            "  inflating: training-set/question_generation/0/info-units/research-problem.json  \n",
            "  inflating: training-set/question_generation/0/info-units/results.json  \n",
            "  inflating: training-set/question_generation/0/sentences.txt  \n",
            "   creating: training-set/question_generation/0/triples/\n",
            "  inflating: training-set/question_generation/0/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/question_generation/0/triples/baselines.txt  \n",
            "  inflating: training-set/question_generation/0/triples/model.txt  \n",
            "  inflating: training-set/question_generation/0/triples/research-problem.txt  \n",
            "  inflating: training-set/question_generation/0/triples/results.txt  \n",
            "   creating: training-set/question_generation/1/\n",
            "  inflating: training-set/question_generation/1/1808.03986v2-Grobid-out.txt  \n",
            "  inflating: training-set/question_generation/1/1808.03986v2-Stanza-out.txt  \n",
            "  inflating: training-set/question_generation/1/1808.03986v2.pdf  \n",
            "  inflating: training-set/question_generation/1/entities.txt  \n",
            "   creating: training-set/question_generation/1/info-units/\n",
            "  inflating: training-set/question_generation/1/info-units/approach.json  \n",
            "  inflating: training-set/question_generation/1/info-units/research-problem.json  \n",
            " extracting: training-set/question_generation/1/sentences.txt  \n",
            "   creating: training-set/question_generation/1/triples/\n",
            "  inflating: training-set/question_generation/1/triples/approach.txt  \n",
            "  inflating: training-set/question_generation/1/triples/research-problem.txt  \n",
            "   creating: training-set/question_similarity/\n",
            "   creating: training-set/question_similarity/0/\n",
            "  inflating: training-set/question_similarity/0/1912.12514v1-Grobid-out.txt  \n",
            "  inflating: training-set/question_similarity/0/1912.12514v1-Stanza-out.txt  \n",
            "  inflating: training-set/question_similarity/0/1912.12514v1.pdf  \n",
            "  inflating: training-set/question_similarity/0/entities.txt  \n",
            "   creating: training-set/question_similarity/0/info-units/\n",
            "  inflating: training-set/question_similarity/0/info-units/experimental-setup.json  \n",
            "  inflating: training-set/question_similarity/0/info-units/model.json  \n",
            "  inflating: training-set/question_similarity/0/info-units/research-problem.json  \n",
            "  inflating: training-set/question_similarity/0/info-units/results.json  \n",
            "  inflating: training-set/question_similarity/0/sentences.txt  \n",
            "   creating: training-set/question_similarity/0/triples/\n",
            "  inflating: training-set/question_similarity/0/triples/experimental-setup.txt  \n",
            "  inflating: training-set/question_similarity/0/triples/model.txt  \n",
            "  inflating: training-set/question_similarity/0/triples/research-problem.txt  \n",
            "  inflating: training-set/question_similarity/0/triples/results.txt  \n",
            "  inflating: training-set/README.md  \n",
            "   creating: training-set/relation_extraction/\n",
            "   creating: training-set/relation_extraction/0/\n",
            "  inflating: training-set/relation_extraction/0/entities.txt  \n",
            "   creating: training-set/relation_extraction/0/info-units/\n",
            "  inflating: training-set/relation_extraction/0/info-units/baselines.json  \n",
            "  inflating: training-set/relation_extraction/0/info-units/hyperparameters.json  \n",
            "  inflating: training-set/relation_extraction/0/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/0/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/0/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/0/P17-1085-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/0/P17-1085-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/0/P17-1085.pdf  \n",
            "  inflating: training-set/relation_extraction/0/sentences.txt  \n",
            "   creating: training-set/relation_extraction/0/triples/\n",
            "  inflating: training-set/relation_extraction/0/triples/baselines.txt  \n",
            "  inflating: training-set/relation_extraction/0/triples/hyperparameters.txt  \n",
            "  inflating: training-set/relation_extraction/0/triples/model.txt  \n",
            "  inflating: training-set/relation_extraction/0/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/0/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/1/\n",
            "   creating: training-set/relation_extraction/10/\n",
            "  inflating: training-set/relation_extraction/10/entities.txt  \n",
            "   creating: training-set/relation_extraction/10/info-units/\n",
            "  inflating: training-set/relation_extraction/10/info-units/hyperparameters.json  \n",
            "  inflating: training-set/relation_extraction/10/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/10/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/10/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/10/P19-1525-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/10/P19-1525-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/10/P19-1525.pdf  \n",
            "  inflating: training-set/relation_extraction/10/sentences.txt  \n",
            "   creating: training-set/relation_extraction/10/triples/\n",
            "  inflating: training-set/relation_extraction/10/triples/hyperparameters.txt  \n",
            "  inflating: training-set/relation_extraction/10/triples/model.txt  \n",
            "  inflating: training-set/relation_extraction/10/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/10/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/11/\n",
            "  inflating: training-set/relation_extraction/11/1812.04361v2-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/11/1812.04361v2-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/11/1812.04361v2.pdf  \n",
            "  inflating: training-set/relation_extraction/11/entities.txt  \n",
            "   creating: training-set/relation_extraction/11/info-units/\n",
            "  inflating: training-set/relation_extraction/11/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/relation_extraction/11/info-units/baselines.json  \n",
            "  inflating: training-set/relation_extraction/11/info-units/code.json  \n",
            "  inflating: training-set/relation_extraction/11/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/11/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/11/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/11/sentences.txt  \n",
            "   creating: training-set/relation_extraction/11/triples/\n",
            "  inflating: training-set/relation_extraction/11/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/relation_extraction/11/triples/baselines.txt  \n",
            " extracting: training-set/relation_extraction/11/triples/code.txt  \n",
            "  inflating: training-set/relation_extraction/11/triples/model.txt  \n",
            "  inflating: training-set/relation_extraction/11/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/11/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/12/\n",
            "  inflating: training-set/relation_extraction/12/1906.07510v5-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/12/1906.07510v5-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/12/1906.07510v5.pdf  \n",
            "  inflating: training-set/relation_extraction/12/entities.txt  \n",
            "   creating: training-set/relation_extraction/12/info-units/\n",
            "  inflating: training-set/relation_extraction/12/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/relation_extraction/12/info-units/baselines.json  \n",
            "  inflating: training-set/relation_extraction/12/info-units/code.json  \n",
            "  inflating: training-set/relation_extraction/12/info-units/hyperparameters.json  \n",
            "  inflating: training-set/relation_extraction/12/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/12/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/12/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/12/sentences.txt  \n",
            "   creating: training-set/relation_extraction/12/triples/\n",
            "  inflating: training-set/relation_extraction/12/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/relation_extraction/12/triples/baselines.txt  \n",
            " extracting: training-set/relation_extraction/12/triples/code.txt  \n",
            "  inflating: training-set/relation_extraction/12/triples/hyperparameters.txt  \n",
            "  inflating: training-set/relation_extraction/12/triples/model.txt  \n",
            " extracting: training-set/relation_extraction/12/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/12/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/13/\n",
            "  inflating: training-set/relation_extraction/13/1906.03158v1-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/13/1906.03158v1-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/13/1906.03158v1.pdf  \n",
            "  inflating: training-set/relation_extraction/13/entities.txt  \n",
            "   creating: training-set/relation_extraction/13/info-units/\n",
            "  inflating: training-set/relation_extraction/13/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/13/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/13/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/13/sentences.txt  \n",
            "   creating: training-set/relation_extraction/13/triples/\n",
            "  inflating: training-set/relation_extraction/13/triples/model.txt  \n",
            "  inflating: training-set/relation_extraction/13/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/13/triples/results.txt  \n",
            "  inflating: training-set/relation_extraction/1/1904.05255v1-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/1/1904.05255v1-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/1/1904.05255v1.pdf  \n",
            "  inflating: training-set/relation_extraction/1/entities.txt  \n",
            "   creating: training-set/relation_extraction/1/info-units/\n",
            "  inflating: training-set/relation_extraction/1/info-units/hyperparameters.json  \n",
            "  inflating: training-set/relation_extraction/1/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/1/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/1/info-units/results.json  \n",
            " extracting: training-set/relation_extraction/1/sentences.txt  \n",
            "   creating: training-set/relation_extraction/1/triples/\n",
            "  inflating: training-set/relation_extraction/1/triples/hyperparameters.txt  \n",
            "  inflating: training-set/relation_extraction/1/triples/model.txt  \n",
            "  inflating: training-set/relation_extraction/1/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/1/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/2/\n",
            "  inflating: training-set/relation_extraction/2/1905.08284v1-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/2/1905.08284v1-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/2/1905.08284v1.pdf  \n",
            "  inflating: training-set/relation_extraction/2/entities.txt  \n",
            "   creating: training-set/relation_extraction/2/info-units/\n",
            "  inflating: training-set/relation_extraction/2/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/relation_extraction/2/info-units/baselines.json  \n",
            "  inflating: training-set/relation_extraction/2/info-units/hyperparameters.json  \n",
            "  inflating: training-set/relation_extraction/2/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/2/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/2/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/2/sentences.txt  \n",
            "   creating: training-set/relation_extraction/2/triples/\n",
            "  inflating: training-set/relation_extraction/2/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/relation_extraction/2/triples/baselines.txt  \n",
            "  inflating: training-set/relation_extraction/2/triples/hyperparameters.txt  \n",
            "  inflating: training-set/relation_extraction/2/triples/model.txt  \n",
            " extracting: training-set/relation_extraction/2/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/2/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/3/\n",
            "  inflating: training-set/relation_extraction/3/1902.01030v2-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/3/1902.01030v2-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/3/1902.01030v2.pdf  \n",
            "  inflating: training-set/relation_extraction/3/entities.txt  \n",
            "   creating: training-set/relation_extraction/3/info-units/\n",
            "  inflating: training-set/relation_extraction/3/info-units/baselines.json  \n",
            "  inflating: training-set/relation_extraction/3/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/3/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/3/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/3/sentences.txt  \n",
            "   creating: training-set/relation_extraction/3/triples/\n",
            "  inflating: training-set/relation_extraction/3/triples/baselines.txt  \n",
            "  inflating: training-set/relation_extraction/3/triples/model.txt  \n",
            "  inflating: training-set/relation_extraction/3/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/3/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/4/\n",
            "  inflating: training-set/relation_extraction/4/D17-1004-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/4/D17-1004-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/4/D17-1004.pdf  \n",
            "  inflating: training-set/relation_extraction/4/entities.txt  \n",
            "   creating: training-set/relation_extraction/4/info-units/\n",
            "  inflating: training-set/relation_extraction/4/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/relation_extraction/4/info-units/dataset.json  \n",
            "  inflating: training-set/relation_extraction/4/info-units/hyperparameters.json  \n",
            "  inflating: training-set/relation_extraction/4/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/4/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/4/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/4/sentences.txt  \n",
            "   creating: training-set/relation_extraction/4/triples/\n",
            "  inflating: training-set/relation_extraction/4/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/relation_extraction/4/triples/dataset.txt  \n",
            "  inflating: training-set/relation_extraction/4/triples/hyperparameters.txt  \n",
            "  inflating: training-set/relation_extraction/4/triples/model.txt  \n",
            "  inflating: training-set/relation_extraction/4/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/4/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/5/\n",
            "  inflating: training-set/relation_extraction/5/1809.10185v1-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/5/1809.10185v1-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/5/1809.10185v1.pdf  \n",
            "  inflating: training-set/relation_extraction/5/entities.txt  \n",
            "   creating: training-set/relation_extraction/5/info-units/\n",
            "  inflating: training-set/relation_extraction/5/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/relation_extraction/5/info-units/baselines.json  \n",
            "  inflating: training-set/relation_extraction/5/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/5/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/5/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/5/sentences.txt  \n",
            "   creating: training-set/relation_extraction/5/triples/\n",
            "  inflating: training-set/relation_extraction/5/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/relation_extraction/5/triples/baselines.txt  \n",
            "  inflating: training-set/relation_extraction/5/triples/model.txt  \n",
            " extracting: training-set/relation_extraction/5/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/5/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/6/\n",
            "  inflating: training-set/relation_extraction/6/D17-1188-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/6/D17-1188-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/6/D17-1188.pdf  \n",
            "  inflating: training-set/relation_extraction/6/entities.txt  \n",
            "   creating: training-set/relation_extraction/6/info-units/\n",
            "  inflating: training-set/relation_extraction/6/info-units/hyperparameters.json  \n",
            "  inflating: training-set/relation_extraction/6/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/6/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/6/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/6/sentences.txt  \n",
            "   creating: training-set/relation_extraction/6/triples/\n",
            "  inflating: training-set/relation_extraction/6/triples/hyperparameters.txt  \n",
            "  inflating: training-set/relation_extraction/6/triples/model.txt  \n",
            "  inflating: training-set/relation_extraction/6/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/6/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/7/\n",
            "  inflating: training-set/relation_extraction/7/1808.06738v2-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/7/1808.06738v2-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/7/1808.06738v2.pdf  \n",
            "  inflating: training-set/relation_extraction/7/entities.txt  \n",
            "   creating: training-set/relation_extraction/7/info-units/\n",
            "  inflating: training-set/relation_extraction/7/info-units/approach.json  \n",
            "  inflating: training-set/relation_extraction/7/info-units/baselines.json  \n",
            "  inflating: training-set/relation_extraction/7/info-units/experimental-setup.json  \n",
            "  inflating: training-set/relation_extraction/7/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/7/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/7/sentences.txt  \n",
            "   creating: training-set/relation_extraction/7/triples/\n",
            "  inflating: training-set/relation_extraction/7/triples/approach.txt  \n",
            "  inflating: training-set/relation_extraction/7/triples/baselines.txt  \n",
            "  inflating: training-set/relation_extraction/7/triples/experimental-setup.txt  \n",
            "  inflating: training-set/relation_extraction/7/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/7/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/8/\n",
            "  inflating: training-set/relation_extraction/8/D15-1203-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/8/D15-1203-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/8/D15-1203.pdf  \n",
            "  inflating: training-set/relation_extraction/8/entities.txt  \n",
            "   creating: training-set/relation_extraction/8/info-units/\n",
            "  inflating: training-set/relation_extraction/8/info-units/baselines.json  \n",
            "  inflating: training-set/relation_extraction/8/info-units/experimental-setup.json  \n",
            "  inflating: training-set/relation_extraction/8/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/8/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/8/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/8/sentences.txt  \n",
            "   creating: training-set/relation_extraction/8/triples/\n",
            "  inflating: training-set/relation_extraction/8/triples/baselines.txt  \n",
            "  inflating: training-set/relation_extraction/8/triples/experimental-setup.txt  \n",
            "  inflating: training-set/relation_extraction/8/triples/model.txt  \n",
            "  inflating: training-set/relation_extraction/8/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/8/triples/results.txt  \n",
            "   creating: training-set/relation_extraction/9/\n",
            "  inflating: training-set/relation_extraction/9/entities.txt  \n",
            "   creating: training-set/relation_extraction/9/info-units/\n",
            "  inflating: training-set/relation_extraction/9/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/relation_extraction/9/info-units/experimental-setup.json  \n",
            "  inflating: training-set/relation_extraction/9/info-units/model.json  \n",
            "  inflating: training-set/relation_extraction/9/info-units/research-problem.json  \n",
            "  inflating: training-set/relation_extraction/9/info-units/results.json  \n",
            "  inflating: training-set/relation_extraction/9/P16-1123-Grobid-out.txt  \n",
            "  inflating: training-set/relation_extraction/9/P16-1123-Stanza-out.txt  \n",
            "  inflating: training-set/relation_extraction/9/P16-1123.pdf  \n",
            "  inflating: training-set/relation_extraction/9/sentences.txt  \n",
            "   creating: training-set/relation_extraction/9/triples/\n",
            "  inflating: training-set/relation_extraction/9/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/relation_extraction/9/triples/experimental-setup.txt  \n",
            "  inflating: training-set/relation_extraction/9/triples/model.txt  \n",
            " extracting: training-set/relation_extraction/9/triples/research-problem.txt  \n",
            "  inflating: training-set/relation_extraction/9/triples/results.txt  \n",
            "   creating: training-set/sarcasm_detection/\n",
            "   creating: training-set/sarcasm_detection/0/\n",
            "  inflating: training-set/sarcasm_detection/0/1704.05579v4-Grobid-out.txt  \n",
            "  inflating: training-set/sarcasm_detection/0/1704.05579v4-Stanza-out.txt  \n",
            "  inflating: training-set/sarcasm_detection/0/1704.05579v4.pdf  \n",
            "  inflating: training-set/sarcasm_detection/0/entities.txt  \n",
            "   creating: training-set/sarcasm_detection/0/info-units/\n",
            "  inflating: training-set/sarcasm_detection/0/info-units/code.json  \n",
            "  inflating: training-set/sarcasm_detection/0/info-units/dataset.json  \n",
            "  inflating: training-set/sarcasm_detection/0/info-units/research-problem.json  \n",
            "  inflating: training-set/sarcasm_detection/0/info-units/results.json  \n",
            " extracting: training-set/sarcasm_detection/0/sentences.txt  \n",
            "   creating: training-set/sarcasm_detection/0/triples/\n",
            " extracting: training-set/sarcasm_detection/0/triples/code.txt  \n",
            "  inflating: training-set/sarcasm_detection/0/triples/dataset.txt  \n",
            " extracting: training-set/sarcasm_detection/0/triples/research-problem.txt  \n",
            "  inflating: training-set/sarcasm_detection/0/triples/results.txt  \n",
            "   creating: training-set/sarcasm_detection/1/\n",
            "  inflating: training-set/sarcasm_detection/1/1805.06413v1-Grobid-out.txt  \n",
            "  inflating: training-set/sarcasm_detection/1/1805.06413v1-Stanza-out.txt  \n",
            "  inflating: training-set/sarcasm_detection/1/1805.06413v1.pdf  \n",
            "  inflating: training-set/sarcasm_detection/1/entities.txt  \n",
            "   creating: training-set/sarcasm_detection/1/info-units/\n",
            "  inflating: training-set/sarcasm_detection/1/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sarcasm_detection/1/info-units/baselines.json  \n",
            "  inflating: training-set/sarcasm_detection/1/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sarcasm_detection/1/info-units/model.json  \n",
            "  inflating: training-set/sarcasm_detection/1/info-units/research-problem.json  \n",
            "  inflating: training-set/sarcasm_detection/1/info-units/results.json  \n",
            "  inflating: training-set/sarcasm_detection/1/sentences.txt  \n",
            "   creating: training-set/sarcasm_detection/1/triples/\n",
            "  inflating: training-set/sarcasm_detection/1/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sarcasm_detection/1/triples/baselines.txt  \n",
            "  inflating: training-set/sarcasm_detection/1/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sarcasm_detection/1/triples/model.txt  \n",
            "  inflating: training-set/sarcasm_detection/1/triples/research-problem.txt  \n",
            "  inflating: training-set/sarcasm_detection/1/triples/results.txt  \n",
            "   creating: training-set/semantic_parsing/\n",
            "   creating: training-set/semantic_parsing/0/\n",
            "  inflating: training-set/semantic_parsing/0/1809.08887v5-Grobid-out.txt  \n",
            "  inflating: training-set/semantic_parsing/0/1809.08887v5-Stanza-out.txt  \n",
            "  inflating: training-set/semantic_parsing/0/1809.08887v5.pdf  \n",
            "  inflating: training-set/semantic_parsing/0/entities.txt  \n",
            "   creating: training-set/semantic_parsing/0/info-units/\n",
            "  inflating: training-set/semantic_parsing/0/info-units/code.json  \n",
            "  inflating: training-set/semantic_parsing/0/info-units/dataset.json  \n",
            "  inflating: training-set/semantic_parsing/0/info-units/research-problem.json  \n",
            "  inflating: training-set/semantic_parsing/0/info-units/results.json  \n",
            "  inflating: training-set/semantic_parsing/0/sentences.txt  \n",
            "   creating: training-set/semantic_parsing/0/triples/\n",
            " extracting: training-set/semantic_parsing/0/triples/code.txt  \n",
            "  inflating: training-set/semantic_parsing/0/triples/dataset.txt  \n",
            "  inflating: training-set/semantic_parsing/0/triples/research-problem.txt  \n",
            "  inflating: training-set/semantic_parsing/0/triples/results.txt  \n",
            "   creating: training-set/semantic_parsing/1/\n",
            "  inflating: training-set/semantic_parsing/1/1810.02720v1-Grobid-out.txt  \n",
            "  inflating: training-set/semantic_parsing/1/1810.02720v1-Stanza-out.txt  \n",
            "  inflating: training-set/semantic_parsing/1/1810.02720v1.pdf  \n",
            "  inflating: training-set/semantic_parsing/1/entities.txt  \n",
            "   creating: training-set/semantic_parsing/1/info-units/\n",
            "  inflating: training-set/semantic_parsing/1/info-units/model.json  \n",
            "  inflating: training-set/semantic_parsing/1/info-units/research-problem.json  \n",
            "  inflating: training-set/semantic_parsing/1/info-units/results.json  \n",
            "  inflating: training-set/semantic_parsing/1/sentences.txt  \n",
            "   creating: training-set/semantic_parsing/1/triples/\n",
            "  inflating: training-set/semantic_parsing/1/triples/model.txt  \n",
            "  inflating: training-set/semantic_parsing/1/triples/research-problem.txt  \n",
            "  inflating: training-set/semantic_parsing/1/triples/results.txt  \n",
            "   creating: training-set/semantic_parsing/2/\n",
            "  inflating: training-set/semantic_parsing/2/1805.04793v1-Grobid-out.txt  \n",
            "  inflating: training-set/semantic_parsing/2/1805.04793v1-Stanza-out.txt  \n",
            "  inflating: training-set/semantic_parsing/2/1805.04793v1.pdf  \n",
            "  inflating: training-set/semantic_parsing/2/entities.txt  \n",
            "   creating: training-set/semantic_parsing/2/info-units/\n",
            "  inflating: training-set/semantic_parsing/2/info-units/code.json  \n",
            "  inflating: training-set/semantic_parsing/2/info-units/hyperparameters.json  \n",
            "  inflating: training-set/semantic_parsing/2/info-units/model.json  \n",
            "  inflating: training-set/semantic_parsing/2/info-units/research-problem.json  \n",
            "  inflating: training-set/semantic_parsing/2/info-units/results.json  \n",
            "  inflating: training-set/semantic_parsing/2/sentences.txt  \n",
            "   creating: training-set/semantic_parsing/2/triples/\n",
            " extracting: training-set/semantic_parsing/2/triples/code.txt  \n",
            "  inflating: training-set/semantic_parsing/2/triples/hyperparameters.txt  \n",
            "  inflating: training-set/semantic_parsing/2/triples/model.txt  \n",
            "  inflating: training-set/semantic_parsing/2/triples/research-problem.txt  \n",
            "  inflating: training-set/semantic_parsing/2/triples/results.txt  \n",
            "   creating: training-set/semantic_role_labeling/\n",
            "   creating: training-set/semantic_role_labeling/0/\n",
            "  inflating: training-set/semantic_role_labeling/0/entities.txt  \n",
            "   creating: training-set/semantic_role_labeling/0/info-units/\n",
            "  inflating: training-set/semantic_role_labeling/0/info-units/code.json  \n",
            "  inflating: training-set/semantic_role_labeling/0/info-units/model.json  \n",
            "  inflating: training-set/semantic_role_labeling/0/info-units/research-problem.json  \n",
            "  inflating: training-set/semantic_role_labeling/0/info-units/results.json  \n",
            "  inflating: training-set/semantic_role_labeling/0/P18-2058-Grobid-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/0/P18-2058-Stanza-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/0/P18-2058.pdf  \n",
            "  inflating: training-set/semantic_role_labeling/0/sentences.txt  \n",
            "   creating: training-set/semantic_role_labeling/0/triples/\n",
            " extracting: training-set/semantic_role_labeling/0/triples/code.txt  \n",
            "  inflating: training-set/semantic_role_labeling/0/triples/model.txt  \n",
            "  inflating: training-set/semantic_role_labeling/0/triples/research-problem.txt  \n",
            "  inflating: training-set/semantic_role_labeling/0/triples/results.txt  \n",
            "   creating: training-set/semantic_role_labeling/1/\n",
            "  inflating: training-set/semantic_role_labeling/1/1804.08199v3-Grobid-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/1/1804.08199v3-Stanza-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/1/1804.08199v3.pdf  \n",
            "  inflating: training-set/semantic_role_labeling/1/entities.txt  \n",
            "   creating: training-set/semantic_role_labeling/1/info-units/\n",
            "  inflating: training-set/semantic_role_labeling/1/info-units/hyperparameters.json  \n",
            "  inflating: training-set/semantic_role_labeling/1/info-units/model.json  \n",
            "  inflating: training-set/semantic_role_labeling/1/info-units/research-problem.json  \n",
            "  inflating: training-set/semantic_role_labeling/1/info-units/results.json  \n",
            "  inflating: training-set/semantic_role_labeling/1/sentences.txt  \n",
            "   creating: training-set/semantic_role_labeling/1/triples/\n",
            "  inflating: training-set/semantic_role_labeling/1/triples/hyperparameters.txt  \n",
            "  inflating: training-set/semantic_role_labeling/1/triples/model.txt  \n",
            "  inflating: training-set/semantic_role_labeling/1/triples/research-problem.txt  \n",
            "  inflating: training-set/semantic_role_labeling/1/triples/results.txt  \n",
            "   creating: training-set/semantic_role_labeling/2/\n",
            "  inflating: training-set/semantic_role_labeling/2/entities.txt  \n",
            "   creating: training-set/semantic_role_labeling/2/info-units/\n",
            "  inflating: training-set/semantic_role_labeling/2/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/semantic_role_labeling/2/info-units/hyperparameters.json  \n",
            "  inflating: training-set/semantic_role_labeling/2/info-units/model.json  \n",
            "  inflating: training-set/semantic_role_labeling/2/info-units/research-problem.json  \n",
            "  inflating: training-set/semantic_role_labeling/2/info-units/results.json  \n",
            "  inflating: training-set/semantic_role_labeling/2/P17-1044-Grobid-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/2/P17-1044-Stanza-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/2/P17-1044.pdf  \n",
            "  inflating: training-set/semantic_role_labeling/2/sentences.txt  \n",
            "   creating: training-set/semantic_role_labeling/2/triples/\n",
            "  inflating: training-set/semantic_role_labeling/2/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/semantic_role_labeling/2/triples/hyperparameters.txt  \n",
            "  inflating: training-set/semantic_role_labeling/2/triples/model.txt  \n",
            "  inflating: training-set/semantic_role_labeling/2/triples/research-problem.txt  \n",
            "  inflating: training-set/semantic_role_labeling/2/triples/results.txt  \n",
            "   creating: training-set/semantic_role_labeling/3/\n",
            "  inflating: training-set/semantic_role_labeling/3/1712.01586v1-Grobid-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/3/1712.01586v1-Stanza-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/3/1712.01586v1.pdf  \n",
            "  inflating: training-set/semantic_role_labeling/3/entities.txt  \n",
            "   creating: training-set/semantic_role_labeling/3/info-units/\n",
            "  inflating: training-set/semantic_role_labeling/3/info-units/hyperparameters.json  \n",
            "  inflating: training-set/semantic_role_labeling/3/info-units/model.json  \n",
            "  inflating: training-set/semantic_role_labeling/3/info-units/research-problem.json  \n",
            "  inflating: training-set/semantic_role_labeling/3/info-units/results.json  \n",
            "  inflating: training-set/semantic_role_labeling/3/sentences.txt  \n",
            "   creating: training-set/semantic_role_labeling/3/triples/\n",
            "  inflating: training-set/semantic_role_labeling/3/triples/hyperparameters.txt  \n",
            "  inflating: training-set/semantic_role_labeling/3/triples/model.txt  \n",
            "  inflating: training-set/semantic_role_labeling/3/triples/research-problem.txt  \n",
            "  inflating: training-set/semantic_role_labeling/3/triples/results.txt  \n",
            "   creating: training-set/semantic_role_labeling/4/\n",
            "  inflating: training-set/semantic_role_labeling/4/1810.02245v1-Grobid-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/4/1810.02245v1-Stanza-out.txt  \n",
            "  inflating: training-set/semantic_role_labeling/4/1810.02245v1.pdf  \n",
            "  inflating: training-set/semantic_role_labeling/4/entities.txt  \n",
            "   creating: training-set/semantic_role_labeling/4/info-units/\n",
            "  inflating: training-set/semantic_role_labeling/4/info-units/baselines.json  \n",
            "  inflating: training-set/semantic_role_labeling/4/info-units/hyperparameters.json  \n",
            "  inflating: training-set/semantic_role_labeling/4/info-units/model.json  \n",
            "  inflating: training-set/semantic_role_labeling/4/info-units/research-problem.json  \n",
            "  inflating: training-set/semantic_role_labeling/4/info-units/results.json  \n",
            "  inflating: training-set/semantic_role_labeling/4/sentences.txt  \n",
            "   creating: training-set/semantic_role_labeling/4/triples/\n",
            " extracting: training-set/semantic_role_labeling/4/triples/baselines.txt  \n",
            "  inflating: training-set/semantic_role_labeling/4/triples/hyperparameters.txt  \n",
            "  inflating: training-set/semantic_role_labeling/4/triples/model.txt  \n",
            "  inflating: training-set/semantic_role_labeling/4/triples/research-problem.txt  \n",
            "  inflating: training-set/semantic_role_labeling/4/triples/results.txt  \n",
            "   creating: training-set/sentence_classification/\n",
            "   creating: training-set/sentence_classification/0/\n",
            "  inflating: training-set/sentence_classification/0/1904.01608v2-Grobid-out.txt  \n",
            "  inflating: training-set/sentence_classification/0/1904.01608v2-Stanza-out.txt  \n",
            "  inflating: training-set/sentence_classification/0/1904.01608v2.pdf  \n",
            "  inflating: training-set/sentence_classification/0/entities.txt  \n",
            "   creating: training-set/sentence_classification/0/info-units/\n",
            "  inflating: training-set/sentence_classification/0/info-units/baselines.json  \n",
            "  inflating: training-set/sentence_classification/0/info-units/code.json  \n",
            "  inflating: training-set/sentence_classification/0/info-units/dataset.json  \n",
            "  inflating: training-set/sentence_classification/0/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentence_classification/0/info-units/model.json  \n",
            "  inflating: training-set/sentence_classification/0/info-units/research-problem.json  \n",
            "  inflating: training-set/sentence_classification/0/info-units/results.json  \n",
            "  inflating: training-set/sentence_classification/0/sentences.txt  \n",
            "   creating: training-set/sentence_classification/0/triples/\n",
            "  inflating: training-set/sentence_classification/0/triples/baselines.txt  \n",
            " extracting: training-set/sentence_classification/0/triples/code.txt  \n",
            "  inflating: training-set/sentence_classification/0/triples/dataset.txt  \n",
            "  inflating: training-set/sentence_classification/0/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentence_classification/0/triples/model.txt  \n",
            "  inflating: training-set/sentence_classification/0/triples/research-problem.txt  \n",
            "  inflating: training-set/sentence_classification/0/triples/results.txt  \n",
            "   creating: training-set/sentence_classification/1/\n",
            "  inflating: training-set/sentence_classification/1/1808.06161v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentence_classification/1/1808.06161v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentence_classification/1/1808.06161v1.pdf  \n",
            "  inflating: training-set/sentence_classification/1/entities.txt  \n",
            "   creating: training-set/sentence_classification/1/info-units/\n",
            "  inflating: training-set/sentence_classification/1/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentence_classification/1/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentence_classification/1/info-units/model.json  \n",
            "  inflating: training-set/sentence_classification/1/info-units/research-problem.json  \n",
            "  inflating: training-set/sentence_classification/1/sentences.txt  \n",
            "   creating: training-set/sentence_classification/1/triples/\n",
            "  inflating: training-set/sentence_classification/1/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentence_classification/1/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentence_classification/1/triples/model.txt  \n",
            "  inflating: training-set/sentence_classification/1/triples/research-problem.txt  \n",
            "   creating: training-set/sentence_classification/2/\n",
            "  inflating: training-set/sentence_classification/2/1806.05516v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentence_classification/2/1806.05516v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentence_classification/2/1806.05516v1.pdf  \n",
            "  inflating: training-set/sentence_classification/2/entities.txt  \n",
            "   creating: training-set/sentence_classification/2/info-units/\n",
            "  inflating: training-set/sentence_classification/2/info-units/approach.json  \n",
            "  inflating: training-set/sentence_classification/2/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentence_classification/2/info-units/research-problem.json  \n",
            "  inflating: training-set/sentence_classification/2/info-units/results.json  \n",
            "  inflating: training-set/sentence_classification/2/sentences.txt  \n",
            "   creating: training-set/sentence_classification/2/triples/\n",
            "  inflating: training-set/sentence_classification/2/triples/approach.txt  \n",
            "  inflating: training-set/sentence_classification/2/triples/hyperparameters.txt  \n",
            " extracting: training-set/sentence_classification/2/triples/research-problem.txt  \n",
            "  inflating: training-set/sentence_classification/2/triples/results.txt  \n",
            "   creating: training-set/sentence_compression/\n",
            "   creating: training-set/sentence_compression/0/\n",
            "  inflating: training-set/sentence_compression/0/entities.txt  \n",
            "   creating: training-set/sentence_compression/0/info-units/\n",
            "  inflating: training-set/sentence_compression/0/info-units/baselines.json  \n",
            "  inflating: training-set/sentence_compression/0/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentence_compression/0/info-units/model.json  \n",
            "  inflating: training-set/sentence_compression/0/info-units/research-problem.json  \n",
            "  inflating: training-set/sentence_compression/0/info-units/results.json  \n",
            "  inflating: training-set/sentence_compression/0/P17-1127-Grobid-out.txt  \n",
            "  inflating: training-set/sentence_compression/0/P17-1127-Stanza-out.txt  \n",
            "  inflating: training-set/sentence_compression/0/P17-1127.pdf  \n",
            "  inflating: training-set/sentence_compression/0/sentences.txt  \n",
            "   creating: training-set/sentence_compression/0/triples/\n",
            "  inflating: training-set/sentence_compression/0/triples/baselines.txt  \n",
            "  inflating: training-set/sentence_compression/0/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentence_compression/0/triples/model.txt  \n",
            " extracting: training-set/sentence_compression/0/triples/research-problem.txt  \n",
            "  inflating: training-set/sentence_compression/0/triples/results.txt  \n",
            "   creating: training-set/sentence_compression/1/\n",
            "  inflating: training-set/sentence_compression/1/D15-1042-Grobid-out.txt  \n",
            "  inflating: training-set/sentence_compression/1/D15-1042-Stanza-out.txt  \n",
            "  inflating: training-set/sentence_compression/1/D15-1042.pdf  \n",
            "  inflating: training-set/sentence_compression/1/entities.txt  \n",
            "   creating: training-set/sentence_compression/1/info-units/\n",
            "  inflating: training-set/sentence_compression/1/info-units/model.json  \n",
            "  inflating: training-set/sentence_compression/1/info-units/research-problem.json  \n",
            "  inflating: training-set/sentence_compression/1/info-units/results.json  \n",
            " extracting: training-set/sentence_compression/1/sentences.txt  \n",
            "   creating: training-set/sentence_compression/1/triples/\n",
            "  inflating: training-set/sentence_compression/1/triples/model.txt  \n",
            "  inflating: training-set/sentence_compression/1/triples/research-problem.txt  \n",
            "  inflating: training-set/sentence_compression/1/triples/results.txt  \n",
            "   creating: training-set/sentence_compression/2/\n",
            "  inflating: training-set/sentence_compression/2/1604.03357v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentence_compression/2/1604.03357v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentence_compression/2/1604.03357v1.pdf  \n",
            "  inflating: training-set/sentence_compression/2/entities.txt  \n",
            "   creating: training-set/sentence_compression/2/info-units/\n",
            "  inflating: training-set/sentence_compression/2/info-units/baselines.json  \n",
            "  inflating: training-set/sentence_compression/2/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentence_compression/2/info-units/model.json  \n",
            "  inflating: training-set/sentence_compression/2/info-units/research-problem.json  \n",
            "  inflating: training-set/sentence_compression/2/info-units/results.json  \n",
            "  inflating: training-set/sentence_compression/2/sentences.txt  \n",
            "   creating: training-set/sentence_compression/2/triples/\n",
            "  inflating: training-set/sentence_compression/2/triples/baselines.txt  \n",
            "  inflating: training-set/sentence_compression/2/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentence_compression/2/triples/model.txt  \n",
            " extracting: training-set/sentence_compression/2/triples/research-problem.txt  \n",
            "  inflating: training-set/sentence_compression/2/triples/results.txt  \n",
            "   creating: training-set/sentence_compression/3/\n",
            "  inflating: training-set/sentence_compression/3/entities.txt  \n",
            "   creating: training-set/sentence_compression/3/info-units/\n",
            "  inflating: training-set/sentence_compression/3/info-units/baselines.json  \n",
            "  inflating: training-set/sentence_compression/3/info-units/code.json  \n",
            "  inflating: training-set/sentence_compression/3/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentence_compression/3/info-units/model.json  \n",
            "  inflating: training-set/sentence_compression/3/info-units/research-problem.json  \n",
            "  inflating: training-set/sentence_compression/3/info-units/results.json  \n",
            "  inflating: training-set/sentence_compression/3/P18-2028-Grobid-out.txt  \n",
            "  inflating: training-set/sentence_compression/3/P18-2028-Stanza-out.txt  \n",
            "  inflating: training-set/sentence_compression/3/P18-2028.pdf  \n",
            "  inflating: training-set/sentence_compression/3/sentences.txt  \n",
            "   creating: training-set/sentence_compression/3/triples/\n",
            "  inflating: training-set/sentence_compression/3/triples/baselines.txt  \n",
            " extracting: training-set/sentence_compression/3/triples/code.txt  \n",
            "  inflating: training-set/sentence_compression/3/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentence_compression/3/triples/model.txt  \n",
            "  inflating: training-set/sentence_compression/3/triples/research-problem.txt  \n",
            "  inflating: training-set/sentence_compression/3/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/\n",
            "   creating: training-set/sentiment_analysis/0/\n",
            "  inflating: training-set/sentiment_analysis/0/1810.04635v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/0/1810.04635v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/0/1810.04635v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/0/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/0/info-units/\n",
            "  inflating: training-set/sentiment_analysis/0/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentiment_analysis/0/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/0/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/0/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/0/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/0/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/0/triples/\n",
            "  inflating: training-set/sentiment_analysis/0/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentiment_analysis/0/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/0/triples/model.txt  \n",
            " extracting: training-set/sentiment_analysis/0/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/0/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/1/\n",
            "   creating: training-set/sentiment_analysis/10/\n",
            "  inflating: training-set/sentiment_analysis/10/1811.00405v4-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/10/1811.00405v4-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/10/1811.00405v4.pdf  \n",
            "  inflating: training-set/sentiment_analysis/10/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/10/info-units/\n",
            "  inflating: training-set/sentiment_analysis/10/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/10/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/10/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/10/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/10/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/10/triples/\n",
            "  inflating: training-set/sentiment_analysis/10/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/10/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/10/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/10/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/11/\n",
            "  inflating: training-set/sentiment_analysis/11/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/11/info-units/\n",
            "  inflating: training-set/sentiment_analysis/11/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/11/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/11/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/11/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/11/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/11/N18-1193-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/11/N18-1193-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/11/N18-1193.pdf  \n",
            "  inflating: training-set/sentiment_analysis/11/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/11/triples/\n",
            "  inflating: training-set/sentiment_analysis/11/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/11/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/11/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/11/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/11/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/12/\n",
            "  inflating: training-set/sentiment_analysis/12/1906.01213v3-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/12/1906.01213v3-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/12/1906.01213v3.pdf  \n",
            "  inflating: training-set/sentiment_analysis/12/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/12/info-units/\n",
            "  inflating: training-set/sentiment_analysis/12/info-units/approach.json  \n",
            "  inflating: training-set/sentiment_analysis/12/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/12/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/12/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/12/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/12/triples/\n",
            "  inflating: training-set/sentiment_analysis/12/triples/approach.txt  \n",
            "  inflating: training-set/sentiment_analysis/12/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/12/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/12/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/13/\n",
            "  inflating: training-set/sentiment_analysis/13/1904.02232v2-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/13/1904.02232v2-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/13/1904.02232v2.pdf  \n",
            "  inflating: training-set/sentiment_analysis/13/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/13/info-units/\n",
            "  inflating: training-set/sentiment_analysis/13/info-units/dataset.json  \n",
            "  inflating: training-set/sentiment_analysis/13/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/13/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/13/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/13/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/13/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/13/triples/\n",
            "  inflating: training-set/sentiment_analysis/13/triples/dataset.txt  \n",
            "  inflating: training-set/sentiment_analysis/13/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/13/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/13/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/13/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/14/\n",
            "  inflating: training-set/sentiment_analysis/14/1809.04505v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/14/1809.04505v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/14/1809.04505v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/14/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/14/info-units/\n",
            "  inflating: training-set/sentiment_analysis/14/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/14/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/14/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/14/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/14/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/14/triples/\n",
            "  inflating: training-set/sentiment_analysis/14/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/14/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/14/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/14/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/15/\n",
            "  inflating: training-set/sentiment_analysis/15/D13-1170-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/15/D13-1170-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/15/D13-1170.pdf  \n",
            "  inflating: training-set/sentiment_analysis/15/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/15/info-units/\n",
            "  inflating: training-set/sentiment_analysis/15/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentiment_analysis/15/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/15/info-units/dataset.json  \n",
            "  inflating: training-set/sentiment_analysis/15/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/15/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/15/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/15/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/15/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/15/triples/\n",
            "  inflating: training-set/sentiment_analysis/15/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentiment_analysis/15/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/15/triples/dataset.txt  \n",
            "  inflating: training-set/sentiment_analysis/15/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/15/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/15/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/15/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/16/\n",
            "  inflating: training-set/sentiment_analysis/16/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/16/info-units/\n",
            "  inflating: training-set/sentiment_analysis/16/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/16/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/16/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/16/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/16/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/16/P18-1088-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/16/P18-1088-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/16/P18-1088.pdf  \n",
            "  inflating: training-set/sentiment_analysis/16/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/16/triples/\n",
            "  inflating: training-set/sentiment_analysis/16/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/16/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/16/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/16/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/16/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/17/\n",
            "  inflating: training-set/sentiment_analysis/17/1503.00075v3-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/17/1503.00075v3-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/17/1503.00075v3.pdf  \n",
            "  inflating: training-set/sentiment_analysis/17/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/17/info-units/\n",
            "  inflating: training-set/sentiment_analysis/17/info-units/code.json  \n",
            "  inflating: training-set/sentiment_analysis/17/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/17/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/17/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/17/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/17/triples/\n",
            " extracting: training-set/sentiment_analysis/17/triples/code.txt  \n",
            "  inflating: training-set/sentiment_analysis/17/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/17/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/17/triples/research-problem.txt  \n",
            "   creating: training-set/sentiment_analysis/18/\n",
            "  inflating: training-set/sentiment_analysis/18/C18-1096-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/18/C18-1096-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/18/C18-1096.pdf  \n",
            "  inflating: training-set/sentiment_analysis/18/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/18/info-units/\n",
            "  inflating: training-set/sentiment_analysis/18/info-units/approach.json  \n",
            "  inflating: training-set/sentiment_analysis/18/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/18/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/18/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/18/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/18/triples/\n",
            "  inflating: training-set/sentiment_analysis/18/triples/approach.txt  \n",
            "  inflating: training-set/sentiment_analysis/18/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/18/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/18/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/19/\n",
            "  inflating: training-set/sentiment_analysis/19/1805.07340v2-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/19/1805.07340v2-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/19/1805.07340v2.pdf  \n",
            "  inflating: training-set/sentiment_analysis/19/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/19/info-units/\n",
            "  inflating: training-set/sentiment_analysis/19/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/19/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/19/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/19/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/19/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/19/triples/\n",
            "  inflating: training-set/sentiment_analysis/19/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/19/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/19/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/19/triples/results.txt  \n",
            "  inflating: training-set/sentiment_analysis/1/1907.07835v2-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/1/1907.07835v2-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/1/1907.07835v2.pdf  \n",
            "  inflating: training-set/sentiment_analysis/1/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/1/info-units/\n",
            "  inflating: training-set/sentiment_analysis/1/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentiment_analysis/1/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/1/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/1/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/1/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/1/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/1/triples/\n",
            "  inflating: training-set/sentiment_analysis/1/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentiment_analysis/1/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/1/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/1/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/1/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/2/\n",
            "   creating: training-set/sentiment_analysis/20/\n",
            "  inflating: training-set/sentiment_analysis/20/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/20/info-units/\n",
            "  inflating: training-set/sentiment_analysis/20/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/20/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/20/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/20/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/20/S17-2126-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/20/S17-2126-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/20/S17-2126.pdf  \n",
            "  inflating: training-set/sentiment_analysis/20/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/20/triples/\n",
            "  inflating: training-set/sentiment_analysis/20/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/20/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/20/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/20/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/21/\n",
            "  inflating: training-set/sentiment_analysis/21/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/21/info-units/\n",
            "  inflating: training-set/sentiment_analysis/21/info-units/code.json  \n",
            "  inflating: training-set/sentiment_analysis/21/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/21/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/21/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/21/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/21/P19-2057-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/21/P19-2057-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/21/P19-2057.pdf  \n",
            "  inflating: training-set/sentiment_analysis/21/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/21/triples/\n",
            " extracting: training-set/sentiment_analysis/21/triples/code.txt  \n",
            "  inflating: training-set/sentiment_analysis/21/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/21/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/21/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/21/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/22/\n",
            "  inflating: training-set/sentiment_analysis/22/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/22/info-units/\n",
            "  inflating: training-set/sentiment_analysis/22/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/22/info-units/code.json  \n",
            "  inflating: training-set/sentiment_analysis/22/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/22/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/22/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/22/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/22/K18-1018-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/22/K18-1018-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/22/K18-1018.pdf  \n",
            "  inflating: training-set/sentiment_analysis/22/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/22/triples/\n",
            "  inflating: training-set/sentiment_analysis/22/triples/baselines.txt  \n",
            " extracting: training-set/sentiment_analysis/22/triples/code.txt  \n",
            "  inflating: training-set/sentiment_analysis/22/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/22/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/22/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/22/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/23/\n",
            "  inflating: training-set/sentiment_analysis/23/1504.01106v5-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/23/1504.01106v5-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/23/1504.01106v5.pdf  \n",
            "  inflating: training-set/sentiment_analysis/23/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/23/info-units/\n",
            "  inflating: training-set/sentiment_analysis/23/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/23/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/23/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/23/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/23/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/23/triples/\n",
            "  inflating: training-set/sentiment_analysis/23/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/23/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/23/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/23/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/24/\n",
            "  inflating: training-set/sentiment_analysis/24/1906.06906v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/24/1906.06906v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/24/1906.06906v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/24/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/24/info-units/\n",
            "  inflating: training-set/sentiment_analysis/24/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentiment_analysis/24/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/24/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/24/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/24/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/24/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/24/triples/\n",
            "  inflating: training-set/sentiment_analysis/24/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentiment_analysis/24/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/24/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/24/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/24/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/25/\n",
            "  inflating: training-set/sentiment_analysis/25/1805.07043v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/25/1805.07043v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/25/1805.07043v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/25/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/25/info-units/\n",
            "  inflating: training-set/sentiment_analysis/25/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/25/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/25/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/25/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/25/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/25/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/25/triples/\n",
            "  inflating: training-set/sentiment_analysis/25/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/25/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/25/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/25/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/25/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/26/\n",
            "  inflating: training-set/sentiment_analysis/26/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/26/info-units/\n",
            "  inflating: training-set/sentiment_analysis/26/info-units/experimental-setup.json  \n",
            "  inflating: training-set/sentiment_analysis/26/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/26/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/26/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/26/P18-1235-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/26/P18-1235-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/26/P18-1235.pdf  \n",
            "  inflating: training-set/sentiment_analysis/26/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/26/triples/\n",
            "  inflating: training-set/sentiment_analysis/26/triples/experimental-setup.txt  \n",
            "  inflating: training-set/sentiment_analysis/26/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/26/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/26/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/27/\n",
            "  inflating: training-set/sentiment_analysis/27/1906.04501v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/27/1906.04501v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/27/1906.04501v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/27/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/27/info-units/\n",
            "  inflating: training-set/sentiment_analysis/27/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/27/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/27/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/27/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/27/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/27/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/27/triples/\n",
            "  inflating: training-set/sentiment_analysis/27/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/27/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/27/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/27/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/27/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/28/\n",
            "  inflating: training-set/sentiment_analysis/28/1902.09314v2-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/28/1902.09314v2-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/28/1902.09314v2.pdf  \n",
            "  inflating: training-set/sentiment_analysis/28/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/28/info-units/\n",
            "  inflating: training-set/sentiment_analysis/28/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentiment_analysis/28/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/28/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/28/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/28/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/28/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/28/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/28/triples/\n",
            "  inflating: training-set/sentiment_analysis/28/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentiment_analysis/28/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/28/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/28/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/28/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/28/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/29/\n",
            "  inflating: training-set/sentiment_analysis/29/1804.06536v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/29/1804.06536v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/29/1804.06536v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/29/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/29/info-units/\n",
            "  inflating: training-set/sentiment_analysis/29/info-units/approach.json  \n",
            "  inflating: training-set/sentiment_analysis/29/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/29/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/29/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/29/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/29/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/29/triples/\n",
            "  inflating: training-set/sentiment_analysis/29/triples/approach.txt  \n",
            "  inflating: training-set/sentiment_analysis/29/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/29/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/29/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/29/triples/results.txt  \n",
            "  inflating: training-set/sentiment_analysis/2/C18-1066-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/2/C18-1066-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/2/C18-1066.pdf  \n",
            "  inflating: training-set/sentiment_analysis/2/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/2/info-units/\n",
            "  inflating: training-set/sentiment_analysis/2/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/2/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/2/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/2/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/2/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/2/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/2/triples/\n",
            "  inflating: training-set/sentiment_analysis/2/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/2/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/2/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/2/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/2/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/3/\n",
            "   creating: training-set/sentiment_analysis/30/\n",
            "  inflating: training-set/sentiment_analysis/30/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/30/info-units/\n",
            "  inflating: training-set/sentiment_analysis/30/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/30/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/30/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/30/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/30/N18-2045-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/30/N18-2045-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/30/N18-2045.pdf  \n",
            "  inflating: training-set/sentiment_analysis/30/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/30/triples/\n",
            "  inflating: training-set/sentiment_analysis/30/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/30/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/30/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/30/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/31/\n",
            "  inflating: training-set/sentiment_analysis/31/D18-1136-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/31/D18-1136-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/31/D18-1136.pdf  \n",
            "  inflating: training-set/sentiment_analysis/31/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/31/info-units/\n",
            "  inflating: training-set/sentiment_analysis/31/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/31/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/31/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/31/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/31/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/31/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/31/triples/\n",
            "  inflating: training-set/sentiment_analysis/31/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/31/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/31/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/31/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/31/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/32/\n",
            "  inflating: training-set/sentiment_analysis/32/1709.00893v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/32/1709.00893v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/32/1709.00893v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/32/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/32/info-units/\n",
            "  inflating: training-set/sentiment_analysis/32/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/32/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/32/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/32/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/32/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/32/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/32/triples/\n",
            "  inflating: training-set/sentiment_analysis/32/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/32/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/32/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/32/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/32/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/33/\n",
            "  inflating: training-set/sentiment_analysis/33/1808.09315v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/33/1808.09315v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/33/1808.09315v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/33/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/33/info-units/\n",
            "  inflating: training-set/sentiment_analysis/33/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/33/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/33/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/33/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/33/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/33/triples/\n",
            "  inflating: training-set/sentiment_analysis/33/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/33/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/33/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/33/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/34/\n",
            "  inflating: training-set/sentiment_analysis/34/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/34/info-units/\n",
            "  inflating: training-set/sentiment_analysis/34/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/34/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/34/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/34/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/34/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/34/triples/\n",
            "  inflating: training-set/sentiment_analysis/34/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/34/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/34/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/34/triples/results.txt  \n",
            "  inflating: training-set/sentiment_analysis/34/W19-4621-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/34/W19-4621-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/34/W19-4621.pdf  \n",
            "   creating: training-set/sentiment_analysis/35/\n",
            "  inflating: training-set/sentiment_analysis/35/1811.10999-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/35/1811.10999-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/35/1811.10999.pdf  \n",
            "  inflating: training-set/sentiment_analysis/35/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/35/info-units/\n",
            "  inflating: training-set/sentiment_analysis/35/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/35/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/35/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/35/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/35/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/35/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/35/triples/\n",
            "  inflating: training-set/sentiment_analysis/35/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/35/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/35/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/35/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/35/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/36/\n",
            "  inflating: training-set/sentiment_analysis/36/1805.01086v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/36/1805.01086v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/36/1805.01086v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/36/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/36/info-units/\n",
            "  inflating: training-set/sentiment_analysis/36/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentiment_analysis/36/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/36/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/36/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/36/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/36/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/36/triples/\n",
            "  inflating: training-set/sentiment_analysis/36/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentiment_analysis/36/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/36/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/36/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/36/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/37/\n",
            "  inflating: training-set/sentiment_analysis/37/D18-1377-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/37/D18-1377-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/37/D18-1377.pdf  \n",
            "  inflating: training-set/sentiment_analysis/37/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/37/info-units/\n",
            "  inflating: training-set/sentiment_analysis/37/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/37/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/37/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/37/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/37/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/37/triples/\n",
            "  inflating: training-set/sentiment_analysis/37/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/37/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/37/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/37/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/38/\n",
            "  inflating: training-set/sentiment_analysis/38/1704.01444v2-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/38/1704.01444v2-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/38/1704.01444v2.pdf  \n",
            "  inflating: training-set/sentiment_analysis/38/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/38/info-units/\n",
            "  inflating: training-set/sentiment_analysis/38/info-units/approach.json  \n",
            "  inflating: training-set/sentiment_analysis/38/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/38/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/38/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/38/triples/\n",
            "  inflating: training-set/sentiment_analysis/38/triples/approach.txt  \n",
            "  inflating: training-set/sentiment_analysis/38/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/38/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/39/\n",
            "  inflating: training-set/sentiment_analysis/39/1610.03771v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/39/1610.03771v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/39/1610.03771v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/39/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/39/info-units/\n",
            "  inflating: training-set/sentiment_analysis/39/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/39/info-units/dataset.json  \n",
            "  inflating: training-set/sentiment_analysis/39/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/39/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/39/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/39/triples/\n",
            "  inflating: training-set/sentiment_analysis/39/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/39/triples/dataset.txt  \n",
            "  inflating: training-set/sentiment_analysis/39/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/39/triples/results.txt  \n",
            "  inflating: training-set/sentiment_analysis/3/1909.10681v2-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/3/1909.10681v2-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/3/1909.10681v2.pdf  \n",
            "  inflating: training-set/sentiment_analysis/3/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/3/info-units/\n",
            "  inflating: training-set/sentiment_analysis/3/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentiment_analysis/3/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/3/info-units/experimental-setup.json  \n",
            "  inflating: training-set/sentiment_analysis/3/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/3/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/3/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/3/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/3/triples/\n",
            "  inflating: training-set/sentiment_analysis/3/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentiment_analysis/3/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/3/triples/experimental-setup.txt  \n",
            "  inflating: training-set/sentiment_analysis/3/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/3/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/3/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/4/\n",
            "   creating: training-set/sentiment_analysis/40/\n",
            "  inflating: training-set/sentiment_analysis/40/D17-1047-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/40/D17-1047-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/40/D17-1047.pdf  \n",
            "  inflating: training-set/sentiment_analysis/40/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/40/info-units/\n",
            "  inflating: training-set/sentiment_analysis/40/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/40/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/40/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/40/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/40/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/40/triples/\n",
            "  inflating: training-set/sentiment_analysis/40/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/40/triples/model.txt  \n",
            " extracting: training-set/sentiment_analysis/40/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/40/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/41/\n",
            "  inflating: training-set/sentiment_analysis/41/D16-1058-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/41/D16-1058-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/41/D16-1058.pdf  \n",
            "  inflating: training-set/sentiment_analysis/41/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/41/info-units/\n",
            "  inflating: training-set/sentiment_analysis/41/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/41/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/41/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/41/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/41/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/41/triples/\n",
            "  inflating: training-set/sentiment_analysis/41/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/41/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/41/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/41/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/42/\n",
            "  inflating: training-set/sentiment_analysis/42/1605.08900v2-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/42/1605.08900v2-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/42/1605.08900v2.pdf  \n",
            "  inflating: training-set/sentiment_analysis/42/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/42/info-units/\n",
            "  inflating: training-set/sentiment_analysis/42/info-units/approach.json  \n",
            "  inflating: training-set/sentiment_analysis/42/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/42/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/42/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/42/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/42/triples/\n",
            "  inflating: training-set/sentiment_analysis/42/triples/approach.txt  \n",
            "  inflating: training-set/sentiment_analysis/42/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/42/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/42/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/43/\n",
            "  inflating: training-set/sentiment_analysis/43/D18-1380-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/43/D18-1380-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/43/D18-1380.pdf  \n",
            "  inflating: training-set/sentiment_analysis/43/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/43/info-units/\n",
            "  inflating: training-set/sentiment_analysis/43/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/43/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/43/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/43/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/43/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/43/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/43/triples/\n",
            "  inflating: training-set/sentiment_analysis/43/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/43/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/43/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/43/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/43/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/44/\n",
            "  inflating: training-set/sentiment_analysis/44/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/44/info-units/\n",
            "  inflating: training-set/sentiment_analysis/44/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/44/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/44/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/44/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/44/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/44/triples/\n",
            "  inflating: training-set/sentiment_analysis/44/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/44/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/44/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/44/triples/results.txt  \n",
            "  inflating: training-set/sentiment_analysis/44/W17-5535-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/44/W17-5535-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/44/W17-5535.pdf  \n",
            "   creating: training-set/sentiment_analysis/45/\n",
            "  inflating: training-set/sentiment_analysis/45/1903.09588v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/45/1903.09588v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/45/1903.09588v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/45/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/45/info-units/\n",
            "  inflating: training-set/sentiment_analysis/45/info-units/approach.json  \n",
            "  inflating: training-set/sentiment_analysis/45/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/45/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/45/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/45/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/45/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/45/triples/\n",
            "  inflating: training-set/sentiment_analysis/45/triples/approach.txt  \n",
            "  inflating: training-set/sentiment_analysis/45/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/45/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/45/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/45/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/46/\n",
            "  inflating: training-set/sentiment_analysis/46/1807.04990v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/46/1807.04990v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/46/1807.04990v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/46/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/46/info-units/\n",
            "  inflating: training-set/sentiment_analysis/46/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/46/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/46/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/46/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/46/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/46/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/46/triples/\n",
            "  inflating: training-set/sentiment_analysis/46/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/46/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/46/triples/model.txt  \n",
            " extracting: training-set/sentiment_analysis/46/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/46/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/47/\n",
            "  inflating: training-set/sentiment_analysis/47/1802.00892v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/47/1802.00892v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/47/1802.00892v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/47/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/47/info-units/\n",
            "  inflating: training-set/sentiment_analysis/47/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/47/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/47/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/47/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/47/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/47/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/47/triples/\n",
            "  inflating: training-set/sentiment_analysis/47/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/47/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/47/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/47/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/47/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/48/\n",
            "  inflating: training-set/sentiment_analysis/48/1810.10437v3-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/48/1810.10437v3-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/48/1810.10437v3.pdf  \n",
            "  inflating: training-set/sentiment_analysis/48/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/48/info-units/\n",
            "  inflating: training-set/sentiment_analysis/48/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/48/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/48/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/48/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/48/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/48/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/48/triples/\n",
            "  inflating: training-set/sentiment_analysis/48/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/48/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/48/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/48/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/48/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/49/\n",
            "  inflating: training-set/sentiment_analysis/49/D18-1382-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/49/D18-1382-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/49/D18-1382.pdf  \n",
            "  inflating: training-set/sentiment_analysis/49/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/49/info-units/\n",
            "  inflating: training-set/sentiment_analysis/49/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/49/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/49/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/49/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/49/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/49/triples/\n",
            "  inflating: training-set/sentiment_analysis/49/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/49/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/49/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/49/triples/results.txt  \n",
            "  inflating: training-set/sentiment_analysis/4/D18-1280-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/4/D18-1280-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/4/D18-1280.pdf  \n",
            "  inflating: training-set/sentiment_analysis/4/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/4/info-units/\n",
            "  inflating: training-set/sentiment_analysis/4/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentiment_analysis/4/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/4/info-units/experimental-setup.json  \n",
            "  inflating: training-set/sentiment_analysis/4/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/4/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/4/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/4/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/4/triples/\n",
            "  inflating: training-set/sentiment_analysis/4/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentiment_analysis/4/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/4/triples/experimental-setup.txt  \n",
            "  inflating: training-set/sentiment_analysis/4/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/4/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/4/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/5/\n",
            "   creating: training-set/sentiment_analysis/50/\n",
            "  inflating: training-set/sentiment_analysis/50/1806.04346v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/50/1806.04346v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/50/1806.04346v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/50/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/50/info-units/\n",
            "  inflating: training-set/sentiment_analysis/50/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/sentiment_analysis/50/info-units/code.json  \n",
            "  inflating: training-set/sentiment_analysis/50/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/50/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/50/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/50/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/50/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/50/triples/\n",
            "  inflating: training-set/sentiment_analysis/50/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/sentiment_analysis/50/triples/code.txt  \n",
            "  inflating: training-set/sentiment_analysis/50/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/50/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/50/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/50/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/51/\n",
            "  inflating: training-set/sentiment_analysis/51/1910.03474v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/51/1910.03474v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/51/1910.03474v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/51/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/51/info-units/\n",
            "  inflating: training-set/sentiment_analysis/51/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/51/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/51/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/51/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/51/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/51/triples/\n",
            "  inflating: training-set/sentiment_analysis/51/triples/baselines.txt  \n",
            "  inflating: training-set/sentiment_analysis/51/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/51/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/51/triples/results.txt  \n",
            "  inflating: training-set/sentiment_analysis/5/1906.01704-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/5/1906.01704-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/5/1906.01704.pdf  \n",
            "  inflating: training-set/sentiment_analysis/5/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/5/info-units/\n",
            "  inflating: training-set/sentiment_analysis/5/info-units/experimental-setup.json  \n",
            "  inflating: training-set/sentiment_analysis/5/info-units/experiments.json  \n",
            "  inflating: training-set/sentiment_analysis/5/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/5/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/5/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/5/triples/\n",
            "  inflating: training-set/sentiment_analysis/5/triples/experimental-setup.txt  \n",
            "  inflating: training-set/sentiment_analysis/5/triples/experiments.txt  \n",
            "  inflating: training-set/sentiment_analysis/5/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/5/triples/research-problem.txt  \n",
            "   creating: training-set/sentiment_analysis/6/\n",
            "  inflating: training-set/sentiment_analysis/6/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/6/info-units/\n",
            "  inflating: training-set/sentiment_analysis/6/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/6/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/6/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/6/P17-1081-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/6/P17-1081-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/6/P17-1081.pdf  \n",
            "  inflating: training-set/sentiment_analysis/6/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/6/triples/\n",
            "  inflating: training-set/sentiment_analysis/6/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/6/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/6/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/7/\n",
            "  inflating: training-set/sentiment_analysis/7/1804.05788v3-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/7/1804.05788v3-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/7/1804.05788v3.pdf  \n",
            "  inflating: training-set/sentiment_analysis/7/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/7/info-units/\n",
            "  inflating: training-set/sentiment_analysis/7/info-units/approach.json  \n",
            "  inflating: training-set/sentiment_analysis/7/info-units/hyperparameters.json  \n",
            "  inflating: training-set/sentiment_analysis/7/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/7/info-units/results.json  \n",
            " extracting: training-set/sentiment_analysis/7/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/7/triples/\n",
            "  inflating: training-set/sentiment_analysis/7/triples/approach.txt  \n",
            "  inflating: training-set/sentiment_analysis/7/triples/hyperparameters.txt  \n",
            "  inflating: training-set/sentiment_analysis/7/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/7/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/8/\n",
            "  inflating: training-set/sentiment_analysis/8/1904.06022v1-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/8/1904.06022v1-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/8/1904.06022v1.pdf  \n",
            "  inflating: training-set/sentiment_analysis/8/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/8/info-units/\n",
            "  inflating: training-set/sentiment_analysis/8/info-units/experimental-setup.json  \n",
            "  inflating: training-set/sentiment_analysis/8/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/8/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/8/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/8/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/8/triples/\n",
            "  inflating: training-set/sentiment_analysis/8/triples/experimental-setup.txt  \n",
            "  inflating: training-set/sentiment_analysis/8/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/8/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/8/triples/results.txt  \n",
            "   creating: training-set/sentiment_analysis/9/\n",
            "  inflating: training-set/sentiment_analysis/9/1912.07976v2-Grobid-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/9/1912.07976v2-Stanza-out.txt  \n",
            "  inflating: training-set/sentiment_analysis/9/1912.07976v2.pdf  \n",
            "  inflating: training-set/sentiment_analysis/9/entities.txt  \n",
            "   creating: training-set/sentiment_analysis/9/info-units/\n",
            "  inflating: training-set/sentiment_analysis/9/info-units/baselines.json  \n",
            "  inflating: training-set/sentiment_analysis/9/info-units/code.json  \n",
            "  inflating: training-set/sentiment_analysis/9/info-units/model.json  \n",
            "  inflating: training-set/sentiment_analysis/9/info-units/research-problem.json  \n",
            "  inflating: training-set/sentiment_analysis/9/info-units/results.json  \n",
            "  inflating: training-set/sentiment_analysis/9/sentences.txt  \n",
            "   creating: training-set/sentiment_analysis/9/triples/\n",
            "  inflating: training-set/sentiment_analysis/9/triples/baselines.txt  \n",
            " extracting: training-set/sentiment_analysis/9/triples/code.txt  \n",
            "  inflating: training-set/sentiment_analysis/9/triples/model.txt  \n",
            "  inflating: training-set/sentiment_analysis/9/triples/research-problem.txt  \n",
            "  inflating: training-set/sentiment_analysis/9/triples/results.txt  \n",
            "   creating: training-set/smile_recognition/\n",
            "   creating: training-set/smile_recognition/0/\n",
            "  inflating: training-set/smile_recognition/0/1602.00172v2-Grobid-out.txt  \n",
            "  inflating: training-set/smile_recognition/0/1602.00172v2-Stanza-out.txt  \n",
            "  inflating: training-set/smile_recognition/0/1602.00172v2.pdf  \n",
            "  inflating: training-set/smile_recognition/0/entities.txt  \n",
            "   creating: training-set/smile_recognition/0/info-units/\n",
            "  inflating: training-set/smile_recognition/0/info-units/experimental-setup.json  \n",
            "  inflating: training-set/smile_recognition/0/info-units/model.json  \n",
            "  inflating: training-set/smile_recognition/0/info-units/research-problem.json  \n",
            "  inflating: training-set/smile_recognition/0/sentences.txt  \n",
            "   creating: training-set/smile_recognition/0/triples/\n",
            "  inflating: training-set/smile_recognition/0/triples/experimental-setup.txt  \n",
            "  inflating: training-set/smile_recognition/0/triples/model.txt  \n",
            "  inflating: training-set/smile_recognition/0/triples/research-problem.txt  \n",
            "   creating: training-set/temporal_information_extraction/\n",
            "   creating: training-set/temporal_information_extraction/0/\n",
            "  inflating: training-set/temporal_information_extraction/0/C16-1007-Grobid-out.txt  \n",
            "  inflating: training-set/temporal_information_extraction/0/C16-1007-Stanza-out.txt  \n",
            "  inflating: training-set/temporal_information_extraction/0/C16-1007.pdf  \n",
            "  inflating: training-set/temporal_information_extraction/0/entities.txt  \n",
            "   creating: training-set/temporal_information_extraction/0/info-units/\n",
            "  inflating: training-set/temporal_information_extraction/0/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/temporal_information_extraction/0/info-units/model.json  \n",
            "  inflating: training-set/temporal_information_extraction/0/info-units/research-problem.json  \n",
            "  inflating: training-set/temporal_information_extraction/0/info-units/results.json  \n",
            "  inflating: training-set/temporal_information_extraction/0/sentences.txt  \n",
            "   creating: training-set/temporal_information_extraction/0/triples/\n",
            "  inflating: training-set/temporal_information_extraction/0/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/temporal_information_extraction/0/triples/model.txt  \n",
            "  inflating: training-set/temporal_information_extraction/0/triples/research-problem.txt  \n",
            "  inflating: training-set/temporal_information_extraction/0/triples/results.txt  \n",
            "   creating: training-set/temporal_information_extraction/1/\n",
            "  inflating: training-set/temporal_information_extraction/1/D17-1108-Grobid-out.txt  \n",
            "  inflating: training-set/temporal_information_extraction/1/D17-1108-Stanza-out.txt  \n",
            "  inflating: training-set/temporal_information_extraction/1/D17-1108.pdf  \n",
            "  inflating: training-set/temporal_information_extraction/1/entities.txt  \n",
            "   creating: training-set/temporal_information_extraction/1/info-units/\n",
            "  inflating: training-set/temporal_information_extraction/1/info-units/approach.json  \n",
            "  inflating: training-set/temporal_information_extraction/1/info-units/baselines.json  \n",
            "  inflating: training-set/temporal_information_extraction/1/info-units/research-problem.json  \n",
            "  inflating: training-set/temporal_information_extraction/1/info-units/results.json  \n",
            "  inflating: training-set/temporal_information_extraction/1/sentences.txt  \n",
            "   creating: training-set/temporal_information_extraction/1/triples/\n",
            "  inflating: training-set/temporal_information_extraction/1/triples/approach.txt  \n",
            "  inflating: training-set/temporal_information_extraction/1/triples/baselines.txt  \n",
            "  inflating: training-set/temporal_information_extraction/1/triples/research-problem.txt  \n",
            "  inflating: training-set/temporal_information_extraction/1/triples/results.txt  \n",
            "   creating: training-set/text-to-speech_synthesis/\n",
            "   creating: training-set/text-to-speech_synthesis/0/\n",
            "  inflating: training-set/text-to-speech_synthesis/0/1904.03446v3-Grobid-out.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/1904.03446v3-Stanza-out.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/1904.03446v3.pdf  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/entities.txt  \n",
            "   creating: training-set/text-to-speech_synthesis/0/info-units/\n",
            "  inflating: training-set/text-to-speech_synthesis/0/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/info-units/approach.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/info-units/experiments.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/info-units/research-problem.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/info-units/results.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/sentences.txt  \n",
            "   creating: training-set/text-to-speech_synthesis/0/triples/\n",
            "  inflating: training-set/text-to-speech_synthesis/0/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/triples/approach.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/triples/experiments.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/triples/research-problem.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/0/triples/results.txt  \n",
            "   creating: training-set/text-to-speech_synthesis/1/\n",
            "  inflating: training-set/text-to-speech_synthesis/1/1905.09263v5-Grobid-out.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/1905.09263v5-Stanza-out.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/1905.09263v5.pdf  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/entities.txt  \n",
            "   creating: training-set/text-to-speech_synthesis/1/info-units/\n",
            "  inflating: training-set/text-to-speech_synthesis/1/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/info-units/model.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/info-units/research-problem.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/info-units/results.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/sentences.txt  \n",
            "   creating: training-set/text-to-speech_synthesis/1/triples/\n",
            "  inflating: training-set/text-to-speech_synthesis/1/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/triples/model.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/triples/research-problem.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/1/triples/results.txt  \n",
            "   creating: training-set/text-to-speech_synthesis/2/\n",
            "  inflating: training-set/text-to-speech_synthesis/2/1806.04558v4-Grobid-out.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/2/1806.04558v4-Stanza-out.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/2/1806.04558v4.pdf  \n",
            "  inflating: training-set/text-to-speech_synthesis/2/entities.txt  \n",
            "   creating: training-set/text-to-speech_synthesis/2/info-units/\n",
            "  inflating: training-set/text-to-speech_synthesis/2/info-units/approach.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/2/info-units/research-problem.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/2/info-units/results.json  \n",
            "  inflating: training-set/text-to-speech_synthesis/2/sentences.txt  \n",
            "   creating: training-set/text-to-speech_synthesis/2/triples/\n",
            "  inflating: training-set/text-to-speech_synthesis/2/triples/approach.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/2/triples/research-problem.txt  \n",
            "  inflating: training-set/text-to-speech_synthesis/2/triples/results.txt  \n",
            "   creating: training-set/text_generation/\n",
            "   creating: training-set/text_generation/0/\n",
            "  inflating: training-set/text_generation/0/1609.05473v6-Grobid-out.txt  \n",
            "  inflating: training-set/text_generation/0/1609.05473v6-Stanza-out.txt  \n",
            "  inflating: training-set/text_generation/0/1609.05473v6.pdf  \n",
            "  inflating: training-set/text_generation/0/entities.txt  \n",
            "   creating: training-set/text_generation/0/info-units/\n",
            "  inflating: training-set/text_generation/0/info-units/baselines.json  \n",
            "  inflating: training-set/text_generation/0/info-units/hyperparameters.json  \n",
            "  inflating: training-set/text_generation/0/info-units/model.json  \n",
            "  inflating: training-set/text_generation/0/info-units/research-problem.json  \n",
            "  inflating: training-set/text_generation/0/info-units/results.json  \n",
            "  inflating: training-set/text_generation/0/sentences.txt  \n",
            "   creating: training-set/text_generation/0/triples/\n",
            "  inflating: training-set/text_generation/0/triples/baselines.txt  \n",
            "  inflating: training-set/text_generation/0/triples/hyperparameters.txt  \n",
            "  inflating: training-set/text_generation/0/triples/model.txt  \n",
            "  inflating: training-set/text_generation/0/triples/research-problem.txt  \n",
            "  inflating: training-set/text_generation/0/triples/results.txt  \n",
            "   creating: training-set/text_generation/1/\n",
            "  inflating: training-set/text_generation/1/1705.11001v3-Grobid-out.txt  \n",
            "  inflating: training-set/text_generation/1/1705.11001v3-Stanza-out.txt  \n",
            "  inflating: training-set/text_generation/1/1705.11001v3.pdf  \n",
            "  inflating: training-set/text_generation/1/entities.txt  \n",
            "   creating: training-set/text_generation/1/info-units/\n",
            "  inflating: training-set/text_generation/1/info-units/model.json  \n",
            "  inflating: training-set/text_generation/1/info-units/research-problem.json  \n",
            "  inflating: training-set/text_generation/1/info-units/results.json  \n",
            "  inflating: training-set/text_generation/1/sentences.txt  \n",
            "   creating: training-set/text_generation/1/triples/\n",
            "  inflating: training-set/text_generation/1/triples/model.txt  \n",
            " extracting: training-set/text_generation/1/triples/research-problem.txt  \n",
            "  inflating: training-set/text_generation/1/triples/results.txt  \n",
            "   creating: training-set/text_generation/2/\n",
            "  inflating: training-set/text_generation/2/1709.08624v2-Grobid-out.txt  \n",
            "  inflating: training-set/text_generation/2/1709.08624v2-Stanza-out.txt  \n",
            "  inflating: training-set/text_generation/2/1709.08624v2.pdf  \n",
            "  inflating: training-set/text_generation/2/entities.txt  \n",
            "   creating: training-set/text_generation/2/info-units/\n",
            "  inflating: training-set/text_generation/2/info-units/approach.json  \n",
            "  inflating: training-set/text_generation/2/info-units/hyperparameters.json  \n",
            "  inflating: training-set/text_generation/2/info-units/research-problem.json  \n",
            "  inflating: training-set/text_generation/2/info-units/results.json  \n",
            "  inflating: training-set/text_generation/2/sentences.txt  \n",
            "   creating: training-set/text_generation/2/triples/\n",
            "  inflating: training-set/text_generation/2/triples/approach.txt  \n",
            "  inflating: training-set/text_generation/2/triples/hyperparameters.txt  \n",
            "  inflating: training-set/text_generation/2/triples/research-problem.txt  \n",
            "  inflating: training-set/text_generation/2/triples/results.txt  \n",
            "   creating: training-set/text_generation/3/\n",
            "  inflating: training-set/text_generation/3/1808.08795v1-Grobid-out.txt  \n",
            "  inflating: training-set/text_generation/3/1808.08795v1-Stanza-out.txt  \n",
            "  inflating: training-set/text_generation/3/1808.08795v1.pdf  \n",
            "  inflating: training-set/text_generation/3/entities.txt  \n",
            "   creating: training-set/text_generation/3/info-units/\n",
            "  inflating: training-set/text_generation/3/info-units/hyperparameters.json  \n",
            "  inflating: training-set/text_generation/3/info-units/model.json  \n",
            "  inflating: training-set/text_generation/3/info-units/research-problem.json  \n",
            "  inflating: training-set/text_generation/3/info-units/results.json  \n",
            "  inflating: training-set/text_generation/3/sentences.txt  \n",
            "   creating: training-set/text_generation/3/triples/\n",
            "  inflating: training-set/text_generation/3/triples/hyperparameters.txt  \n",
            "  inflating: training-set/text_generation/3/triples/model.txt  \n",
            "  inflating: training-set/text_generation/3/triples/research-problem.txt  \n",
            "  inflating: training-set/text_generation/3/triples/results.txt  \n",
            "   creating: training-set/text_generation/4/\n",
            "  inflating: training-set/text_generation/4/1808.08703v2-Grobid-out.txt  \n",
            "  inflating: training-set/text_generation/4/1808.08703v2-Stanza-out.txt  \n",
            "  inflating: training-set/text_generation/4/1808.08703v2.pdf  \n",
            "  inflating: training-set/text_generation/4/entities.txt  \n",
            "   creating: training-set/text_generation/4/info-units/\n",
            "  inflating: training-set/text_generation/4/info-units/code.json  \n",
            "  inflating: training-set/text_generation/4/info-units/hyperparameters.json  \n",
            "  inflating: training-set/text_generation/4/info-units/research-problem.json  \n",
            " extracting: training-set/text_generation/4/sentences.txt  \n",
            "   creating: training-set/text_generation/4/triples/\n",
            "  inflating: training-set/text_generation/4/triples/code.txt  \n",
            "  inflating: training-set/text_generation/4/triples/hyperparameters.txt  \n",
            "  inflating: training-set/text_generation/4/triples/research-problem.txt  \n",
            "   creating: training-set/text_generation/5/\n",
            "  inflating: training-set/text_generation/5/1702.08139v2-Grobid-out.txt  \n",
            "  inflating: training-set/text_generation/5/1702.08139v2-Stanza-out.txt  \n",
            "  inflating: training-set/text_generation/5/1702.08139v2.pdf  \n",
            "  inflating: training-set/text_generation/5/entities.txt  \n",
            "   creating: training-set/text_generation/5/info-units/\n",
            "  inflating: training-set/text_generation/5/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/text_generation/5/info-units/hyperparameters.json  \n",
            "  inflating: training-set/text_generation/5/info-units/model.json  \n",
            "  inflating: training-set/text_generation/5/info-units/research-problem.json  \n",
            "  inflating: training-set/text_generation/5/info-units/results.json  \n",
            "  inflating: training-set/text_generation/5/sentences.txt  \n",
            "   creating: training-set/text_generation/5/triples/\n",
            "  inflating: training-set/text_generation/5/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/text_generation/5/triples/hyperparameters.txt  \n",
            "  inflating: training-set/text_generation/5/triples/model.txt  \n",
            "  inflating: training-set/text_generation/5/triples/research-problem.txt  \n",
            "  inflating: training-set/text_generation/5/triples/results.txt  \n",
            "   creating: training-set/text_summarization/\n",
            "   creating: training-set/text_summarization/0/\n",
            "  inflating: training-set/text_summarization/0/1812.05407v1-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/0/1812.05407v1-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/0/1812.05407v1.pdf  \n",
            "  inflating: training-set/text_summarization/0/entities.txt  \n",
            "   creating: training-set/text_summarization/0/info-units/\n",
            "  inflating: training-set/text_summarization/0/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/text_summarization/0/info-units/baselines.json  \n",
            "  inflating: training-set/text_summarization/0/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/0/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/0/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/0/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/0/sentences.txt  \n",
            "   creating: training-set/text_summarization/0/triples/\n",
            "  inflating: training-set/text_summarization/0/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/text_summarization/0/triples/baselines.txt  \n",
            "  inflating: training-set/text_summarization/0/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text_summarization/0/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/0/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/0/triples/results.txt  \n",
            "   creating: training-set/text_summarization/1/\n",
            "   creating: training-set/text_summarization/10/\n",
            "  inflating: training-set/text_summarization/10/entities.txt  \n",
            "   creating: training-set/text_summarization/10/info-units/\n",
            "  inflating: training-set/text_summarization/10/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/text_summarization/10/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/10/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/10/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/10/P18-1064-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/10/P18-1064-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/10/P18-1064.pdf  \n",
            "  inflating: training-set/text_summarization/10/sentences.txt  \n",
            "   creating: training-set/text_summarization/10/triples/\n",
            "  inflating: training-set/text_summarization/10/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/text_summarization/10/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/10/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/10/triples/results.txt  \n",
            "   creating: training-set/text_summarization/11/\n",
            "  inflating: training-set/text_summarization/11/entities.txt  \n",
            "   creating: training-set/text_summarization/11/info-units/\n",
            "  inflating: training-set/text_summarization/11/info-units/baselines.json  \n",
            "  inflating: training-set/text_summarization/11/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/11/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/11/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/11/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/11/P18-2027-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/11/P18-2027-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/11/P18-2027.pdf  \n",
            "  inflating: training-set/text_summarization/11/sentences.txt  \n",
            "   creating: training-set/text_summarization/11/triples/\n",
            "  inflating: training-set/text_summarization/11/triples/baselines.txt  \n",
            "  inflating: training-set/text_summarization/11/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text_summarization/11/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/11/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/11/triples/results.txt  \n",
            "   creating: training-set/text_summarization/12/\n",
            "  inflating: training-set/text_summarization/12/entities.txt  \n",
            "   creating: training-set/text_summarization/12/info-units/\n",
            "  inflating: training-set/text_summarization/12/info-units/baselines.json  \n",
            "  inflating: training-set/text_summarization/12/info-units/hyperparameters.json  \n",
            "  inflating: training-set/text_summarization/12/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/12/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/12/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/12/P17-1101-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/12/P17-1101-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/12/P17-1101.pdf  \n",
            "  inflating: training-set/text_summarization/12/sentences.txt  \n",
            "   creating: training-set/text_summarization/12/triples/\n",
            "  inflating: training-set/text_summarization/12/triples/baselines.txt  \n",
            "  inflating: training-set/text_summarization/12/triples/hyperparameters.txt  \n",
            "  inflating: training-set/text_summarization/12/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/12/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/12/triples/results.txt  \n",
            "   creating: training-set/text_summarization/13/\n",
            "  inflating: training-set/text_summarization/13/entities.txt  \n",
            "   creating: training-set/text_summarization/13/info-units/\n",
            "  inflating: training-set/text_summarization/13/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/text_summarization/13/info-units/approach.json  \n",
            "  inflating: training-set/text_summarization/13/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/13/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/13/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/13/sentences.txt  \n",
            "   creating: training-set/text_summarization/13/triples/\n",
            "  inflating: training-set/text_summarization/13/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/text_summarization/13/triples/approach.txt  \n",
            "  inflating: training-set/text_summarization/13/triples/experimental-setup.txt  \n",
            " extracting: training-set/text_summarization/13/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/13/triples/results.txt  \n",
            "  inflating: training-set/text_summarization/13/W17-4505-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/13/W17-4505-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/13/W17-4505.pdf  \n",
            "   creating: training-set/text_summarization/14/\n",
            "  inflating: training-set/text_summarization/14/C18-1121-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/14/C18-1121-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/14/C18-1121.pdf  \n",
            "  inflating: training-set/text_summarization/14/entities.txt  \n",
            "   creating: training-set/text_summarization/14/info-units/\n",
            "  inflating: training-set/text_summarization/14/info-units/ablation-analysis.json  \n",
            "  inflating: training-set/text_summarization/14/info-units/baselines.json  \n",
            "  inflating: training-set/text_summarization/14/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/14/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/14/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/14/sentences.txt  \n",
            "   creating: training-set/text_summarization/14/triples/\n",
            "  inflating: training-set/text_summarization/14/triples/ablation-analysis.txt  \n",
            "  inflating: training-set/text_summarization/14/triples/baselines.txt  \n",
            "  inflating: training-set/text_summarization/14/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/14/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/14/triples/results.txt  \n",
            "  inflating: training-set/text_summarization/1/1909.01953v1-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/1/1909.01953v1-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/1/1909.01953v1.pdf  \n",
            "  inflating: training-set/text_summarization/1/entities.txt  \n",
            "   creating: training-set/text_summarization/1/info-units/\n",
            "  inflating: training-set/text_summarization/1/info-units/baselines.json  \n",
            "  inflating: training-set/text_summarization/1/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/1/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/1/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/1/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/1/sentences.txt  \n",
            "   creating: training-set/text_summarization/1/triples/\n",
            "  inflating: training-set/text_summarization/1/triples/baselines.txt  \n",
            "  inflating: training-set/text_summarization/1/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text_summarization/1/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/1/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/1/triples/results.txt  \n",
            "   creating: training-set/text_summarization/2/\n",
            "  inflating: training-set/text_summarization/2/C18-1146-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/2/C18-1146-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/2/C18-1146.pdf  \n",
            "  inflating: training-set/text_summarization/2/entities.txt  \n",
            "   creating: training-set/text_summarization/2/info-units/\n",
            "  inflating: training-set/text_summarization/2/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/2/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/2/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/2/sentences.txt  \n",
            "   creating: training-set/text_summarization/2/triples/\n",
            "  inflating: training-set/text_summarization/2/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/2/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/2/triples/results.txt  \n",
            "   creating: training-set/text_summarization/3/\n",
            "  inflating: training-set/text_summarization/3/1910.08486v1-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/3/1910.08486v1-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/3/1910.08486v1.pdf  \n",
            "  inflating: training-set/text_summarization/3/entities.txt  \n",
            "   creating: training-set/text_summarization/3/info-units/\n",
            "  inflating: training-set/text_summarization/3/info-units/baselines.json  \n",
            "  inflating: training-set/text_summarization/3/info-units/code.json  \n",
            "  inflating: training-set/text_summarization/3/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/3/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/3/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/3/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/3/sentences.txt  \n",
            "   creating: training-set/text_summarization/3/triples/\n",
            "  inflating: training-set/text_summarization/3/triples/baselines.txt  \n",
            " extracting: training-set/text_summarization/3/triples/code.txt  \n",
            "  inflating: training-set/text_summarization/3/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text_summarization/3/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/3/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/3/triples/results.txt  \n",
            "   creating: training-set/text_summarization/4/\n",
            "  inflating: training-set/text_summarization/4/entities.txt  \n",
            "   creating: training-set/text_summarization/4/info-units/\n",
            "  inflating: training-set/text_summarization/4/info-units/baselines.json  \n",
            "  inflating: training-set/text_summarization/4/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/4/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/4/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/4/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/4/N18-1064-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/4/N18-1064-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/4/N18-1064.pdf  \n",
            "  inflating: training-set/text_summarization/4/sentences.txt  \n",
            "   creating: training-set/text_summarization/4/triples/\n",
            "  inflating: training-set/text_summarization/4/triples/baselines.txt  \n",
            "  inflating: training-set/text_summarization/4/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text_summarization/4/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/4/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/4/triples/results.txt  \n",
            "   creating: training-set/text_summarization/5/\n",
            "  inflating: training-set/text_summarization/5/entities.txt  \n",
            "   creating: training-set/text_summarization/5/info-units/\n",
            "  inflating: training-set/text_summarization/5/info-units/approach.json  \n",
            "  inflating: training-set/text_summarization/5/info-units/baselines.json  \n",
            "  inflating: training-set/text_summarization/5/info-units/code.json  \n",
            "  inflating: training-set/text_summarization/5/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/5/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/5/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/5/P18-1015-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/5/P18-1015-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/5/P18-1015.pdf  \n",
            "  inflating: training-set/text_summarization/5/sentences.txt  \n",
            "   creating: training-set/text_summarization/5/triples/\n",
            "  inflating: training-set/text_summarization/5/triples/approach.txt  \n",
            "  inflating: training-set/text_summarization/5/triples/baselines.txt  \n",
            " extracting: training-set/text_summarization/5/triples/code.txt  \n",
            "  inflating: training-set/text_summarization/5/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text_summarization/5/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/5/triples/results.txt  \n",
            "   creating: training-set/text_summarization/6/\n",
            "  inflating: training-set/text_summarization/6/D17-1222-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/6/D17-1222-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/6/D17-1222.pdf  \n",
            "  inflating: training-set/text_summarization/6/entities.txt  \n",
            "   creating: training-set/text_summarization/6/info-units/\n",
            "  inflating: training-set/text_summarization/6/info-units/baselines.json  \n",
            "  inflating: training-set/text_summarization/6/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/6/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/6/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/6/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/6/sentences.txt  \n",
            "   creating: training-set/text_summarization/6/triples/\n",
            "  inflating: training-set/text_summarization/6/triples/baselines.txt  \n",
            "  inflating: training-set/text_summarization/6/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text_summarization/6/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/6/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/6/triples/results.txt  \n",
            "   creating: training-set/text_summarization/7/\n",
            "  inflating: training-set/text_summarization/7/E17-2047-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/7/E17-2047-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/7/E17-2047.pdf  \n",
            "  inflating: training-set/text_summarization/7/entities.txt  \n",
            "   creating: training-set/text_summarization/7/info-units/\n",
            "  inflating: training-set/text_summarization/7/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/7/info-units/research-problem.json  \n",
            " extracting: training-set/text_summarization/7/sentences.txt  \n",
            "   creating: training-set/text_summarization/7/triples/\n",
            "  inflating: training-set/text_summarization/7/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/7/triples/research-problem.txt  \n",
            "   creating: training-set/text_summarization/8/\n",
            "  inflating: training-set/text_summarization/8/1808.10792v2-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/8/1808.10792v2-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/8/1808.10792v2.pdf  \n",
            "  inflating: training-set/text_summarization/8/entities.txt  \n",
            "   creating: training-set/text_summarization/8/info-units/\n",
            "  inflating: training-set/text_summarization/8/info-units/approach.json  \n",
            "  inflating: training-set/text_summarization/8/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/8/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/8/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/8/sentences.txt  \n",
            "   creating: training-set/text_summarization/8/triples/\n",
            "  inflating: training-set/text_summarization/8/triples/approach.txt  \n",
            "  inflating: training-set/text_summarization/8/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text_summarization/8/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/8/triples/results.txt  \n",
            "   creating: training-set/text_summarization/9/\n",
            "  inflating: training-set/text_summarization/9/entities.txt  \n",
            "   creating: training-set/text_summarization/9/info-units/\n",
            "  inflating: training-set/text_summarization/9/info-units/experimental-setup.json  \n",
            "  inflating: training-set/text_summarization/9/info-units/model.json  \n",
            "  inflating: training-set/text_summarization/9/info-units/research-problem.json  \n",
            "  inflating: training-set/text_summarization/9/info-units/results.json  \n",
            "  inflating: training-set/text_summarization/9/N16-1012-Grobid-out.txt  \n",
            "  inflating: training-set/text_summarization/9/N16-1012-Stanza-out.txt  \n",
            "  inflating: training-set/text_summarization/9/N16-1012.pdf  \n",
            "  inflating: training-set/text_summarization/9/sentences.txt  \n",
            "   creating: training-set/text_summarization/9/triples/\n",
            "  inflating: training-set/text_summarization/9/triples/experimental-setup.txt  \n",
            "  inflating: training-set/text_summarization/9/triples/model.txt  \n",
            "  inflating: training-set/text_summarization/9/triples/research-problem.txt  \n",
            "  inflating: training-set/text_summarization/9/triples/results.txt  \n",
            "   creating: training-set/topic_models/\n",
            "   creating: training-set/topic_models/0/\n",
            "  inflating: training-set/topic_models/0/1908.07599v3-Grobid-out.txt  \n",
            "  inflating: training-set/topic_models/0/1908.07599v3-Stanza-out.txt  \n",
            "  inflating: training-set/topic_models/0/1908.07599v3.pdf  \n",
            "  inflating: training-set/topic_models/0/entities.txt  \n",
            "   creating: training-set/topic_models/0/info-units/\n",
            "  inflating: training-set/topic_models/0/info-units/baselines.json  \n",
            "  inflating: training-set/topic_models/0/info-units/hyperparameters.json  \n",
            "  inflating: training-set/topic_models/0/info-units/model.json  \n",
            "  inflating: training-set/topic_models/0/info-units/research-problem.json  \n",
            "  inflating: training-set/topic_models/0/info-units/results.json  \n",
            "  inflating: training-set/topic_models/0/sentences.txt  \n",
            "   creating: training-set/topic_models/0/triples/\n",
            "  inflating: training-set/topic_models/0/triples/baselines.txt  \n",
            "  inflating: training-set/topic_models/0/triples/hyperparameters.txt  \n",
            "  inflating: training-set/topic_models/0/triples/model.txt  \n",
            "  inflating: training-set/topic_models/0/triples/research-problem.txt  \n",
            "  inflating: training-set/topic_models/0/triples/results.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip training-set.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2C-1KA2tkXM"
      },
      "outputs": [],
      "source": [
        "!rm training-set/desktop.ini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVruWVcKpKMY",
        "outputId": "0c370b34-bb17-453f-afc0-49e3bbc4d359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbOWx5hFtIst"
      },
      "outputs": [],
      "source": [
        "from transformers import logging\n",
        "logging.set_verbosity_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79cHX0leo0Uy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFV1JXR8I0oi"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhgiee0ZI2ez"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH74y4yx0Q-7"
      },
      "source": [
        "## **Dataset Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QBFDQv_tbSB"
      },
      "outputs": [],
      "source": [
        "def load_articles(data_dir):\n",
        "    articles = []\n",
        "    contributions = []\n",
        "\n",
        "    for category in os.listdir(data_dir):\n",
        "        if category != 'README.md' and category != '.git':\n",
        "            article_category = os.path.join(data_dir, category)\n",
        "\n",
        "            for foldname in sorted(os.listdir(article_category)):\n",
        "                article_index = os.path.join(article_category, foldname)\n",
        "\n",
        "                with open(glob.glob(os.path.join(article_index, '*-Stanza-out.txt'))[0], encoding='utf-8') as f:\n",
        "                    article = f.read()\n",
        "                    articles.append(article.lower())\n",
        "\n",
        "                with open(os.path.join(article_index, 'sentences.txt'), encoding='utf-8') as f:\n",
        "                    contribution = []\n",
        "                    for line in f.readlines():\n",
        "                        article_contribution = int(line.strip())\n",
        "                        contribution.append(article_contribution)\n",
        "                    contributions.append(contribution)\n",
        "    return articles, contributions\n",
        "\n",
        "def article2sentence_and_labels(articles, contributions):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    for i, article in enumerate(articles):\n",
        "        contribution = contributions[i]\n",
        "\n",
        "        sents = article.split('\\n')[0:-1]\n",
        "        for row, sent in enumerate(sents):\n",
        "            sentences.append(sent)\n",
        "            if (row + 1) in contribution:\n",
        "                labels.append(1)\n",
        "            else:\n",
        "                labels.append(0)\n",
        "    return sentences, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLXB5Z78I5wB"
      },
      "outputs": [],
      "source": [
        "train_data_dir = 'training-set/'\n",
        "train_articles, train_contributions = load_articles(train_data_dir)\n",
        "train_sentences, train_labels = article2sentence_and_labels(train_articles, train_contributions)\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_sentences, train_labels, test_size=.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFY4Cgtn0YIq"
      },
      "source": [
        "# **Pretrained Model**\n",
        "\n",
        "### **SCIBERT**\n",
        "\n",
        "This is the pretrained model presented in SciBERT: A Pretrained Language Model for Scientific Text, which is a BERT model trained on scientific text.\n",
        "\n",
        "The training corpus was papers taken from Semantic Scholar. Corpus size is 1.14M papers, 3.1B tokens. We use the full text of the papers in training, not just abstracts.\n",
        "\n",
        "This model has been trained specifially on scientific texts, such as research papers, scientific articles, and patents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "47599f6779c8449884b5f93699c58aa7",
            "24c774030bdb4cc9973e26f98c36f8e4",
            "6c6eb4efced04678b51f736e56bfe831",
            "1341181052444a6b8dde547abdf5993e",
            "eadbf234d36c4b248b555ef8791f5ce2",
            "b1f627608064424cac91992d671dee5e",
            "4f629a11388e4d05a5f5a4191d1d64c9",
            "5f3c612076744795a6fc71778f02f10d",
            "aa5737d0c18f41a49fd7bf15cfdc903a",
            "72e98287f02a41609d2ad822e898211d",
            "e26b82242fad4bfd8614f89f25d50ab9",
            "69ff2732322b44b1a86c7b3e1b94a4b7",
            "efd2da662e6a43fcb7772d7202294b2f",
            "086146de732f4c68b5bae1b85f512de8",
            "13ec423978a644c78e78d7efe93b212e",
            "02d49eb81ff34a3ba425b57a9243eb83",
            "5a42eb0b72084b5588cdc457ed01b760",
            "cb491e68d4ca4c739b94c96264a0e83b",
            "f165db17d19d4030b42f17f40bb85ddc",
            "e7aeac43c13343d49ac758cb2a6c69f8",
            "bb9991352a964efc86d1b9e82d86052a",
            "33cf417a7ed448799e18d52717fa0472",
            "eaece8403cf341d2818deb9fa15f3121",
            "02b34604f7924972a6e103e888f5f5ff",
            "cb7a0c92375242ffac709cfcfafff90c",
            "3c27a7757a2f4200ab5320e3b3f9705b",
            "52051f3ca8884b268159aa57dff99d55",
            "6710d8fe39884237a23ace2a38e27996",
            "bec2af17e55d4282b680843c1c0beda4",
            "156c9ed92752450fb5efec14aec65e32",
            "8354e4e7c82a479e82d75a0d111677ab",
            "d9e1d2052d5040ea8bece28a9bd07bfc",
            "65a66a19e24c4f34b6a719bc49478920"
          ]
        },
        "id": "UZNluH3gI9po",
        "outputId": "85022d99-c699-4156-80f8-45d0e0d819cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47599f6779c8449884b5f93699c58aa7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69ff2732322b44b1a86c7b3e1b94a4b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaece8403cf341d2818deb9fa15f3121",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/pytorch_model.bin\n",
            "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "\n",
        "# Load the pre-trained model with a classification head on top\n",
        "model = BertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased', num_labels=2)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_hBbrgo1cR6"
      },
      "source": [
        "### **Tokenize the Train and validation sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N56efMhFI_z5",
        "outputId": "c0fc9212-3cde-4e88-c044-15d88892f865"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "train_encodings = tokenizer(train_sentences, truncation=True, padding=True, return_tensors='pt')\n",
        "val_encodings = tokenizer(val_sentences, truncation=True, padding=True, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EctBpGw7JCj6"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(val_labels))\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcxwFtmXuBqM",
        "outputId": "75f32de7-d4fc-49ca-f6f9-0e8ca9a9424a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset : 44160\n",
            "Validation Dataset: 11041\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Dataset :\", len(train_dataset))\n",
        "print(\"Validation Dataset:\",len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM0qgqfcJMR3"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG5d5kRvJOd0",
        "outputId": "e225ebbc-4312-43f0-826e-2cf5efa9d8f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Train Loss: 0.233, Train Acc: 0.913, Val Loss: 0.290, Val Acc: 0.871\n",
            "Epoch 2/10, Train Loss: 0.189, Train Acc: 0.927, Val Loss: 0.284, Val Acc: 0.878\n",
            "Epoch 3/10, Train Loss: 0.148, Train Acc: 0.941, Val Loss: 0.279, Val Acc: 0.893\n",
            "Epoch 4/10, Train Loss: 0.107, Train Acc: 0.958, Val Loss: 0.267, Val Acc: 0.899\n",
            "Epoch 5/10, Train Loss: 0.078, Train Acc: 0.970, Val Loss: 0.247, Val Acc: 0.908\n",
            "Epoch 6/10, Train Loss: 0.058, Train Acc: 0.979, Val Loss: 0.234, Val Acc: 0.898\n",
            "Epoch 7/10, Train Loss: 0.047, Train Acc: 0.984, Val Loss: 0.223, Val Acc: 0.910\n",
            "Epoch 8/10, Train Loss: 0.042, Train Acc: 0.985, Val Loss: 0.229, Val Acc: 0.905\n",
            "Epoch 9/10, Train Loss: 0.035, Train Acc: 0.988, Val Loss: 0.210, Val Acc: 0.910\n",
            "Epoch 10/10, Train Loss: 0.034, Train Acc: 0.989,Val Loss: 0.207, Val Acc: 0.916\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, train_acc = 0, 0\n",
        "    for batch in train_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {'input_ids': batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels': batch[2]}\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs)\n",
        "        loss = loss_fn(outputs.logits, inputs['labels'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (outputs.logits.argmax(dim=1) == inputs['labels']).float().mean().item()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_acc /= len(train_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_acc = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'labels': batch[2]}\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (outputs.logits.argmax(dim=1) == inputs['labels']).float().mean().item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_acc /= len(val_dataloader)\n",
        "\n",
        "    \n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f}, Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3P7XFGA2X1l"
      },
      "source": [
        "## **Model Evaluvation**\n",
        "\n",
        "Model evaluvation using test dataset provided by [SemEval-2021 Task 11: NLPContributionGraph](https://zenodo.org/record/4737071#.ZAMIttJBw3H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e2J8dAm2mqX",
        "outputId": "e5fe4ca2-a5b4-4c83-c680-0c5879ddf8c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-04 11:58:48--  https://zenodo.org/record/4737071/files/test-set.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 225826758 (215M) [application/octet-stream]\n",
            "Saving to: ‘test-set.zip’\n",
            "\n",
            "test-set.zip        100%[===================>] 215.36M  8.30MB/s    in 2m 36s  \n",
            "\n",
            "2023-03-04 12:01:26 (1.38 MB/s) - ‘test-set.zip’ saved [225826758/225826758]\n",
            "\n",
            "Archive:  test-set.zip\n",
            "   creating: test-set/\n",
            "   creating: test-set/constituency_parsing/\n",
            "   creating: test-set/constituency_parsing/0/\n",
            "  inflating: test-set/constituency_parsing/0/1602.07776v4-Grobid-out.txt  \n",
            "  inflating: test-set/constituency_parsing/0/1602.07776v4-Stanza-out.txt  \n",
            "  inflating: test-set/constituency_parsing/0/1602.07776v4.pdf  \n",
            "  inflating: test-set/constituency_parsing/0/entities.txt  \n",
            "   creating: test-set/constituency_parsing/0/info-units/\n",
            "  inflating: test-set/constituency_parsing/0/info-units/hyperparameters.json  \n",
            "  inflating: test-set/constituency_parsing/0/info-units/model.json  \n",
            "  inflating: test-set/constituency_parsing/0/info-units/research-problem.json  \n",
            " extracting: test-set/constituency_parsing/0/section-line-nums.txt  \n",
            "  inflating: test-set/constituency_parsing/0/sentences.txt  \n",
            "   creating: test-set/constituency_parsing/0/triples/\n",
            "  inflating: test-set/constituency_parsing/0/triples/hyperparameters.txt  \n",
            "  inflating: test-set/constituency_parsing/0/triples/model.txt  \n",
            " extracting: test-set/constituency_parsing/0/triples/research-problem.txt  \n",
            "   creating: test-set/constituency_parsing/1/\n",
            "  inflating: test-set/constituency_parsing/1/1903.07785v1-Grobid-out.txt  \n",
            "  inflating: test-set/constituency_parsing/1/1903.07785v1-Stanza-out.txt  \n",
            "  inflating: test-set/constituency_parsing/1/1903.07785v1.pdf  \n",
            "  inflating: test-set/constituency_parsing/1/entities.txt  \n",
            "   creating: test-set/constituency_parsing/1/info-units/\n",
            "  inflating: test-set/constituency_parsing/1/info-units/approach.json  \n",
            "  inflating: test-set/constituency_parsing/1/info-units/experimental-setup.json  \n",
            "  inflating: test-set/constituency_parsing/1/info-units/research-problem.json  \n",
            "  inflating: test-set/constituency_parsing/1/info-units/results.json  \n",
            " extracting: test-set/constituency_parsing/1/section-line-nums.txt  \n",
            "  inflating: test-set/constituency_parsing/1/sentences.txt  \n",
            "   creating: test-set/constituency_parsing/1/triples/\n",
            "  inflating: test-set/constituency_parsing/1/triples/approach.txt  \n",
            "  inflating: test-set/constituency_parsing/1/triples/experimental-setup.txt  \n",
            "  inflating: test-set/constituency_parsing/1/triples/research-problem.txt  \n",
            "  inflating: test-set/constituency_parsing/1/triples/results.txt  \n",
            "   creating: test-set/constituency_parsing/2/\n",
            "  inflating: test-set/constituency_parsing/2/entities.txt  \n",
            "   creating: test-set/constituency_parsing/2/info-units/\n",
            "  inflating: test-set/constituency_parsing/2/info-units/approach.json  \n",
            "  inflating: test-set/constituency_parsing/2/info-units/research-problem.json  \n",
            "  inflating: test-set/constituency_parsing/2/info-units/results.json  \n",
            "  inflating: test-set/constituency_parsing/2/P18-2097-Grobid-out.txt  \n",
            "  inflating: test-set/constituency_parsing/2/P18-2097-Stanza-out.txt  \n",
            "  inflating: test-set/constituency_parsing/2/P18-2097.pdf  \n",
            " extracting: test-set/constituency_parsing/2/section-line-nums.txt  \n",
            " extracting: test-set/constituency_parsing/2/sentences.txt  \n",
            "   creating: test-set/constituency_parsing/2/triples/\n",
            "  inflating: test-set/constituency_parsing/2/triples/approach.txt  \n",
            " extracting: test-set/constituency_parsing/2/triples/research-problem.txt  \n",
            "  inflating: test-set/constituency_parsing/2/triples/results.txt  \n",
            "   creating: test-set/constituency_parsing/3/\n",
            "  inflating: test-set/constituency_parsing/3/1412.7449v3-Grobid-out.txt  \n",
            "  inflating: test-set/constituency_parsing/3/1412.7449v3-Stanza-out.txt  \n",
            "  inflating: test-set/constituency_parsing/3/1412.7449v3.pdf  \n",
            "  inflating: test-set/constituency_parsing/3/entities.txt  \n",
            "   creating: test-set/constituency_parsing/3/info-units/\n",
            "  inflating: test-set/constituency_parsing/3/info-units/approach.json  \n",
            "  inflating: test-set/constituency_parsing/3/info-units/hyperparameters.json  \n",
            "  inflating: test-set/constituency_parsing/3/info-units/research-problem.json  \n",
            "  inflating: test-set/constituency_parsing/3/info-units/results.json  \n",
            " extracting: test-set/constituency_parsing/3/section-line-nums.txt  \n",
            "  inflating: test-set/constituency_parsing/3/sentences.txt  \n",
            "   creating: test-set/constituency_parsing/3/triples/\n",
            "  inflating: test-set/constituency_parsing/3/triples/approach.txt  \n",
            "  inflating: test-set/constituency_parsing/3/triples/hyperparameters.txt  \n",
            "  inflating: test-set/constituency_parsing/3/triples/research-problem.txt  \n",
            "  inflating: test-set/constituency_parsing/3/triples/results.txt  \n",
            "   creating: test-set/constituency_parsing/4/\n",
            "  inflating: test-set/constituency_parsing/4/1707.03058v1-Grobid-out.txt  \n",
            "  inflating: test-set/constituency_parsing/4/1707.03058v1-Stanza-out.txt  \n",
            "  inflating: test-set/constituency_parsing/4/1707.03058v1.pdf  \n",
            "  inflating: test-set/constituency_parsing/4/entities.txt  \n",
            "   creating: test-set/constituency_parsing/4/info-units/\n",
            "  inflating: test-set/constituency_parsing/4/info-units/approach.json  \n",
            "  inflating: test-set/constituency_parsing/4/info-units/research-problem.json  \n",
            "  inflating: test-set/constituency_parsing/4/info-units/results.json  \n",
            " extracting: test-set/constituency_parsing/4/section-line-nums.txt  \n",
            "  inflating: test-set/constituency_parsing/4/sentences.txt  \n",
            "   creating: test-set/constituency_parsing/4/triples/\n",
            "  inflating: test-set/constituency_parsing/4/triples/approach.txt  \n",
            "  inflating: test-set/constituency_parsing/4/triples/research-problem.txt  \n",
            "  inflating: test-set/constituency_parsing/4/triples/results.txt  \n",
            "   creating: test-set/constituency_parsing/5/\n",
            "  inflating: test-set/constituency_parsing/5/entities.txt  \n",
            "   creating: test-set/constituency_parsing/5/info-units/\n",
            "  inflating: test-set/constituency_parsing/5/info-units/code.json  \n",
            "  inflating: test-set/constituency_parsing/5/info-units/model.json  \n",
            "  inflating: test-set/constituency_parsing/5/info-units/research-problem.json  \n",
            "  inflating: test-set/constituency_parsing/5/info-units/results.json  \n",
            "  inflating: test-set/constituency_parsing/5/Q17-1029-Grobid-out.txt  \n",
            "  inflating: test-set/constituency_parsing/5/Q17-1029-Stanza-out.txt  \n",
            "  inflating: test-set/constituency_parsing/5/Q17-1029.pdf  \n",
            " extracting: test-set/constituency_parsing/5/section-line-nums.txt  \n",
            "  inflating: test-set/constituency_parsing/5/sentences.txt  \n",
            "   creating: test-set/constituency_parsing/5/triples/\n",
            " extracting: test-set/constituency_parsing/5/triples/code.txt  \n",
            "  inflating: test-set/constituency_parsing/5/triples/model.txt  \n",
            "  inflating: test-set/constituency_parsing/5/triples/research-problem.txt  \n",
            "  inflating: test-set/constituency_parsing/5/triples/results.txt  \n",
            "   creating: test-set/constituency_parsing/6/\n",
            "  inflating: test-set/constituency_parsing/6/D16-1257-Grobid-out.txt  \n",
            "  inflating: test-set/constituency_parsing/6/D16-1257-Stanza-out.txt  \n",
            "  inflating: test-set/constituency_parsing/6/D16-1257.pdf  \n",
            "  inflating: test-set/constituency_parsing/6/entities.txt  \n",
            "   creating: test-set/constituency_parsing/6/info-units/\n",
            "  inflating: test-set/constituency_parsing/6/info-units/hyperparameters.json  \n",
            "  inflating: test-set/constituency_parsing/6/info-units/model.json  \n",
            "  inflating: test-set/constituency_parsing/6/info-units/research-problem.json  \n",
            "  inflating: test-set/constituency_parsing/6/info-units/results.json  \n",
            " extracting: test-set/constituency_parsing/6/section-line-nums.txt  \n",
            "  inflating: test-set/constituency_parsing/6/sentences.txt  \n",
            "   creating: test-set/constituency_parsing/6/triples/\n",
            "  inflating: test-set/constituency_parsing/6/triples/hyperparameters.txt  \n",
            "  inflating: test-set/constituency_parsing/6/triples/model.txt  \n",
            "  inflating: test-set/constituency_parsing/6/triples/research-problem.txt  \n",
            "  inflating: test-set/constituency_parsing/6/triples/results.txt  \n",
            "   creating: test-set/constituency_parsing/7/\n",
            "  inflating: test-set/constituency_parsing/7/1611.05774v2-Grobid-out.txt  \n",
            "  inflating: test-set/constituency_parsing/7/1611.05774v2-Stanza-out.txt  \n",
            "  inflating: test-set/constituency_parsing/7/1611.05774v2.pdf  \n",
            "  inflating: test-set/constituency_parsing/7/entities.txt  \n",
            "   creating: test-set/constituency_parsing/7/info-units/\n",
            "  inflating: test-set/constituency_parsing/7/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/constituency_parsing/7/info-units/approach.json  \n",
            "  inflating: test-set/constituency_parsing/7/info-units/research-problem.json  \n",
            "  inflating: test-set/constituency_parsing/7/info-units/results.json  \n",
            " extracting: test-set/constituency_parsing/7/section-line-nums.txt  \n",
            "  inflating: test-set/constituency_parsing/7/sentences.txt  \n",
            "   creating: test-set/constituency_parsing/7/triples/\n",
            "  inflating: test-set/constituency_parsing/7/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/constituency_parsing/7/triples/approach.txt  \n",
            "  inflating: test-set/constituency_parsing/7/triples/research-problem.txt  \n",
            "  inflating: test-set/constituency_parsing/7/triples/results.txt  \n",
            "   creating: test-set/constituency_parsing/8/\n",
            "  inflating: test-set/constituency_parsing/8/1805.01052v1-Grobid-out.txt  \n",
            "  inflating: test-set/constituency_parsing/8/1805.01052v1-Stanza-out.txt  \n",
            "  inflating: test-set/constituency_parsing/8/1805.01052v1.pdf  \n",
            "  inflating: test-set/constituency_parsing/8/entities.txt  \n",
            "   creating: test-set/constituency_parsing/8/info-units/\n",
            "  inflating: test-set/constituency_parsing/8/info-units/model.json  \n",
            "  inflating: test-set/constituency_parsing/8/info-units/research-problem.json  \n",
            "  inflating: test-set/constituency_parsing/8/info-units/results.json  \n",
            " extracting: test-set/constituency_parsing/8/section-line-nums.txt  \n",
            "  inflating: test-set/constituency_parsing/8/sentences.txt  \n",
            "   creating: test-set/constituency_parsing/8/triples/\n",
            "  inflating: test-set/constituency_parsing/8/triples/model.txt  \n",
            " extracting: test-set/constituency_parsing/8/triples/research-problem.txt  \n",
            "  inflating: test-set/constituency_parsing/8/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/\n",
            "   creating: test-set/coreference_resolution/0/\n",
            "  inflating: test-set/coreference_resolution/0/1606.01323v2-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/0/1606.01323v2-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/0/1606.01323v2.pdf  \n",
            "  inflating: test-set/coreference_resolution/0/entities.txt  \n",
            "   creating: test-set/coreference_resolution/0/info-units/\n",
            "  inflating: test-set/coreference_resolution/0/info-units/model.json  \n",
            "  inflating: test-set/coreference_resolution/0/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/0/info-units/results.json  \n",
            " extracting: test-set/coreference_resolution/0/section-line-nums.txt  \n",
            " extracting: test-set/coreference_resolution/0/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/0/triples/\n",
            "  inflating: test-set/coreference_resolution/0/triples/model.txt  \n",
            " extracting: test-set/coreference_resolution/0/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/0/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/1/\n",
            "  inflating: test-set/coreference_resolution/1/entities.txt  \n",
            "   creating: test-set/coreference_resolution/1/info-units/\n",
            "  inflating: test-set/coreference_resolution/1/info-units/hyperparameters.json  \n",
            "  inflating: test-set/coreference_resolution/1/info-units/model.json  \n",
            "  inflating: test-set/coreference_resolution/1/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/1/info-units/results.json  \n",
            "  inflating: test-set/coreference_resolution/1/P19-1064-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/1/P19-1064-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/1/P19-1064.pdf  \n",
            " extracting: test-set/coreference_resolution/1/section-line-nums.txt  \n",
            "  inflating: test-set/coreference_resolution/1/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/1/triples/\n",
            "  inflating: test-set/coreference_resolution/1/triples/hyperparameters.txt  \n",
            "  inflating: test-set/coreference_resolution/1/triples/model.txt  \n",
            " extracting: test-set/coreference_resolution/1/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/1/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/2/\n",
            "  inflating: test-set/coreference_resolution/2/1609.08667v3-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/2/1609.08667v3-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/2/1609.08667v3.pdf  \n",
            "  inflating: test-set/coreference_resolution/2/entities.txt  \n",
            "   creating: test-set/coreference_resolution/2/info-units/\n",
            "  inflating: test-set/coreference_resolution/2/info-units/approach.json  \n",
            "  inflating: test-set/coreference_resolution/2/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/2/info-units/results.json  \n",
            " extracting: test-set/coreference_resolution/2/section-line-nums.txt  \n",
            " extracting: test-set/coreference_resolution/2/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/2/triples/\n",
            "  inflating: test-set/coreference_resolution/2/triples/approach.txt  \n",
            "  inflating: test-set/coreference_resolution/2/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/2/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/3/\n",
            "  inflating: test-set/coreference_resolution/3/1804.05392v1-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/3/1804.05392v1-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/3/1804.05392v1.pdf  \n",
            "  inflating: test-set/coreference_resolution/3/entities.txt  \n",
            "   creating: test-set/coreference_resolution/3/info-units/\n",
            "  inflating: test-set/coreference_resolution/3/info-units/model.json  \n",
            "  inflating: test-set/coreference_resolution/3/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/3/info-units/results.json  \n",
            " extracting: test-set/coreference_resolution/3/section-line-nums.txt  \n",
            "  inflating: test-set/coreference_resolution/3/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/3/triples/\n",
            "  inflating: test-set/coreference_resolution/3/triples/model.txt  \n",
            " extracting: test-set/coreference_resolution/3/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/3/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/4/\n",
            "  inflating: test-set/coreference_resolution/4/1706.02256v2-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/4/1706.02256v2-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/4/1706.02256v2.pdf  \n",
            "  inflating: test-set/coreference_resolution/4/entities.txt  \n",
            "   creating: test-set/coreference_resolution/4/info-units/\n",
            "  inflating: test-set/coreference_resolution/4/info-units/baselines.json  \n",
            "  inflating: test-set/coreference_resolution/4/info-units/code.json  \n",
            "  inflating: test-set/coreference_resolution/4/info-units/experimental-setup.json  \n",
            "  inflating: test-set/coreference_resolution/4/info-units/model.json  \n",
            "  inflating: test-set/coreference_resolution/4/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/4/info-units/results.json  \n",
            " extracting: test-set/coreference_resolution/4/section-line-nums.txt  \n",
            "  inflating: test-set/coreference_resolution/4/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/4/triples/\n",
            "  inflating: test-set/coreference_resolution/4/triples/baselines.txt  \n",
            "  inflating: test-set/coreference_resolution/4/triples/code.txt  \n",
            "  inflating: test-set/coreference_resolution/4/triples/experimental-setup.txt  \n",
            "  inflating: test-set/coreference_resolution/4/triples/model.txt  \n",
            "  inflating: test-set/coreference_resolution/4/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/4/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/5/\n",
            "  inflating: test-set/coreference_resolution/5/1604.03035v1-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/5/1604.03035v1-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/5/1604.03035v1.pdf  \n",
            "  inflating: test-set/coreference_resolution/5/entities.txt  \n",
            "   creating: test-set/coreference_resolution/5/info-units/\n",
            "  inflating: test-set/coreference_resolution/5/info-units/code.json  \n",
            "  inflating: test-set/coreference_resolution/5/info-units/experimental-setup.json  \n",
            "  inflating: test-set/coreference_resolution/5/info-units/model.json  \n",
            "  inflating: test-set/coreference_resolution/5/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/5/info-units/results.json  \n",
            " extracting: test-set/coreference_resolution/5/section-line-nums.txt  \n",
            "  inflating: test-set/coreference_resolution/5/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/5/triples/\n",
            " extracting: test-set/coreference_resolution/5/triples/code.txt  \n",
            "  inflating: test-set/coreference_resolution/5/triples/experimental-setup.txt  \n",
            "  inflating: test-set/coreference_resolution/5/triples/model.txt  \n",
            "  inflating: test-set/coreference_resolution/5/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/5/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/6/\n",
            "  inflating: test-set/coreference_resolution/6/D18-1518-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/6/D18-1518-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/6/D18-1518.pdf  \n",
            "  inflating: test-set/coreference_resolution/6/entities.txt  \n",
            "   creating: test-set/coreference_resolution/6/info-units/\n",
            "  inflating: test-set/coreference_resolution/6/info-units/hyperparameters.json  \n",
            "  inflating: test-set/coreference_resolution/6/info-units/model.json  \n",
            "  inflating: test-set/coreference_resolution/6/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/6/info-units/results.json  \n",
            " extracting: test-set/coreference_resolution/6/section-line-nums.txt  \n",
            "  inflating: test-set/coreference_resolution/6/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/6/triples/\n",
            "  inflating: test-set/coreference_resolution/6/triples/hyperparameters.txt  \n",
            "  inflating: test-set/coreference_resolution/6/triples/model.txt  \n",
            "  inflating: test-set/coreference_resolution/6/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/6/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/7/\n",
            "  inflating: test-set/coreference_resolution/7/entities.txt  \n",
            "   creating: test-set/coreference_resolution/7/info-units/\n",
            "  inflating: test-set/coreference_resolution/7/info-units/model.json  \n",
            "  inflating: test-set/coreference_resolution/7/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/7/info-units/results.json  \n",
            "  inflating: test-set/coreference_resolution/7/P19-1066-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/7/P19-1066-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/7/P19-1066.pdf  \n",
            " extracting: test-set/coreference_resolution/7/section-line-nums.txt  \n",
            "  inflating: test-set/coreference_resolution/7/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/7/triples/\n",
            "  inflating: test-set/coreference_resolution/7/triples/model.txt  \n",
            " extracting: test-set/coreference_resolution/7/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/7/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/8/\n",
            "  inflating: test-set/coreference_resolution/8/1707.07045v2-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/8/1707.07045v2-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/8/1707.07045v2.pdf  \n",
            "  inflating: test-set/coreference_resolution/8/entities.txt  \n",
            "   creating: test-set/coreference_resolution/8/info-units/\n",
            "  inflating: test-set/coreference_resolution/8/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/coreference_resolution/8/info-units/experimental-setup.json  \n",
            "  inflating: test-set/coreference_resolution/8/info-units/model.json  \n",
            "  inflating: test-set/coreference_resolution/8/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/8/info-units/results.json  \n",
            " extracting: test-set/coreference_resolution/8/section-line-nums.txt  \n",
            "  inflating: test-set/coreference_resolution/8/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/8/triples/\n",
            "  inflating: test-set/coreference_resolution/8/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/coreference_resolution/8/triples/experimental-setup.txt  \n",
            "  inflating: test-set/coreference_resolution/8/triples/model.txt  \n",
            "  inflating: test-set/coreference_resolution/8/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/8/triples/results.txt  \n",
            "   creating: test-set/coreference_resolution/9/\n",
            "  inflating: test-set/coreference_resolution/9/1908.09091v4-Grobid-out.txt  \n",
            "  inflating: test-set/coreference_resolution/9/1908.09091v4-Stanza-out.txt  \n",
            "  inflating: test-set/coreference_resolution/9/1908.09091v4.pdf  \n",
            "  inflating: test-set/coreference_resolution/9/entities.txt  \n",
            "   creating: test-set/coreference_resolution/9/info-units/\n",
            "  inflating: test-set/coreference_resolution/9/info-units/approach.json  \n",
            "  inflating: test-set/coreference_resolution/9/info-units/code.json  \n",
            "  inflating: test-set/coreference_resolution/9/info-units/experimental-setup.json  \n",
            "  inflating: test-set/coreference_resolution/9/info-units/research-problem.json  \n",
            "  inflating: test-set/coreference_resolution/9/info-units/results.json  \n",
            " extracting: test-set/coreference_resolution/9/section-line-nums.txt  \n",
            "  inflating: test-set/coreference_resolution/9/sentences.txt  \n",
            "   creating: test-set/coreference_resolution/9/triples/\n",
            "  inflating: test-set/coreference_resolution/9/triples/approach.txt  \n",
            " extracting: test-set/coreference_resolution/9/triples/code.txt  \n",
            "  inflating: test-set/coreference_resolution/9/triples/experimental-setup.txt  \n",
            " extracting: test-set/coreference_resolution/9/triples/research-problem.txt  \n",
            "  inflating: test-set/coreference_resolution/9/triples/results.txt  \n",
            "   creating: test-set/data-to-text_generation/\n",
            "   creating: test-set/data-to-text_generation/0/\n",
            "  inflating: test-set/data-to-text_generation/0/1912.10011v1-Grobid-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/0/1912.10011v1-Stanza-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/0/1912.10011v1.pdf  \n",
            "  inflating: test-set/data-to-text_generation/0/entities.txt  \n",
            "   creating: test-set/data-to-text_generation/0/info-units/\n",
            "  inflating: test-set/data-to-text_generation/0/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/data-to-text_generation/0/info-units/baselines.json  \n",
            "  inflating: test-set/data-to-text_generation/0/info-units/code.json  \n",
            "  inflating: test-set/data-to-text_generation/0/info-units/experimental-setup.json  \n",
            "  inflating: test-set/data-to-text_generation/0/info-units/model.json  \n",
            "  inflating: test-set/data-to-text_generation/0/info-units/research-problem.json  \n",
            " extracting: test-set/data-to-text_generation/0/section-line-nums.txt  \n",
            "  inflating: test-set/data-to-text_generation/0/sentences.txt  \n",
            "   creating: test-set/data-to-text_generation/0/triples/\n",
            "  inflating: test-set/data-to-text_generation/0/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/data-to-text_generation/0/triples/baselines.txt  \n",
            " extracting: test-set/data-to-text_generation/0/triples/code.txt  \n",
            "  inflating: test-set/data-to-text_generation/0/triples/experimental-setup.txt  \n",
            "  inflating: test-set/data-to-text_generation/0/triples/model.txt  \n",
            "  inflating: test-set/data-to-text_generation/0/triples/research-problem.txt  \n",
            "   creating: test-set/data-to-text_generation/1/\n",
            "  inflating: test-set/data-to-text_generation/1/1805.06553v1-Grobid-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/1/1805.06553v1-Stanza-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/1/1805.06553v1.pdf  \n",
            "  inflating: test-set/data-to-text_generation/1/entities.txt  \n",
            "   creating: test-set/data-to-text_generation/1/info-units/\n",
            "  inflating: test-set/data-to-text_generation/1/info-units/experimental-setup.json  \n",
            "  inflating: test-set/data-to-text_generation/1/info-units/experiments.json  \n",
            "  inflating: test-set/data-to-text_generation/1/info-units/model.json  \n",
            "  inflating: test-set/data-to-text_generation/1/info-units/research-problem.json  \n",
            " extracting: test-set/data-to-text_generation/1/section-line-nums.txt  \n",
            "  inflating: test-set/data-to-text_generation/1/sentences.txt  \n",
            "   creating: test-set/data-to-text_generation/1/triples/\n",
            "  inflating: test-set/data-to-text_generation/1/triples/experimental-setup.txt  \n",
            "  inflating: test-set/data-to-text_generation/1/triples/experiments.txt  \n",
            "  inflating: test-set/data-to-text_generation/1/triples/model.txt  \n",
            "  inflating: test-set/data-to-text_generation/1/triples/research-problem.txt  \n",
            "   creating: test-set/data-to-text_generation/2/\n",
            "  inflating: test-set/data-to-text_generation/2/1810.09995v1-Grobid-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/2/1810.09995v1-Stanza-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/2/1810.09995v1.pdf  \n",
            "  inflating: test-set/data-to-text_generation/2/entities.txt  \n",
            "   creating: test-set/data-to-text_generation/2/info-units/\n",
            "  inflating: test-set/data-to-text_generation/2/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/data-to-text_generation/2/info-units/research-problem.json  \n",
            "  inflating: test-set/data-to-text_generation/2/info-units/results.json  \n",
            " extracting: test-set/data-to-text_generation/2/section-line-nums.txt  \n",
            "  inflating: test-set/data-to-text_generation/2/sentences.txt  \n",
            "   creating: test-set/data-to-text_generation/2/triples/\n",
            "  inflating: test-set/data-to-text_generation/2/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/data-to-text_generation/2/triples/research-problem.txt  \n",
            "  inflating: test-set/data-to-text_generation/2/triples/results.txt  \n",
            "   creating: test-set/data-to-text_generation/3/\n",
            "  inflating: test-set/data-to-text_generation/3/1904.01301v2-Grobid-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/3/1904.01301v2-Stanza-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/3/1904.01301v2.pdf  \n",
            "  inflating: test-set/data-to-text_generation/3/entities.txt  \n",
            "   creating: test-set/data-to-text_generation/3/info-units/\n",
            "  inflating: test-set/data-to-text_generation/3/info-units/approach.json  \n",
            "  inflating: test-set/data-to-text_generation/3/info-units/research-problem.json  \n",
            "  inflating: test-set/data-to-text_generation/3/info-units/results.json  \n",
            " extracting: test-set/data-to-text_generation/3/section-line-nums.txt  \n",
            " extracting: test-set/data-to-text_generation/3/sentences.txt  \n",
            "   creating: test-set/data-to-text_generation/3/triples/\n",
            "  inflating: test-set/data-to-text_generation/3/triples/approach.txt  \n",
            "  inflating: test-set/data-to-text_generation/3/triples/research-problem.txt  \n",
            "  inflating: test-set/data-to-text_generation/3/triples/results.txt  \n",
            "   creating: test-set/data-to-text_generation/4/\n",
            "  inflating: test-set/data-to-text_generation/4/1809.00582v2-Grobid-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/4/1809.00582v2-Stanza-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/4/1809.00582v2.pdf  \n",
            "  inflating: test-set/data-to-text_generation/4/entities.txt  \n",
            "   creating: test-set/data-to-text_generation/4/info-units/\n",
            "  inflating: test-set/data-to-text_generation/4/info-units/experimental-setup.json  \n",
            "  inflating: test-set/data-to-text_generation/4/info-units/model.json  \n",
            "  inflating: test-set/data-to-text_generation/4/info-units/research-problem.json  \n",
            "  inflating: test-set/data-to-text_generation/4/info-units/results.json  \n",
            " extracting: test-set/data-to-text_generation/4/section-line-nums.txt  \n",
            "  inflating: test-set/data-to-text_generation/4/sentences.txt  \n",
            "   creating: test-set/data-to-text_generation/4/triples/\n",
            "  inflating: test-set/data-to-text_generation/4/triples/experimental-setup.txt  \n",
            "  inflating: test-set/data-to-text_generation/4/triples/model.txt  \n",
            " extracting: test-set/data-to-text_generation/4/triples/research-problem.txt  \n",
            "  inflating: test-set/data-to-text_generation/4/triples/results.txt  \n",
            "   creating: test-set/data-to-text_generation/5/\n",
            "  inflating: test-set/data-to-text_generation/5/1904.03396v2-Grobid-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/5/1904.03396v2-Stanza-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/5/1904.03396v2.pdf  \n",
            "  inflating: test-set/data-to-text_generation/5/entities.txt  \n",
            "   creating: test-set/data-to-text_generation/5/info-units/\n",
            "  inflating: test-set/data-to-text_generation/5/info-units/baselines.json  \n",
            "  inflating: test-set/data-to-text_generation/5/info-units/code.json  \n",
            "  inflating: test-set/data-to-text_generation/5/info-units/model.json  \n",
            "  inflating: test-set/data-to-text_generation/5/info-units/research-problem.json  \n",
            "  inflating: test-set/data-to-text_generation/5/info-units/results.json  \n",
            " extracting: test-set/data-to-text_generation/5/section-line-nums.txt  \n",
            "  inflating: test-set/data-to-text_generation/5/sentences.txt  \n",
            "   creating: test-set/data-to-text_generation/5/triples/\n",
            "  inflating: test-set/data-to-text_generation/5/triples/baselines.txt  \n",
            " extracting: test-set/data-to-text_generation/5/triples/code.txt  \n",
            "  inflating: test-set/data-to-text_generation/5/triples/model.txt  \n",
            "  inflating: test-set/data-to-text_generation/5/triples/research-problem.txt  \n",
            "  inflating: test-set/data-to-text_generation/5/triples/results.txt  \n",
            "   creating: test-set/data-to-text_generation/6/\n",
            "  inflating: test-set/data-to-text_generation/6/1904.11838v3-Grobid-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/6/1904.11838v3-Stanza-out.txt  \n",
            "  inflating: test-set/data-to-text_generation/6/1904.11838v3.pdf  \n",
            "  inflating: test-set/data-to-text_generation/6/entities.txt  \n",
            "   creating: test-set/data-to-text_generation/6/info-units/\n",
            "  inflating: test-set/data-to-text_generation/6/info-units/experimental-setup.json  \n",
            "  inflating: test-set/data-to-text_generation/6/info-units/model.json  \n",
            "  inflating: test-set/data-to-text_generation/6/info-units/research-problem.json  \n",
            "  inflating: test-set/data-to-text_generation/6/info-units/results.json  \n",
            " extracting: test-set/data-to-text_generation/6/section-line-nums.txt  \n",
            "  inflating: test-set/data-to-text_generation/6/sentences.txt  \n",
            "   creating: test-set/data-to-text_generation/6/triples/\n",
            "  inflating: test-set/data-to-text_generation/6/triples/experimental-setup.txt  \n",
            "  inflating: test-set/data-to-text_generation/6/triples/model.txt  \n",
            "  inflating: test-set/data-to-text_generation/6/triples/research-problem.txt  \n",
            "  inflating: test-set/data-to-text_generation/6/triples/results.txt  \n",
            "   creating: test-set/dependency_parsing/\n",
            "   creating: test-set/dependency_parsing/0/\n",
            "  inflating: test-set/dependency_parsing/0/1807.03955v2-Grobid-out.txt  \n",
            "  inflating: test-set/dependency_parsing/0/1807.03955v2-Stanza-out.txt  \n",
            "  inflating: test-set/dependency_parsing/0/1807.03955v2.pdf  \n",
            "  inflating: test-set/dependency_parsing/0/entities.txt  \n",
            "   creating: test-set/dependency_parsing/0/info-units/\n",
            "  inflating: test-set/dependency_parsing/0/info-units/code.json  \n",
            "  inflating: test-set/dependency_parsing/0/info-units/experimental-setup.json  \n",
            "  inflating: test-set/dependency_parsing/0/info-units/model.json  \n",
            "  inflating: test-set/dependency_parsing/0/info-units/research-problem.json  \n",
            "  inflating: test-set/dependency_parsing/0/info-units/results.json  \n",
            " extracting: test-set/dependency_parsing/0/section-line-nums.txt  \n",
            "  inflating: test-set/dependency_parsing/0/sentences.txt  \n",
            "   creating: test-set/dependency_parsing/0/triples/\n",
            " extracting: test-set/dependency_parsing/0/triples/code.txt  \n",
            "  inflating: test-set/dependency_parsing/0/triples/experimental-setup.txt  \n",
            "  inflating: test-set/dependency_parsing/0/triples/model.txt  \n",
            "  inflating: test-set/dependency_parsing/0/triples/research-problem.txt  \n",
            "  inflating: test-set/dependency_parsing/0/triples/results.txt  \n",
            "   creating: test-set/dependency_parsing/1/\n",
            "  inflating: test-set/dependency_parsing/1/1603.04351v3-Grobid-out.txt  \n",
            "  inflating: test-set/dependency_parsing/1/1603.04351v3-Stanza-out.txt  \n",
            "  inflating: test-set/dependency_parsing/1/1603.04351v3.pdf  \n",
            "  inflating: test-set/dependency_parsing/1/entities.txt  \n",
            "   creating: test-set/dependency_parsing/1/info-units/\n",
            "  inflating: test-set/dependency_parsing/1/info-units/approach.json  \n",
            "  inflating: test-set/dependency_parsing/1/info-units/code.json  \n",
            "  inflating: test-set/dependency_parsing/1/info-units/experimental-setup.json  \n",
            "  inflating: test-set/dependency_parsing/1/info-units/research-problem.json  \n",
            "  inflating: test-set/dependency_parsing/1/info-units/results.json  \n",
            " extracting: test-set/dependency_parsing/1/section-line-nums.txt  \n",
            "  inflating: test-set/dependency_parsing/1/sentences.txt  \n",
            "   creating: test-set/dependency_parsing/1/triples/\n",
            "  inflating: test-set/dependency_parsing/1/triples/approach.txt  \n",
            " extracting: test-set/dependency_parsing/1/triples/code.txt  \n",
            "  inflating: test-set/dependency_parsing/1/triples/experimental-setup.txt  \n",
            " extracting: test-set/dependency_parsing/1/triples/research-problem.txt  \n",
            "  inflating: test-set/dependency_parsing/1/triples/results.txt  \n",
            "   creating: test-set/dependency_parsing/2/\n",
            "  inflating: test-set/dependency_parsing/2/1609.07561v1-Grobid-out.txt  \n",
            "  inflating: test-set/dependency_parsing/2/1609.07561v1-Stanza-out.txt  \n",
            "  inflating: test-set/dependency_parsing/2/1609.07561v1.pdf  \n",
            "  inflating: test-set/dependency_parsing/2/entities.txt  \n",
            "   creating: test-set/dependency_parsing/2/info-units/\n",
            "  inflating: test-set/dependency_parsing/2/info-units/approach.json  \n",
            "  inflating: test-set/dependency_parsing/2/info-units/research-problem.json  \n",
            "  inflating: test-set/dependency_parsing/2/info-units/results.json  \n",
            " extracting: test-set/dependency_parsing/2/section-line-nums.txt  \n",
            "  inflating: test-set/dependency_parsing/2/sentences.txt  \n",
            "   creating: test-set/dependency_parsing/2/triples/\n",
            "  inflating: test-set/dependency_parsing/2/triples/approach.txt  \n",
            "  inflating: test-set/dependency_parsing/2/triples/research-problem.txt  \n",
            "  inflating: test-set/dependency_parsing/2/triples/results.txt  \n",
            "   creating: test-set/dependency_parsing/3/\n",
            "  inflating: test-set/dependency_parsing/3/1808.03731v2-Grobid-out.txt  \n",
            "  inflating: test-set/dependency_parsing/3/1808.03731v2-Stanza-out.txt  \n",
            "  inflating: test-set/dependency_parsing/3/1808.03731v2.pdf  \n",
            "  inflating: test-set/dependency_parsing/3/entities.txt  \n",
            "   creating: test-set/dependency_parsing/3/info-units/\n",
            "  inflating: test-set/dependency_parsing/3/info-units/approach.json  \n",
            "  inflating: test-set/dependency_parsing/3/info-units/code.json  \n",
            "  inflating: test-set/dependency_parsing/3/info-units/hyperparameters.json  \n",
            "  inflating: test-set/dependency_parsing/3/info-units/research-problem.json  \n",
            "  inflating: test-set/dependency_parsing/3/info-units/results.json  \n",
            " extracting: test-set/dependency_parsing/3/section-line-nums.txt  \n",
            "  inflating: test-set/dependency_parsing/3/sentences.txt  \n",
            "   creating: test-set/dependency_parsing/3/triples/\n",
            "  inflating: test-set/dependency_parsing/3/triples/approach.txt  \n",
            " extracting: test-set/dependency_parsing/3/triples/code.txt  \n",
            "  inflating: test-set/dependency_parsing/3/triples/hyperparameters.txt  \n",
            "  inflating: test-set/dependency_parsing/3/triples/research-problem.txt  \n",
            "  inflating: test-set/dependency_parsing/3/triples/results.txt  \n",
            "   creating: test-set/dependency_parsing/4/\n",
            "  inflating: test-set/dependency_parsing/4/1805.01087v1-Grobid-out.txt  \n",
            "  inflating: test-set/dependency_parsing/4/1805.01087v1-Stanza-out.txt  \n",
            "  inflating: test-set/dependency_parsing/4/1805.01087v1.pdf  \n",
            "  inflating: test-set/dependency_parsing/4/entities.txt  \n",
            "   creating: test-set/dependency_parsing/4/info-units/\n",
            "  inflating: test-set/dependency_parsing/4/info-units/baselines.json  \n",
            "  inflating: test-set/dependency_parsing/4/info-units/model.json  \n",
            "  inflating: test-set/dependency_parsing/4/info-units/research-problem.json  \n",
            "  inflating: test-set/dependency_parsing/4/info-units/results.json  \n",
            " extracting: test-set/dependency_parsing/4/section-line-nums.txt  \n",
            "  inflating: test-set/dependency_parsing/4/sentences.txt  \n",
            "   creating: test-set/dependency_parsing/4/triples/\n",
            "  inflating: test-set/dependency_parsing/4/triples/baselines.txt  \n",
            "  inflating: test-set/dependency_parsing/4/triples/model.txt  \n",
            " extracting: test-set/dependency_parsing/4/triples/research-problem.txt  \n",
            "  inflating: test-set/dependency_parsing/4/triples/results.txt  \n",
            "   creating: test-set/dependency_parsing/5/\n",
            "  inflating: test-set/dependency_parsing/5/1506.06158v1-Grobid-out.txt  \n",
            "  inflating: test-set/dependency_parsing/5/1506.06158v1-Stanza-out.txt  \n",
            "  inflating: test-set/dependency_parsing/5/1506.06158v1.pdf  \n",
            "  inflating: test-set/dependency_parsing/5/entities.txt  \n",
            "   creating: test-set/dependency_parsing/5/info-units/\n",
            "  inflating: test-set/dependency_parsing/5/info-units/approach.json  \n",
            "  inflating: test-set/dependency_parsing/5/info-units/hyperparameters.json  \n",
            "  inflating: test-set/dependency_parsing/5/info-units/research-problem.json  \n",
            "  inflating: test-set/dependency_parsing/5/info-units/results.json  \n",
            " extracting: test-set/dependency_parsing/5/section-line-nums.txt  \n",
            "  inflating: test-set/dependency_parsing/5/sentences.txt  \n",
            "   creating: test-set/dependency_parsing/5/triples/\n",
            "  inflating: test-set/dependency_parsing/5/triples/approach.txt  \n",
            "  inflating: test-set/dependency_parsing/5/triples/hyperparameters.txt  \n",
            "  inflating: test-set/dependency_parsing/5/triples/research-problem.txt  \n",
            "  inflating: test-set/dependency_parsing/5/triples/results.txt  \n",
            "   creating: test-set/dependency_parsing/6/\n",
            "  inflating: test-set/dependency_parsing/6/1611.01734v3-Grobid-out.txt  \n",
            "  inflating: test-set/dependency_parsing/6/1611.01734v3-Stanza-out.txt  \n",
            "  inflating: test-set/dependency_parsing/6/1611.01734v3.pdf  \n",
            "  inflating: test-set/dependency_parsing/6/entities.txt  \n",
            "   creating: test-set/dependency_parsing/6/info-units/\n",
            "  inflating: test-set/dependency_parsing/6/info-units/approach.json  \n",
            "  inflating: test-set/dependency_parsing/6/info-units/hyperparameters.json  \n",
            "  inflating: test-set/dependency_parsing/6/info-units/research-problem.json  \n",
            "  inflating: test-set/dependency_parsing/6/info-units/results.json  \n",
            " extracting: test-set/dependency_parsing/6/section-line-nums.txt  \n",
            " extracting: test-set/dependency_parsing/6/sentences.txt  \n",
            "   creating: test-set/dependency_parsing/6/triples/\n",
            "  inflating: test-set/dependency_parsing/6/triples/approach.txt  \n",
            "  inflating: test-set/dependency_parsing/6/triples/hyperparameters.txt  \n",
            " extracting: test-set/dependency_parsing/6/triples/research-problem.txt  \n",
            "  inflating: test-set/dependency_parsing/6/triples/results.txt  \n",
            "   creating: test-set/dependency_parsing/7/\n",
            "  inflating: test-set/dependency_parsing/7/1603.03793v2-Grobid-out.txt  \n",
            "  inflating: test-set/dependency_parsing/7/1603.03793v2-Stanza-out.txt  \n",
            "  inflating: test-set/dependency_parsing/7/1603.03793v2.pdf  \n",
            "  inflating: test-set/dependency_parsing/7/entities.txt  \n",
            "   creating: test-set/dependency_parsing/7/info-units/\n",
            "  inflating: test-set/dependency_parsing/7/info-units/approach.json  \n",
            "  inflating: test-set/dependency_parsing/7/info-units/research-problem.json  \n",
            "  inflating: test-set/dependency_parsing/7/info-units/results.json  \n",
            " extracting: test-set/dependency_parsing/7/section-line-nums.txt  \n",
            " extracting: test-set/dependency_parsing/7/sentences.txt  \n",
            "   creating: test-set/dependency_parsing/7/triples/\n",
            "  inflating: test-set/dependency_parsing/7/triples/approach.txt  \n",
            "  inflating: test-set/dependency_parsing/7/triples/research-problem.txt  \n",
            "  inflating: test-set/dependency_parsing/7/triples/results.txt  \n",
            "   creating: test-set/dependency_parsing/8/\n",
            "  inflating: test-set/dependency_parsing/8/1603.06042v2-Grobid-out.txt  \n",
            "  inflating: test-set/dependency_parsing/8/1603.06042v2-Stanza-out.txt  \n",
            "  inflating: test-set/dependency_parsing/8/1603.06042v2.pdf  \n",
            "  inflating: test-set/dependency_parsing/8/entities.txt  \n",
            "   creating: test-set/dependency_parsing/8/info-units/\n",
            "  inflating: test-set/dependency_parsing/8/info-units/experiments.json  \n",
            "  inflating: test-set/dependency_parsing/8/info-units/model.json  \n",
            "  inflating: test-set/dependency_parsing/8/info-units/research-problem.json  \n",
            " extracting: test-set/dependency_parsing/8/section-line-nums.txt  \n",
            "  inflating: test-set/dependency_parsing/8/sentences.txt  \n",
            "   creating: test-set/dependency_parsing/8/triples/\n",
            "  inflating: test-set/dependency_parsing/8/triples/experiments.txt  \n",
            "  inflating: test-set/dependency_parsing/8/triples/model.txt  \n",
            "  inflating: test-set/dependency_parsing/8/triples/research-problem.txt  \n",
            "   creating: test-set/document_classification/\n",
            "   creating: test-set/document_classification/0/\n",
            "  inflating: test-set/document_classification/0/1607.01759v3-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/0/1607.01759v3-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/0/1607.01759v3.pdf  \n",
            "  inflating: test-set/document_classification/0/entities.txt  \n",
            "   creating: test-set/document_classification/0/info-units/\n",
            "  inflating: test-set/document_classification/0/info-units/approach.json  \n",
            "  inflating: test-set/document_classification/0/info-units/experiments.json  \n",
            "  inflating: test-set/document_classification/0/info-units/research-problem.json  \n",
            " extracting: test-set/document_classification/0/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/0/sentences.txt  \n",
            "   creating: test-set/document_classification/0/triples/\n",
            "  inflating: test-set/document_classification/0/triples/approach.txt  \n",
            "  inflating: test-set/document_classification/0/triples/experiments.txt  \n",
            " extracting: test-set/document_classification/0/triples/research-problem.txt  \n",
            "   creating: test-set/document_classification/1/\n",
            "   creating: test-set/document_classification/10/\n",
            "  inflating: test-set/document_classification/10/1909.01259-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/10/1909.01259-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/10/1909.01259.pdf  \n",
            "  inflating: test-set/document_classification/10/entities.txt  \n",
            "   creating: test-set/document_classification/10/info-units/\n",
            "  inflating: test-set/document_classification/10/info-units/baselines.json  \n",
            "  inflating: test-set/document_classification/10/info-units/code.json  \n",
            "  inflating: test-set/document_classification/10/info-units/hyperparameters.json  \n",
            "  inflating: test-set/document_classification/10/info-units/model.json  \n",
            "  inflating: test-set/document_classification/10/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/10/info-units/results.json  \n",
            " extracting: test-set/document_classification/10/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/10/sentences.txt  \n",
            "   creating: test-set/document_classification/10/triples/\n",
            "  inflating: test-set/document_classification/10/triples/baselines.txt  \n",
            "  inflating: test-set/document_classification/10/triples/code.txt  \n",
            "  inflating: test-set/document_classification/10/triples/hyperparameters.txt  \n",
            "  inflating: test-set/document_classification/10/triples/model.txt  \n",
            " extracting: test-set/document_classification/10/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/10/triples/results.txt  \n",
            "   creating: test-set/document_classification/11/\n",
            "  inflating: test-set/document_classification/11/C18-1172-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/11/C18-1172-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/11/C18-1172.pdf  \n",
            "  inflating: test-set/document_classification/11/entities.txt  \n",
            "   creating: test-set/document_classification/11/info-units/\n",
            "  inflating: test-set/document_classification/11/info-units/baselines.json  \n",
            "  inflating: test-set/document_classification/11/info-units/model.json  \n",
            "  inflating: test-set/document_classification/11/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/11/info-units/results.json  \n",
            " extracting: test-set/document_classification/11/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/11/sentences.txt  \n",
            "   creating: test-set/document_classification/11/triples/\n",
            "  inflating: test-set/document_classification/11/triples/baselines.txt  \n",
            "  inflating: test-set/document_classification/11/triples/model.txt  \n",
            " extracting: test-set/document_classification/11/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/11/triples/results.txt  \n",
            "   creating: test-set/document_classification/12/\n",
            "  inflating: test-set/document_classification/12/1809.05679v3-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/12/1809.05679v3-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/12/1809.05679v3.pdf  \n",
            "  inflating: test-set/document_classification/12/entities.txt  \n",
            "   creating: test-set/document_classification/12/info-units/\n",
            "  inflating: test-set/document_classification/12/info-units/baselines.json  \n",
            "  inflating: test-set/document_classification/12/info-units/code.json  \n",
            "  inflating: test-set/document_classification/12/info-units/hyperparameters.json  \n",
            "  inflating: test-set/document_classification/12/info-units/model.json  \n",
            "  inflating: test-set/document_classification/12/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/12/info-units/results.json  \n",
            " extracting: test-set/document_classification/12/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/12/sentences.txt  \n",
            "   creating: test-set/document_classification/12/triples/\n",
            "  inflating: test-set/document_classification/12/triples/baselines.txt  \n",
            " extracting: test-set/document_classification/12/triples/code.txt  \n",
            "  inflating: test-set/document_classification/12/triples/hyperparameters.txt  \n",
            "  inflating: test-set/document_classification/12/triples/model.txt  \n",
            " extracting: test-set/document_classification/12/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/12/triples/results.txt  \n",
            "   creating: test-set/document_classification/13/\n",
            "  inflating: test-set/document_classification/13/entities.txt  \n",
            "   creating: test-set/document_classification/13/info-units/\n",
            "  inflating: test-set/document_classification/13/info-units/model.json  \n",
            "  inflating: test-set/document_classification/13/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/13/info-units/results.json  \n",
            "  inflating: test-set/document_classification/13/P17-1052-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/13/P17-1052-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/13/P17-1052.pdf  \n",
            " extracting: test-set/document_classification/13/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/13/sentences.txt  \n",
            "   creating: test-set/document_classification/13/triples/\n",
            "  inflating: test-set/document_classification/13/triples/model.txt  \n",
            " extracting: test-set/document_classification/13/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/13/triples/results.txt  \n",
            "   creating: test-set/document_classification/14/\n",
            "  inflating: test-set/document_classification/14/1602.02373v2-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/14/1602.02373v2-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/14/1602.02373v2.pdf  \n",
            "  inflating: test-set/document_classification/14/entities.txt  \n",
            "   creating: test-set/document_classification/14/info-units/\n",
            "  inflating: test-set/document_classification/14/info-units/code.json  \n",
            "  inflating: test-set/document_classification/14/info-units/experiments.json  \n",
            "  inflating: test-set/document_classification/14/info-units/model.json  \n",
            "  inflating: test-set/document_classification/14/info-units/research-problem.json  \n",
            " extracting: test-set/document_classification/14/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/14/sentences.txt  \n",
            "   creating: test-set/document_classification/14/triples/\n",
            " extracting: test-set/document_classification/14/triples/code.txt  \n",
            "  inflating: test-set/document_classification/14/triples/experiments.txt  \n",
            "  inflating: test-set/document_classification/14/triples/model.txt  \n",
            "  inflating: test-set/document_classification/14/triples/research-problem.txt  \n",
            "   creating: test-set/document_classification/15/\n",
            "  inflating: test-set/document_classification/15/1605.07725v3-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/15/1605.07725v3-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/15/1605.07725v3.pdf  \n",
            "  inflating: test-set/document_classification/15/entities.txt  \n",
            "   creating: test-set/document_classification/15/info-units/\n",
            "  inflating: test-set/document_classification/15/info-units/approach.json  \n",
            "  inflating: test-set/document_classification/15/info-units/code.json  \n",
            "  inflating: test-set/document_classification/15/info-units/experimental-setup.json  \n",
            "  inflating: test-set/document_classification/15/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/15/info-units/results.json  \n",
            " extracting: test-set/document_classification/15/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/15/sentences.txt  \n",
            "   creating: test-set/document_classification/15/triples/\n",
            "  inflating: test-set/document_classification/15/triples/approach.txt  \n",
            "  inflating: test-set/document_classification/15/triples/code.txt  \n",
            "  inflating: test-set/document_classification/15/triples/experimental-setup.txt  \n",
            " extracting: test-set/document_classification/15/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/15/triples/results.txt  \n",
            "   creating: test-set/document_classification/16/\n",
            "  inflating: test-set/document_classification/16/1511.08630v2-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/16/1511.08630v2-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/16/1511.08630v2.pdf  \n",
            "  inflating: test-set/document_classification/16/entities.txt  \n",
            "   creating: test-set/document_classification/16/info-units/\n",
            "  inflating: test-set/document_classification/16/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/document_classification/16/info-units/experimental-setup.json  \n",
            "  inflating: test-set/document_classification/16/info-units/model.json  \n",
            "  inflating: test-set/document_classification/16/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/16/info-units/results.json  \n",
            " extracting: test-set/document_classification/16/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/16/sentences.txt  \n",
            "   creating: test-set/document_classification/16/triples/\n",
            "  inflating: test-set/document_classification/16/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/document_classification/16/triples/experimental-setup.txt  \n",
            "  inflating: test-set/document_classification/16/triples/model.txt  \n",
            " extracting: test-set/document_classification/16/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/16/triples/results.txt  \n",
            "   creating: test-set/document_classification/17/\n",
            "  inflating: test-set/document_classification/17/1606.01781v2-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/17/1606.01781v2-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/17/1606.01781v2.pdf  \n",
            "  inflating: test-set/document_classification/17/entities.txt  \n",
            "   creating: test-set/document_classification/17/info-units/\n",
            "  inflating: test-set/document_classification/17/info-units/approach.json  \n",
            "  inflating: test-set/document_classification/17/info-units/experimental-setup.json  \n",
            "  inflating: test-set/document_classification/17/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/17/info-units/results.json  \n",
            " extracting: test-set/document_classification/17/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/17/sentences.txt  \n",
            "   creating: test-set/document_classification/17/triples/\n",
            "  inflating: test-set/document_classification/17/triples/approach.txt  \n",
            "  inflating: test-set/document_classification/17/triples/experimental-setup.txt  \n",
            " extracting: test-set/document_classification/17/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/17/triples/results.txt  \n",
            "   creating: test-set/document_classification/18/\n",
            "  inflating: test-set/document_classification/18/1509.01626v3-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/18/1509.01626v3-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/18/1509.01626v3.pdf  \n",
            "  inflating: test-set/document_classification/18/entities.txt  \n",
            "   creating: test-set/document_classification/18/info-units/\n",
            "  inflating: test-set/document_classification/18/info-units/baselines.json  \n",
            "  inflating: test-set/document_classification/18/info-units/model.json  \n",
            "  inflating: test-set/document_classification/18/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/18/info-units/results.json  \n",
            " extracting: test-set/document_classification/18/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/18/sentences.txt  \n",
            "   creating: test-set/document_classification/18/triples/\n",
            "  inflating: test-set/document_classification/18/triples/baselines.txt  \n",
            "  inflating: test-set/document_classification/18/triples/model.txt  \n",
            " extracting: test-set/document_classification/18/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/18/triples/results.txt  \n",
            "   creating: test-set/document_classification/19/\n",
            "  inflating: test-set/document_classification/19/1611.06639v1-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/19/1611.06639v1-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/19/1611.06639v1.pdf  \n",
            "  inflating: test-set/document_classification/19/entities.txt  \n",
            "   creating: test-set/document_classification/19/info-units/\n",
            "  inflating: test-set/document_classification/19/info-units/hyperparameters.json  \n",
            "  inflating: test-set/document_classification/19/info-units/models.json  \n",
            "  inflating: test-set/document_classification/19/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/19/info-units/results.json  \n",
            " extracting: test-set/document_classification/19/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/19/sentences.txt  \n",
            "   creating: test-set/document_classification/19/triples/\n",
            "  inflating: test-set/document_classification/19/triples/hyperparameters.txt  \n",
            "  inflating: test-set/document_classification/19/triples/models.txt  \n",
            " extracting: test-set/document_classification/19/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/19/triples/results.txt  \n",
            "  inflating: test-set/document_classification/1/1909.07009v2-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/1/1909.07009v2-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/1/1909.07009v2.pdf  \n",
            "  inflating: test-set/document_classification/1/entities.txt  \n",
            "   creating: test-set/document_classification/1/info-units/\n",
            "  inflating: test-set/document_classification/1/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/document_classification/1/info-units/baselines.json  \n",
            "  inflating: test-set/document_classification/1/info-units/model.json  \n",
            "  inflating: test-set/document_classification/1/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/1/info-units/results.json  \n",
            " extracting: test-set/document_classification/1/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/1/sentences.txt  \n",
            "   creating: test-set/document_classification/1/triples/\n",
            "  inflating: test-set/document_classification/1/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/document_classification/1/triples/baselines.txt  \n",
            "  inflating: test-set/document_classification/1/triples/model.txt  \n",
            "  inflating: test-set/document_classification/1/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/1/triples/results.txt  \n",
            "   creating: test-set/document_classification/2/\n",
            "   creating: test-set/document_classification/20/\n",
            "  inflating: test-set/document_classification/20/1812.01207v1-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/20/1812.01207v1-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/20/1812.01207v1.pdf  \n",
            "  inflating: test-set/document_classification/20/entities.txt  \n",
            "   creating: test-set/document_classification/20/info-units/\n",
            "  inflating: test-set/document_classification/20/info-units/model.json  \n",
            "  inflating: test-set/document_classification/20/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/20/info-units/results.json  \n",
            " extracting: test-set/document_classification/20/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/20/sentences.txt  \n",
            "   creating: test-set/document_classification/20/triples/\n",
            "  inflating: test-set/document_classification/20/triples/model.txt  \n",
            " extracting: test-set/document_classification/20/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/20/triples/results.txt  \n",
            "  inflating: test-set/document_classification/2/entities.txt  \n",
            "   creating: test-set/document_classification/2/info-units/\n",
            "  inflating: test-set/document_classification/2/info-units/approach.json  \n",
            "  inflating: test-set/document_classification/2/info-units/experimental-setup.json  \n",
            "  inflating: test-set/document_classification/2/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/2/info-units/results.json  \n",
            "  inflating: test-set/document_classification/2/N19-1408-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/2/N19-1408-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/2/N19-1408.pdf  \n",
            " extracting: test-set/document_classification/2/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/2/sentences.txt  \n",
            "   creating: test-set/document_classification/2/triples/\n",
            "  inflating: test-set/document_classification/2/triples/approach.txt  \n",
            "  inflating: test-set/document_classification/2/triples/experimental-setup.txt  \n",
            " extracting: test-set/document_classification/2/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/2/triples/results.txt  \n",
            "   creating: test-set/document_classification/3/\n",
            "  inflating: test-set/document_classification/3/1901.09821v1-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/3/1901.09821v1-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/3/1901.09821v1.pdf  \n",
            "  inflating: test-set/document_classification/3/entities.txt  \n",
            "   creating: test-set/document_classification/3/info-units/\n",
            "  inflating: test-set/document_classification/3/info-units/experimental-setup.json  \n",
            "  inflating: test-set/document_classification/3/info-units/model.json  \n",
            "  inflating: test-set/document_classification/3/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/3/info-units/results.json  \n",
            " extracting: test-set/document_classification/3/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/3/sentences.txt  \n",
            "   creating: test-set/document_classification/3/triples/\n",
            "  inflating: test-set/document_classification/3/triples/experimental-setup.txt  \n",
            "  inflating: test-set/document_classification/3/triples/model.txt  \n",
            " extracting: test-set/document_classification/3/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/3/triples/results.txt  \n",
            "   creating: test-set/document_classification/4/\n",
            "  inflating: test-set/document_classification/4/1805.04174v1-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/4/1805.04174v1-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/4/1805.04174v1.pdf  \n",
            "  inflating: test-set/document_classification/4/entities.txt  \n",
            "   creating: test-set/document_classification/4/info-units/\n",
            "  inflating: test-set/document_classification/4/info-units/baselines.json  \n",
            "  inflating: test-set/document_classification/4/info-units/code.json  \n",
            "  inflating: test-set/document_classification/4/info-units/experimental-setup.json  \n",
            "  inflating: test-set/document_classification/4/info-units/model.json  \n",
            "  inflating: test-set/document_classification/4/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/4/info-units/results.json  \n",
            " extracting: test-set/document_classification/4/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/4/sentences.txt  \n",
            "   creating: test-set/document_classification/4/triples/\n",
            "  inflating: test-set/document_classification/4/triples/baselines.txt  \n",
            " extracting: test-set/document_classification/4/triples/code.txt  \n",
            "  inflating: test-set/document_classification/4/triples/experimental-setup.txt  \n",
            "  inflating: test-set/document_classification/4/triples/model.txt  \n",
            " extracting: test-set/document_classification/4/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/4/triples/results.txt  \n",
            "   creating: test-set/document_classification/5/\n",
            "  inflating: test-set/document_classification/5/1709.08267v2-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/5/1709.08267v2-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/5/1709.08267v2.pdf  \n",
            "  inflating: test-set/document_classification/5/entities.txt  \n",
            "   creating: test-set/document_classification/5/info-units/\n",
            "  inflating: test-set/document_classification/5/info-units/baselines.json  \n",
            "  inflating: test-set/document_classification/5/info-units/experimental-setup.json  \n",
            "  inflating: test-set/document_classification/5/info-units/model.json  \n",
            "  inflating: test-set/document_classification/5/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/5/info-units/results.json  \n",
            " extracting: test-set/document_classification/5/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/5/sentences.txt  \n",
            "   creating: test-set/document_classification/5/triples/\n",
            "  inflating: test-set/document_classification/5/triples/baselines.txt  \n",
            "  inflating: test-set/document_classification/5/triples/experimental-setup.txt  \n",
            "  inflating: test-set/document_classification/5/triples/model.txt  \n",
            "  inflating: test-set/document_classification/5/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/5/triples/results.txt  \n",
            "   creating: test-set/document_classification/6/\n",
            "  inflating: test-set/document_classification/6/1811.09386v1-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/6/1811.09386v1-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/6/1811.09386v1.pdf  \n",
            "  inflating: test-set/document_classification/6/entities.txt  \n",
            "   creating: test-set/document_classification/6/info-units/\n",
            "  inflating: test-set/document_classification/6/info-units/experiments.json  \n",
            "  inflating: test-set/document_classification/6/info-units/model.json  \n",
            "  inflating: test-set/document_classification/6/info-units/research-problem.json  \n",
            " extracting: test-set/document_classification/6/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/6/sentences.txt  \n",
            "   creating: test-set/document_classification/6/triples/\n",
            "  inflating: test-set/document_classification/6/triples/experiments.txt  \n",
            "  inflating: test-set/document_classification/6/triples/model.txt  \n",
            " extracting: test-set/document_classification/6/triples/research-problem.txt  \n",
            "   creating: test-set/document_classification/7/\n",
            "  inflating: test-set/document_classification/7/1805.09821v1-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/7/1805.09821v1-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/7/1805.09821v1.pdf  \n",
            "  inflating: test-set/document_classification/7/entities.txt  \n",
            "   creating: test-set/document_classification/7/info-units/\n",
            "  inflating: test-set/document_classification/7/info-units/approach.json  \n",
            "  inflating: test-set/document_classification/7/info-units/experiments.json  \n",
            "  inflating: test-set/document_classification/7/info-units/research-problem.json  \n",
            " extracting: test-set/document_classification/7/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/7/sentences.txt  \n",
            "   creating: test-set/document_classification/7/triples/\n",
            "  inflating: test-set/document_classification/7/triples/approach.txt  \n",
            "  inflating: test-set/document_classification/7/triples/experiments.txt  \n",
            "  inflating: test-set/document_classification/7/triples/research-problem.txt  \n",
            "   creating: test-set/document_classification/8/\n",
            "  inflating: test-set/document_classification/8/entities.txt  \n",
            "   creating: test-set/document_classification/8/info-units/\n",
            "  inflating: test-set/document_classification/8/info-units/hyperparameters.json  \n",
            "  inflating: test-set/document_classification/8/info-units/model.json  \n",
            "  inflating: test-set/document_classification/8/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/8/info-units/results.json  \n",
            "  inflating: test-set/document_classification/8/P18-1215-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/8/P18-1215-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/8/P18-1215.pdf  \n",
            " extracting: test-set/document_classification/8/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/8/sentences.txt  \n",
            "   creating: test-set/document_classification/8/triples/\n",
            "  inflating: test-set/document_classification/8/triples/hyperparameters.txt  \n",
            "  inflating: test-set/document_classification/8/triples/model.txt  \n",
            " extracting: test-set/document_classification/8/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/8/triples/results.txt  \n",
            "   creating: test-set/document_classification/9/\n",
            "  inflating: test-set/document_classification/9/1804.00538v4-Grobid-out.txt  \n",
            "  inflating: test-set/document_classification/9/1804.00538v4-Stanza-out.txt  \n",
            "  inflating: test-set/document_classification/9/1804.00538v4.pdf  \n",
            "  inflating: test-set/document_classification/9/entities.txt  \n",
            "   creating: test-set/document_classification/9/info-units/\n",
            "  inflating: test-set/document_classification/9/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/document_classification/9/info-units/approach.json  \n",
            "  inflating: test-set/document_classification/9/info-units/baselines.json  \n",
            "  inflating: test-set/document_classification/9/info-units/code.json  \n",
            "  inflating: test-set/document_classification/9/info-units/hyperparameters.json  \n",
            "  inflating: test-set/document_classification/9/info-units/research-problem.json  \n",
            "  inflating: test-set/document_classification/9/info-units/results.json  \n",
            " extracting: test-set/document_classification/9/section-line-nums.txt  \n",
            "  inflating: test-set/document_classification/9/sentences.txt  \n",
            "   creating: test-set/document_classification/9/triples/\n",
            "  inflating: test-set/document_classification/9/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/document_classification/9/triples/approach.txt  \n",
            "  inflating: test-set/document_classification/9/triples/baselines.txt  \n",
            "  inflating: test-set/document_classification/9/triples/code.txt  \n",
            "  inflating: test-set/document_classification/9/triples/hyperparameters.txt  \n",
            " extracting: test-set/document_classification/9/triples/research-problem.txt  \n",
            "  inflating: test-set/document_classification/9/triples/results.txt  \n",
            "   creating: test-set/entity_linking/\n",
            "   creating: test-set/entity_linking/0/\n",
            "  inflating: test-set/entity_linking/0/1704.04920v3-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/0/1704.04920v3-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/0/1704.04920v3.pdf  \n",
            "  inflating: test-set/entity_linking/0/entities.txt  \n",
            "   creating: test-set/entity_linking/0/info-units/\n",
            "  inflating: test-set/entity_linking/0/info-units/code.json  \n",
            "  inflating: test-set/entity_linking/0/info-units/experimental-setup.json  \n",
            "  inflating: test-set/entity_linking/0/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/0/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/0/info-units/results.json  \n",
            " extracting: test-set/entity_linking/0/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/0/sentences.txt  \n",
            "   creating: test-set/entity_linking/0/triples/\n",
            " extracting: test-set/entity_linking/0/triples/code.txt  \n",
            "  inflating: test-set/entity_linking/0/triples/experimental-setup.txt  \n",
            "  inflating: test-set/entity_linking/0/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/0/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/0/triples/results.txt  \n",
            "   creating: test-set/entity_linking/1/\n",
            "   creating: test-set/entity_linking/10/\n",
            "  inflating: test-set/entity_linking/10/entities.txt  \n",
            "   creating: test-set/entity_linking/10/info-units/\n",
            "  inflating: test-set/entity_linking/10/info-units/experiments.json  \n",
            "  inflating: test-set/entity_linking/10/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/10/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/10/N18-1202-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/10/N18-1202-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/10/N18-1202.pdf  \n",
            " extracting: test-set/entity_linking/10/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/10/sentences.txt  \n",
            "   creating: test-set/entity_linking/10/triples/\n",
            "  inflating: test-set/entity_linking/10/triples/experiments.txt  \n",
            "  inflating: test-set/entity_linking/10/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/10/triples/research-problem.txt  \n",
            "   creating: test-set/entity_linking/11/\n",
            "  inflating: test-set/entity_linking/11/1905.05677v3-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/11/1905.05677v3-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/11/1905.05677v3.pdf  \n",
            "  inflating: test-set/entity_linking/11/entities.txt  \n",
            "   creating: test-set/entity_linking/11/info-units/\n",
            "  inflating: test-set/entity_linking/11/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/entity_linking/11/info-units/experimental-setup.json  \n",
            "  inflating: test-set/entity_linking/11/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/11/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/11/info-units/results.json  \n",
            " extracting: test-set/entity_linking/11/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/11/sentences.txt  \n",
            "   creating: test-set/entity_linking/11/triples/\n",
            "  inflating: test-set/entity_linking/11/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/entity_linking/11/triples/experimental-setup.txt  \n",
            "  inflating: test-set/entity_linking/11/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/11/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/11/triples/results.txt  \n",
            "   creating: test-set/entity_linking/12/\n",
            "  inflating: test-set/entity_linking/12/entities.txt  \n",
            "   creating: test-set/entity_linking/12/info-units/\n",
            "  inflating: test-set/entity_linking/12/info-units/baselines.json  \n",
            "  inflating: test-set/entity_linking/12/info-units/hyperparameters.json  \n",
            "  inflating: test-set/entity_linking/12/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/12/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/12/info-units/results.json  \n",
            "  inflating: test-set/entity_linking/12/P18-1230-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/12/P18-1230-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/12/P18-1230.pdf  \n",
            " extracting: test-set/entity_linking/12/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/12/sentences.txt  \n",
            "   creating: test-set/entity_linking/12/triples/\n",
            "  inflating: test-set/entity_linking/12/triples/baselines.txt  \n",
            "  inflating: test-set/entity_linking/12/triples/hyperparameters.txt  \n",
            "  inflating: test-set/entity_linking/12/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/12/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/12/triples/results.txt  \n",
            "   creating: test-set/entity_linking/13/\n",
            "  inflating: test-set/entity_linking/13/1606.03568v2-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/13/1606.03568v2-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/13/1606.03568v2.pdf  \n",
            "  inflating: test-set/entity_linking/13/entities.txt  \n",
            "   creating: test-set/entity_linking/13/info-units/\n",
            "  inflating: test-set/entity_linking/13/info-units/experimental-setup.json  \n",
            "  inflating: test-set/entity_linking/13/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/13/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/13/info-units/results.json  \n",
            " extracting: test-set/entity_linking/13/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/13/sentences.txt  \n",
            "   creating: test-set/entity_linking/13/triples/\n",
            "  inflating: test-set/entity_linking/13/triples/experimental-setup.txt  \n",
            "  inflating: test-set/entity_linking/13/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/13/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/13/triples/results.txt  \n",
            "   creating: test-set/entity_linking/14/\n",
            "  inflating: test-set/entity_linking/14/1801.01900v1-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/14/1801.01900v1-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/14/1801.01900v1.pdf  \n",
            "  inflating: test-set/entity_linking/14/entities.txt  \n",
            "   creating: test-set/entity_linking/14/info-units/\n",
            "  inflating: test-set/entity_linking/14/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/14/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/14/info-units/results.json  \n",
            " extracting: test-set/entity_linking/14/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/14/sentences.txt  \n",
            "   creating: test-set/entity_linking/14/triples/\n",
            "  inflating: test-set/entity_linking/14/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/14/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/14/triples/results.txt  \n",
            "   creating: test-set/entity_linking/15/\n",
            "  inflating: test-set/entity_linking/15/1804.08460v1-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/15/1804.08460v1-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/15/1804.08460v1.pdf  \n",
            "  inflating: test-set/entity_linking/15/entities.txt  \n",
            "   creating: test-set/entity_linking/15/info-units/\n",
            "  inflating: test-set/entity_linking/15/info-units/approach.json  \n",
            "  inflating: test-set/entity_linking/15/info-units/baselines.json  \n",
            "  inflating: test-set/entity_linking/15/info-units/code.json  \n",
            "  inflating: test-set/entity_linking/15/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/15/info-units/results.json  \n",
            " extracting: test-set/entity_linking/15/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/15/sentences.txt  \n",
            "   creating: test-set/entity_linking/15/triples/\n",
            "  inflating: test-set/entity_linking/15/triples/approach.txt  \n",
            "  inflating: test-set/entity_linking/15/triples/baselines.txt  \n",
            " extracting: test-set/entity_linking/15/triples/code.txt  \n",
            "  inflating: test-set/entity_linking/15/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/15/triples/results.txt  \n",
            "   creating: test-set/entity_linking/16/\n",
            "  inflating: test-set/entity_linking/16/1802.09059v1-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/16/1802.09059v1-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/16/1802.09059v1.pdf  \n",
            "  inflating: test-set/entity_linking/16/entities.txt  \n",
            "   creating: test-set/entity_linking/16/info-units/\n",
            "  inflating: test-set/entity_linking/16/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/16/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/16/info-units/results.json  \n",
            " extracting: test-set/entity_linking/16/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/16/sentences.txt  \n",
            "   creating: test-set/entity_linking/16/triples/\n",
            "  inflating: test-set/entity_linking/16/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/16/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/16/triples/results.txt  \n",
            "  inflating: test-set/entity_linking/1/1909.00426v1-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/1/1909.00426v1-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/1/1909.00426v1.pdf  \n",
            "  inflating: test-set/entity_linking/1/entities.txt  \n",
            "   creating: test-set/entity_linking/1/info-units/\n",
            "  inflating: test-set/entity_linking/1/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/1/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/1/info-units/results.json  \n",
            " extracting: test-set/entity_linking/1/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/1/sentences.txt  \n",
            "   creating: test-set/entity_linking/1/triples/\n",
            "  inflating: test-set/entity_linking/1/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/1/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/1/triples/results.txt  \n",
            "   creating: test-set/entity_linking/2/\n",
            "  inflating: test-set/entity_linking/2/D17-1120-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/2/D17-1120-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/2/D17-1120.pdf  \n",
            "  inflating: test-set/entity_linking/2/entities.txt  \n",
            "   creating: test-set/entity_linking/2/info-units/\n",
            "  inflating: test-set/entity_linking/2/info-units/hyperparameters.json  \n",
            "  inflating: test-set/entity_linking/2/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/2/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/2/info-units/results.json  \n",
            " extracting: test-set/entity_linking/2/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/2/sentences.txt  \n",
            "   creating: test-set/entity_linking/2/triples/\n",
            "  inflating: test-set/entity_linking/2/triples/hyperparameters.txt  \n",
            "  inflating: test-set/entity_linking/2/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/2/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/2/triples/results.txt  \n",
            "   creating: test-set/entity_linking/3/\n",
            "  inflating: test-set/entity_linking/3/1909.10430v2-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/3/1909.10430v2-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/3/1909.10430v2.pdf  \n",
            "  inflating: test-set/entity_linking/3/entities.txt  \n",
            "   creating: test-set/entity_linking/3/info-units/\n",
            "  inflating: test-set/entity_linking/3/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/3/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/3/info-units/results.json  \n",
            " extracting: test-set/entity_linking/3/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/3/sentences.txt  \n",
            "   creating: test-set/entity_linking/3/triples/\n",
            "  inflating: test-set/entity_linking/3/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/3/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/3/triples/results.txt  \n",
            "   creating: test-set/entity_linking/4/\n",
            "  inflating: test-set/entity_linking/4/1705.02494v3-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/4/1705.02494v3-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/4/1705.02494v3.pdf  \n",
            "  inflating: test-set/entity_linking/4/entities.txt  \n",
            "   creating: test-set/entity_linking/4/info-units/\n",
            "  inflating: test-set/entity_linking/4/info-units/baselines.json  \n",
            "  inflating: test-set/entity_linking/4/info-units/code.json  \n",
            "  inflating: test-set/entity_linking/4/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/4/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/4/info-units/results.json  \n",
            " extracting: test-set/entity_linking/4/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/4/sentences.txt  \n",
            "   creating: test-set/entity_linking/4/triples/\n",
            "  inflating: test-set/entity_linking/4/triples/baselines.txt  \n",
            " extracting: test-set/entity_linking/4/triples/code.txt  \n",
            "  inflating: test-set/entity_linking/4/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/4/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/4/triples/results.txt  \n",
            "   creating: test-set/entity_linking/5/\n",
            "  inflating: test-set/entity_linking/5/1811.00960v1-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/5/1811.00960v1-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/5/1811.00960v1.pdf  \n",
            "  inflating: test-set/entity_linking/5/entities.txt  \n",
            "   creating: test-set/entity_linking/5/info-units/\n",
            "  inflating: test-set/entity_linking/5/info-units/code.json  \n",
            "  inflating: test-set/entity_linking/5/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/5/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/5/info-units/results.json  \n",
            " extracting: test-set/entity_linking/5/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/5/sentences.txt  \n",
            "   creating: test-set/entity_linking/5/triples/\n",
            " extracting: test-set/entity_linking/5/triples/code.txt  \n",
            "  inflating: test-set/entity_linking/5/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/5/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/5/triples/results.txt  \n",
            "   creating: test-set/entity_linking/6/\n",
            "  inflating: test-set/entity_linking/6/1805.08028v2-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/6/1805.08028v2-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/6/1805.08028v2.pdf  \n",
            "  inflating: test-set/entity_linking/6/entities.txt  \n",
            "   creating: test-set/entity_linking/6/info-units/\n",
            "  inflating: test-set/entity_linking/6/info-units/hyperparameters.json  \n",
            "  inflating: test-set/entity_linking/6/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/6/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/6/info-units/results.json  \n",
            " extracting: test-set/entity_linking/6/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/6/sentences.txt  \n",
            "   creating: test-set/entity_linking/6/triples/\n",
            "  inflating: test-set/entity_linking/6/triples/hyperparameters.txt  \n",
            "  inflating: test-set/entity_linking/6/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/6/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/6/triples/results.txt  \n",
            "   creating: test-set/entity_linking/7/\n",
            "  inflating: test-set/entity_linking/7/1603.07012v2-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/7/1603.07012v2-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/7/1603.07012v2.pdf  \n",
            "  inflating: test-set/entity_linking/7/entities.txt  \n",
            "   creating: test-set/entity_linking/7/info-units/\n",
            "  inflating: test-set/entity_linking/7/info-units/experiments.json  \n",
            "  inflating: test-set/entity_linking/7/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/7/info-units/research-problem.json  \n",
            " extracting: test-set/entity_linking/7/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/7/sentences.txt  \n",
            "   creating: test-set/entity_linking/7/triples/\n",
            "  inflating: test-set/entity_linking/7/triples/experiments.txt  \n",
            "  inflating: test-set/entity_linking/7/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/7/triples/research-problem.txt  \n",
            "   creating: test-set/entity_linking/8/\n",
            "  inflating: test-set/entity_linking/8/2001.03765v1-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/8/2001.03765v1-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/8/2001.03765v1.pdf  \n",
            "  inflating: test-set/entity_linking/8/entities.txt  \n",
            "   creating: test-set/entity_linking/8/info-units/\n",
            "  inflating: test-set/entity_linking/8/info-units/experimental-setup.json  \n",
            "  inflating: test-set/entity_linking/8/info-units/experiments.json  \n",
            "  inflating: test-set/entity_linking/8/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/8/info-units/research-problem.json  \n",
            " extracting: test-set/entity_linking/8/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/8/sentences.txt  \n",
            "   creating: test-set/entity_linking/8/triples/\n",
            "  inflating: test-set/entity_linking/8/triples/experimental-setup.txt  \n",
            "  inflating: test-set/entity_linking/8/triples/experiments.txt  \n",
            "  inflating: test-set/entity_linking/8/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/8/triples/research-problem.txt  \n",
            "   creating: test-set/entity_linking/9/\n",
            "  inflating: test-set/entity_linking/9/1601.01343v4-Grobid-out.txt  \n",
            "  inflating: test-set/entity_linking/9/1601.01343v4-Stanza-out.txt  \n",
            "  inflating: test-set/entity_linking/9/1601.01343v4.pdf  \n",
            "  inflating: test-set/entity_linking/9/entities.txt  \n",
            "   creating: test-set/entity_linking/9/info-units/\n",
            "  inflating: test-set/entity_linking/9/info-units/model.json  \n",
            "  inflating: test-set/entity_linking/9/info-units/research-problem.json  \n",
            "  inflating: test-set/entity_linking/9/info-units/results.json  \n",
            " extracting: test-set/entity_linking/9/section-line-nums.txt  \n",
            "  inflating: test-set/entity_linking/9/sentences.txt  \n",
            "   creating: test-set/entity_linking/9/triples/\n",
            "  inflating: test-set/entity_linking/9/triples/model.txt  \n",
            "  inflating: test-set/entity_linking/9/triples/research-problem.txt  \n",
            "  inflating: test-set/entity_linking/9/triples/results.txt  \n",
            "   creating: test-set/face_alignment/\n",
            "   creating: test-set/face_alignment/0/\n",
            "  inflating: test-set/face_alignment/0/1701.05360v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/0/1701.05360v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/0/1701.05360v1.pdf  \n",
            "  inflating: test-set/face_alignment/0/entities.txt  \n",
            "   creating: test-set/face_alignment/0/info-units/\n",
            "  inflating: test-set/face_alignment/0/info-units/experiments.json  \n",
            "  inflating: test-set/face_alignment/0/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/0/info-units/research-problem.json  \n",
            " extracting: test-set/face_alignment/0/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/0/sentences.txt  \n",
            "   creating: test-set/face_alignment/0/triples/\n",
            "  inflating: test-set/face_alignment/0/triples/experiments.txt  \n",
            "  inflating: test-set/face_alignment/0/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/0/triples/research-problem.txt  \n",
            "   creating: test-set/face_alignment/1/\n",
            "   creating: test-set/face_alignment/10/\n",
            "  inflating: test-set/face_alignment/10/1711.06753v5-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/10/1711.06753v5-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/10/1711.06753v5.pdf  \n",
            "  inflating: test-set/face_alignment/10/entities.txt  \n",
            "   creating: test-set/face_alignment/10/info-units/\n",
            "  inflating: test-set/face_alignment/10/info-units/approach.json  \n",
            "  inflating: test-set/face_alignment/10/info-units/baselines.json  \n",
            "  inflating: test-set/face_alignment/10/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/10/info-units/experiments.json  \n",
            "  inflating: test-set/face_alignment/10/info-units/research-problem.json  \n",
            " extracting: test-set/face_alignment/10/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/10/sentences.txt  \n",
            "   creating: test-set/face_alignment/10/triples/\n",
            "  inflating: test-set/face_alignment/10/triples/approach.txt  \n",
            "  inflating: test-set/face_alignment/10/triples/baselines.txt  \n",
            "  inflating: test-set/face_alignment/10/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/10/triples/experiments.txt  \n",
            "  inflating: test-set/face_alignment/10/triples/research-problem.txt  \n",
            "   creating: test-set/face_alignment/11/\n",
            "  inflating: test-set/face_alignment/11/1806.06098v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/11/1806.06098v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/11/1806.06098v1.pdf  \n",
            "  inflating: test-set/face_alignment/11/entities.txt  \n",
            "   creating: test-set/face_alignment/11/info-units/\n",
            "  inflating: test-set/face_alignment/11/info-units/experiments.json  \n",
            "  inflating: test-set/face_alignment/11/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/11/info-units/research-problem.json  \n",
            " extracting: test-set/face_alignment/11/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/11/sentences.txt  \n",
            "   creating: test-set/face_alignment/11/triples/\n",
            "  inflating: test-set/face_alignment/11/triples/experiments.txt  \n",
            "  inflating: test-set/face_alignment/11/triples/model.txt  \n",
            " extracting: test-set/face_alignment/11/triples/research-problem.txt  \n",
            "   creating: test-set/face_alignment/12/\n",
            "  inflating: test-set/face_alignment/12/1709.01442v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/12/1709.01442v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/12/1709.01442v1.pdf  \n",
            "  inflating: test-set/face_alignment/12/entities.txt  \n",
            "   creating: test-set/face_alignment/12/info-units/\n",
            "  inflating: test-set/face_alignment/12/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/12/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/12/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/12/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/12/info-units/results.json  \n",
            " extracting: test-set/face_alignment/12/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/12/sentences.txt  \n",
            "   creating: test-set/face_alignment/12/triples/\n",
            "  inflating: test-set/face_alignment/12/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_alignment/12/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/12/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/12/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/12/triples/results.txt  \n",
            "   creating: test-set/face_alignment/13/\n",
            "  inflating: test-set/face_alignment/13/1804.03786v3-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/13/1804.03786v3-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/13/1804.03786v3.pdf  \n",
            "  inflating: test-set/face_alignment/13/entities.txt  \n",
            "   creating: test-set/face_alignment/13/info-units/\n",
            "  inflating: test-set/face_alignment/13/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/13/info-units/approach.json  \n",
            "  inflating: test-set/face_alignment/13/info-units/hyperparameters.json  \n",
            "  inflating: test-set/face_alignment/13/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/13/info-units/results.json  \n",
            " extracting: test-set/face_alignment/13/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/13/sentences.txt  \n",
            "   creating: test-set/face_alignment/13/triples/\n",
            "  inflating: test-set/face_alignment/13/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_alignment/13/triples/approach.txt  \n",
            "  inflating: test-set/face_alignment/13/triples/hyperparameters.txt  \n",
            "  inflating: test-set/face_alignment/13/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/13/triples/results.txt  \n",
            "   creating: test-set/face_alignment/14/\n",
            "  inflating: test-set/face_alignment/14/1707.05653v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/14/1707.05653v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/14/1707.05653v2.pdf  \n",
            "  inflating: test-set/face_alignment/14/entities.txt  \n",
            "   creating: test-set/face_alignment/14/info-units/\n",
            "  inflating: test-set/face_alignment/14/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/14/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/14/info-units/research-problem.json  \n",
            " extracting: test-set/face_alignment/14/section-line-nums.txt  \n",
            " extracting: test-set/face_alignment/14/sentences.txt  \n",
            "   creating: test-set/face_alignment/14/triples/\n",
            "  inflating: test-set/face_alignment/14/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/14/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/14/triples/research-problem.txt  \n",
            "   creating: test-set/face_alignment/15/\n",
            "  inflating: test-set/face_alignment/15/1511.07212v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/15/1511.07212v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/15/1511.07212v1.pdf  \n",
            "  inflating: test-set/face_alignment/15/entities.txt  \n",
            "   creating: test-set/face_alignment/15/info-units/\n",
            "  inflating: test-set/face_alignment/15/info-units/code.json  \n",
            "  inflating: test-set/face_alignment/15/info-units/experiments.json  \n",
            "  inflating: test-set/face_alignment/15/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/15/info-units/research-problem.json  \n",
            " extracting: test-set/face_alignment/15/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/15/sentences.txt  \n",
            "   creating: test-set/face_alignment/15/triples/\n",
            " extracting: test-set/face_alignment/15/triples/code.txt  \n",
            "  inflating: test-set/face_alignment/15/triples/experiments.txt  \n",
            "  inflating: test-set/face_alignment/15/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/15/triples/research-problem.txt  \n",
            "   creating: test-set/face_alignment/16/\n",
            "  inflating: test-set/face_alignment/16/1808.01558v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/16/1808.01558v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/16/1808.01558v2.pdf  \n",
            "  inflating: test-set/face_alignment/16/entities.txt  \n",
            "   creating: test-set/face_alignment/16/info-units/\n",
            "  inflating: test-set/face_alignment/16/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/16/info-units/baselines.json  \n",
            "  inflating: test-set/face_alignment/16/info-units/code.json  \n",
            "  inflating: test-set/face_alignment/16/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/16/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/16/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/16/info-units/results.json  \n",
            " extracting: test-set/face_alignment/16/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/16/sentences.txt  \n",
            "   creating: test-set/face_alignment/16/triples/\n",
            "  inflating: test-set/face_alignment/16/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_alignment/16/triples/baselines.txt  \n",
            " extracting: test-set/face_alignment/16/triples/code.txt  \n",
            "  inflating: test-set/face_alignment/16/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/16/triples/model.txt  \n",
            " extracting: test-set/face_alignment/16/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/16/triples/results.txt  \n",
            "   creating: test-set/face_alignment/17/\n",
            "  inflating: test-set/face_alignment/17/1908.06440v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/17/1908.06440v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/17/1908.06440v1.pdf  \n",
            "  inflating: test-set/face_alignment/17/entities.txt  \n",
            "   creating: test-set/face_alignment/17/info-units/\n",
            "  inflating: test-set/face_alignment/17/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/17/info-units/code.json  \n",
            "  inflating: test-set/face_alignment/17/info-units/experiments.json  \n",
            "  inflating: test-set/face_alignment/17/info-units/hyperparameters.json  \n",
            "  inflating: test-set/face_alignment/17/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/17/info-units/research-problem.json  \n",
            " extracting: test-set/face_alignment/17/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/17/sentences.txt  \n",
            "   creating: test-set/face_alignment/17/triples/\n",
            "  inflating: test-set/face_alignment/17/triples/ablation-analysis.txt  \n",
            " extracting: test-set/face_alignment/17/triples/code.txt  \n",
            "  inflating: test-set/face_alignment/17/triples/experiments.txt  \n",
            "  inflating: test-set/face_alignment/17/triples/hyperparameters.txt  \n",
            "  inflating: test-set/face_alignment/17/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/17/triples/research-problem.txt  \n",
            "   creating: test-set/face_alignment/18/\n",
            "  inflating: test-set/face_alignment/18/1706.01789v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/18/1706.01789v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/18/1706.01789v2.pdf  \n",
            "  inflating: test-set/face_alignment/18/entities.txt  \n",
            "   creating: test-set/face_alignment/18/info-units/\n",
            "  inflating: test-set/face_alignment/18/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/18/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/18/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/18/info-units/results.json  \n",
            " extracting: test-set/face_alignment/18/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/18/sentences.txt  \n",
            "   creating: test-set/face_alignment/18/triples/\n",
            "  inflating: test-set/face_alignment/18/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/18/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/18/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/18/triples/results.txt  \n",
            "  inflating: test-set/face_alignment/1/1703.07834v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/1/1703.07834v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/1/1703.07834v2.pdf  \n",
            "  inflating: test-set/face_alignment/1/entities.txt  \n",
            "   creating: test-set/face_alignment/1/info-units/\n",
            "  inflating: test-set/face_alignment/1/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/1/info-units/approach.json  \n",
            "  inflating: test-set/face_alignment/1/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/1/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/1/info-units/results.json  \n",
            " extracting: test-set/face_alignment/1/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/1/sentences.txt  \n",
            "   creating: test-set/face_alignment/1/triples/\n",
            "  inflating: test-set/face_alignment/1/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_alignment/1/triples/approach.txt  \n",
            "  inflating: test-set/face_alignment/1/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/1/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/1/triples/results.txt  \n",
            "   creating: test-set/face_alignment/2/\n",
            "  inflating: test-set/face_alignment/2/1904.02549v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/2/1904.02549v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/2/1904.02549v1.pdf  \n",
            "  inflating: test-set/face_alignment/2/entities.txt  \n",
            "   creating: test-set/face_alignment/2/info-units/\n",
            "  inflating: test-set/face_alignment/2/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/2/info-units/hyperparameters.json  \n",
            "  inflating: test-set/face_alignment/2/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/2/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/2/info-units/results.json  \n",
            " extracting: test-set/face_alignment/2/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/2/sentences.txt  \n",
            "   creating: test-set/face_alignment/2/triples/\n",
            "  inflating: test-set/face_alignment/2/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_alignment/2/triples/hyperparameters.txt  \n",
            "  inflating: test-set/face_alignment/2/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/2/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/2/triples/results.txt  \n",
            "   creating: test-set/face_alignment/3/\n",
            "  inflating: test-set/face_alignment/3/1904.07399v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/3/1904.07399v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/3/1904.07399v2.pdf  \n",
            "  inflating: test-set/face_alignment/3/entities.txt  \n",
            "   creating: test-set/face_alignment/3/info-units/\n",
            "  inflating: test-set/face_alignment/3/info-units/code.json  \n",
            "  inflating: test-set/face_alignment/3/info-units/hyperparameters.json  \n",
            "  inflating: test-set/face_alignment/3/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/3/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/3/info-units/results.json  \n",
            " extracting: test-set/face_alignment/3/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/3/sentences.txt  \n",
            "   creating: test-set/face_alignment/3/triples/\n",
            " extracting: test-set/face_alignment/3/triples/code.txt  \n",
            "  inflating: test-set/face_alignment/3/triples/hyperparameters.txt  \n",
            "  inflating: test-set/face_alignment/3/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/3/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/3/triples/results.txt  \n",
            "   creating: test-set/face_alignment/4/\n",
            "  inflating: test-set/face_alignment/4/1803.06598v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/4/1803.06598v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/4/1803.06598v1.pdf  \n",
            "  inflating: test-set/face_alignment/4/entities.txt  \n",
            "   creating: test-set/face_alignment/4/info-units/\n",
            "  inflating: test-set/face_alignment/4/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/4/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/4/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/4/info-units/results.json  \n",
            " extracting: test-set/face_alignment/4/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/4/sentences.txt  \n",
            "   creating: test-set/face_alignment/4/triples/\n",
            "  inflating: test-set/face_alignment/4/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/4/triples/model.txt  \n",
            " extracting: test-set/face_alignment/4/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/4/triples/results.txt  \n",
            "   creating: test-set/face_alignment/5/\n",
            "  inflating: test-set/face_alignment/5/1805.10483v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/5/1805.10483v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/5/1805.10483v1.pdf  \n",
            "  inflating: test-set/face_alignment/5/entities.txt  \n",
            "   creating: test-set/face_alignment/5/info-units/\n",
            "  inflating: test-set/face_alignment/5/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/5/info-units/approach.json  \n",
            "  inflating: test-set/face_alignment/5/info-units/code.json  \n",
            "  inflating: test-set/face_alignment/5/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/5/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/5/info-units/results.json  \n",
            " extracting: test-set/face_alignment/5/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/5/sentences.txt  \n",
            "   creating: test-set/face_alignment/5/triples/\n",
            "  inflating: test-set/face_alignment/5/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_alignment/5/triples/approach.txt  \n",
            " extracting: test-set/face_alignment/5/triples/code.txt  \n",
            "  inflating: test-set/face_alignment/5/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/5/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/5/triples/results.txt  \n",
            "   creating: test-set/face_alignment/6/\n",
            "  inflating: test-set/face_alignment/6/1902.01831v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/6/1902.01831v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/6/1902.01831v2.pdf  \n",
            "  inflating: test-set/face_alignment/6/entities.txt  \n",
            "   creating: test-set/face_alignment/6/info-units/\n",
            "  inflating: test-set/face_alignment/6/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/6/info-units/hyperparameters.json  \n",
            "  inflating: test-set/face_alignment/6/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/6/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/6/info-units/results.json  \n",
            " extracting: test-set/face_alignment/6/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/6/sentences.txt  \n",
            "   creating: test-set/face_alignment/6/triples/\n",
            "  inflating: test-set/face_alignment/6/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_alignment/6/triples/hyperparameters.txt  \n",
            "  inflating: test-set/face_alignment/6/triples/model.txt  \n",
            " extracting: test-set/face_alignment/6/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/6/triples/results.txt  \n",
            "   creating: test-set/face_alignment/7/\n",
            "  inflating: test-set/face_alignment/7/1803.07835v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/7/1803.07835v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/7/1803.07835v1.pdf  \n",
            "  inflating: test-set/face_alignment/7/entities.txt  \n",
            "   creating: test-set/face_alignment/7/info-units/\n",
            "  inflating: test-set/face_alignment/7/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/7/info-units/code.json  \n",
            "  inflating: test-set/face_alignment/7/info-units/hyperparameters.json  \n",
            "  inflating: test-set/face_alignment/7/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/7/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/7/info-units/results.json  \n",
            " extracting: test-set/face_alignment/7/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/7/sentences.txt  \n",
            "   creating: test-set/face_alignment/7/triples/\n",
            "  inflating: test-set/face_alignment/7/triples/ablation-analysis.txt  \n",
            " extracting: test-set/face_alignment/7/triples/code.txt  \n",
            "  inflating: test-set/face_alignment/7/triples/hyperparameters.txt  \n",
            "  inflating: test-set/face_alignment/7/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/7/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/7/triples/results.txt  \n",
            "   creating: test-set/face_alignment/8/\n",
            "  inflating: test-set/face_alignment/8/1903.09359v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/8/1903.09359v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/8/1903.09359v1.pdf  \n",
            "  inflating: test-set/face_alignment/8/entities.txt  \n",
            "   creating: test-set/face_alignment/8/info-units/\n",
            "  inflating: test-set/face_alignment/8/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/8/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/8/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/8/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/8/info-units/results.json  \n",
            " extracting: test-set/face_alignment/8/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/8/sentences.txt  \n",
            "   creating: test-set/face_alignment/8/triples/\n",
            "  inflating: test-set/face_alignment/8/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_alignment/8/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/8/triples/model.txt  \n",
            "  inflating: test-set/face_alignment/8/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/8/triples/results.txt  \n",
            "   creating: test-set/face_alignment/9/\n",
            "  inflating: test-set/face_alignment/9/1903.10661v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_alignment/9/1903.10661v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_alignment/9/1903.10661v1.pdf  \n",
            "  inflating: test-set/face_alignment/9/entities.txt  \n",
            "   creating: test-set/face_alignment/9/info-units/\n",
            "  inflating: test-set/face_alignment/9/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_alignment/9/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_alignment/9/info-units/model.json  \n",
            "  inflating: test-set/face_alignment/9/info-units/research-problem.json  \n",
            "  inflating: test-set/face_alignment/9/info-units/results.json  \n",
            " extracting: test-set/face_alignment/9/section-line-nums.txt  \n",
            "  inflating: test-set/face_alignment/9/sentences.txt  \n",
            "   creating: test-set/face_alignment/9/triples/\n",
            "  inflating: test-set/face_alignment/9/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_alignment/9/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_alignment/9/triples/model.txt  \n",
            " extracting: test-set/face_alignment/9/triples/research-problem.txt  \n",
            "  inflating: test-set/face_alignment/9/triples/results.txt  \n",
            "   creating: test-set/face_detection/\n",
            "   creating: test-set/face_detection/0/\n",
            "  inflating: test-set/face_detection/0/1905.01585v3-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/0/1905.01585v3-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/0/1905.01585v3.pdf  \n",
            "  inflating: test-set/face_detection/0/entities.txt  \n",
            "   creating: test-set/face_detection/0/info-units/\n",
            "  inflating: test-set/face_detection/0/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/0/info-units/model.json  \n",
            "  inflating: test-set/face_detection/0/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/0/info-units/results.json  \n",
            " extracting: test-set/face_detection/0/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/0/sentences.txt  \n",
            "   creating: test-set/face_detection/0/triples/\n",
            "  inflating: test-set/face_detection/0/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/0/triples/model.txt  \n",
            " extracting: test-set/face_detection/0/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/0/triples/results.txt  \n",
            "   creating: test-set/face_detection/1/\n",
            "   creating: test-set/face_detection/10/\n",
            "  inflating: test-set/face_detection/10/1810.10220v3-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/10/1810.10220v3-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/10/1810.10220v3.pdf  \n",
            "  inflating: test-set/face_detection/10/entities.txt  \n",
            "   creating: test-set/face_detection/10/info-units/\n",
            "  inflating: test-set/face_detection/10/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_detection/10/info-units/code.json  \n",
            "  inflating: test-set/face_detection/10/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/10/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/10/info-units/model.json  \n",
            "  inflating: test-set/face_detection/10/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/10/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/10/sentences.txt  \n",
            "   creating: test-set/face_detection/10/triples/\n",
            "  inflating: test-set/face_detection/10/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_detection/10/triples/code.txt  \n",
            "  inflating: test-set/face_detection/10/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/10/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/10/triples/model.txt  \n",
            " extracting: test-set/face_detection/10/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/11/\n",
            "  inflating: test-set/face_detection/11/1607.07155v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/11/1607.07155v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/11/1607.07155v1.pdf  \n",
            "  inflating: test-set/face_detection/11/entities.txt  \n",
            "   creating: test-set/face_detection/11/info-units/\n",
            "  inflating: test-set/face_detection/11/info-units/code.json  \n",
            "  inflating: test-set/face_detection/11/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/11/info-units/model.json  \n",
            "  inflating: test-set/face_detection/11/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/11/info-units/results.json  \n",
            " extracting: test-set/face_detection/11/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/11/sentences.txt  \n",
            "   creating: test-set/face_detection/11/triples/\n",
            " extracting: test-set/face_detection/11/triples/code.txt  \n",
            "  inflating: test-set/face_detection/11/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/11/triples/model.txt  \n",
            "  inflating: test-set/face_detection/11/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/11/triples/results.txt  \n",
            "   creating: test-set/face_detection/12/\n",
            "  inflating: test-set/face_detection/12/1904.13300v3-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/12/1904.13300v3-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/12/1904.13300v3.pdf  \n",
            "  inflating: test-set/face_detection/12/entities.txt  \n",
            "   creating: test-set/face_detection/12/info-units/\n",
            "  inflating: test-set/face_detection/12/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/12/info-units/model.json  \n",
            "  inflating: test-set/face_detection/12/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/12/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/12/sentences.txt  \n",
            "   creating: test-set/face_detection/12/triples/\n",
            "  inflating: test-set/face_detection/12/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/12/triples/model.txt  \n",
            "  inflating: test-set/face_detection/12/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/13/\n",
            "  inflating: test-set/face_detection/13/1905.00641v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/13/1905.00641v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/13/1905.00641v2.pdf  \n",
            "  inflating: test-set/face_detection/13/entities.txt  \n",
            "   creating: test-set/face_detection/13/info-units/\n",
            "  inflating: test-set/face_detection/13/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_detection/13/info-units/code.json  \n",
            "  inflating: test-set/face_detection/13/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/13/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/13/info-units/model.json  \n",
            "  inflating: test-set/face_detection/13/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/13/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/13/sentences.txt  \n",
            "   creating: test-set/face_detection/13/triples/\n",
            "  inflating: test-set/face_detection/13/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_detection/13/triples/code.txt  \n",
            "  inflating: test-set/face_detection/13/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/13/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/13/triples/model.txt  \n",
            "  inflating: test-set/face_detection/13/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/14/\n",
            "  inflating: test-set/face_detection/14/1511.06523v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/14/1511.06523v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/14/1511.06523v1.pdf  \n",
            "  inflating: test-set/face_detection/14/entities.txt  \n",
            "   creating: test-set/face_detection/14/info-units/\n",
            "  inflating: test-set/face_detection/14/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_detection/14/info-units/baselines.json  \n",
            "  inflating: test-set/face_detection/14/info-units/dataset.json  \n",
            "  inflating: test-set/face_detection/14/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/14/info-units/results.json  \n",
            " extracting: test-set/face_detection/14/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/14/sentences.txt  \n",
            "   creating: test-set/face_detection/14/triples/\n",
            "  inflating: test-set/face_detection/14/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_detection/14/triples/baselines.txt  \n",
            "  inflating: test-set/face_detection/14/triples/dataset.txt  \n",
            " extracting: test-set/face_detection/14/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/14/triples/results.txt  \n",
            "   creating: test-set/face_detection/15/\n",
            "  inflating: test-set/face_detection/15/1708.05234v4-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/15/1708.05234v4-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/15/1708.05234v4.pdf  \n",
            "  inflating: test-set/face_detection/15/entities.txt  \n",
            "   creating: test-set/face_detection/15/info-units/\n",
            "  inflating: test-set/face_detection/15/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_detection/15/info-units/code.json  \n",
            "  inflating: test-set/face_detection/15/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/15/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/15/info-units/model.json  \n",
            "  inflating: test-set/face_detection/15/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/15/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/15/sentences.txt  \n",
            "   creating: test-set/face_detection/15/triples/\n",
            "  inflating: test-set/face_detection/15/triples/ablation-analysis.txt  \n",
            " extracting: test-set/face_detection/15/triples/code.txt  \n",
            "  inflating: test-set/face_detection/15/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/15/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/15/triples/model.txt  \n",
            " extracting: test-set/face_detection/15/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/16/\n",
            "  inflating: test-set/face_detection/16/1603.01249v3-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/16/1603.01249v3-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/16/1603.01249v3.pdf  \n",
            "  inflating: test-set/face_detection/16/entities.txt  \n",
            "   creating: test-set/face_detection/16/info-units/\n",
            "  inflating: test-set/face_detection/16/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/16/info-units/model.json  \n",
            "  inflating: test-set/face_detection/16/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/16/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/16/sentences.txt  \n",
            "   creating: test-set/face_detection/16/triples/\n",
            "  inflating: test-set/face_detection/16/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/16/triples/model.txt  \n",
            "  inflating: test-set/face_detection/16/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/17/\n",
            "  inflating: test-set/face_detection/17/1803.07737v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/17/1803.07737v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/17/1803.07737v2.pdf  \n",
            "  inflating: test-set/face_detection/17/entities.txt  \n",
            "   creating: test-set/face_detection/17/info-units/\n",
            "  inflating: test-set/face_detection/17/info-units/code.json  \n",
            "  inflating: test-set/face_detection/17/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/17/info-units/model.json  \n",
            "  inflating: test-set/face_detection/17/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/17/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/17/sentences.txt  \n",
            "   creating: test-set/face_detection/17/triples/\n",
            "  inflating: test-set/face_detection/17/triples/code.txt  \n",
            "  inflating: test-set/face_detection/17/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/17/triples/model.txt  \n",
            " extracting: test-set/face_detection/17/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/18/\n",
            "  inflating: test-set/face_detection/18/1606.05413v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/18/1606.05413v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/18/1606.05413v1.pdf  \n",
            "  inflating: test-set/face_detection/18/entities.txt  \n",
            "   creating: test-set/face_detection/18/info-units/\n",
            "  inflating: test-set/face_detection/18/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/18/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/18/info-units/model.json  \n",
            "  inflating: test-set/face_detection/18/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/18/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/18/sentences.txt  \n",
            "   creating: test-set/face_detection/18/triples/\n",
            "  inflating: test-set/face_detection/18/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/18/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/18/triples/model.txt  \n",
            "  inflating: test-set/face_detection/18/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/19/\n",
            "  inflating: test-set/face_detection/19/1802.02142v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/19/1802.02142v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/19/1802.02142v1.pdf  \n",
            "  inflating: test-set/face_detection/19/entities.txt  \n",
            "   creating: test-set/face_detection/19/info-units/\n",
            "  inflating: test-set/face_detection/19/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/19/info-units/model.json  \n",
            "  inflating: test-set/face_detection/19/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/19/info-units/results.json  \n",
            " extracting: test-set/face_detection/19/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/19/sentences.txt  \n",
            "   creating: test-set/face_detection/19/triples/\n",
            "  inflating: test-set/face_detection/19/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/19/triples/model.txt  \n",
            " extracting: test-set/face_detection/19/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/19/triples/results.txt  \n",
            "  inflating: test-set/face_detection/1/1906.06579v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/1/1906.06579v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/1/1906.06579v2.pdf  \n",
            "  inflating: test-set/face_detection/1/entities.txt  \n",
            "   creating: test-set/face_detection/1/info-units/\n",
            "  inflating: test-set/face_detection/1/info-units/code.json  \n",
            "  inflating: test-set/face_detection/1/info-units/hyperparameters.json  \n",
            "  inflating: test-set/face_detection/1/info-units/model.json  \n",
            "  inflating: test-set/face_detection/1/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/1/info-units/results.json  \n",
            " extracting: test-set/face_detection/1/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/1/sentences.txt  \n",
            "   creating: test-set/face_detection/1/triples/\n",
            " extracting: test-set/face_detection/1/triples/code.txt  \n",
            "  inflating: test-set/face_detection/1/triples/hyperparameters.txt  \n",
            "  inflating: test-set/face_detection/1/triples/model.txt  \n",
            "  inflating: test-set/face_detection/1/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/1/triples/results.txt  \n",
            "   creating: test-set/face_detection/2/\n",
            "   creating: test-set/face_detection/20/\n",
            "  inflating: test-set/face_detection/20/1407.4023v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/20/1407.4023v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/20/1407.4023v2.pdf  \n",
            "  inflating: test-set/face_detection/20/entities.txt  \n",
            "   creating: test-set/face_detection/20/info-units/\n",
            "  inflating: test-set/face_detection/20/info-units/model.json  \n",
            "  inflating: test-set/face_detection/20/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/20/info-units/results.json  \n",
            " extracting: test-set/face_detection/20/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/20/sentences.txt  \n",
            "   creating: test-set/face_detection/20/triples/\n",
            "  inflating: test-set/face_detection/20/triples/model.txt  \n",
            "  inflating: test-set/face_detection/20/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/20/triples/results.txt  \n",
            "   creating: test-set/face_detection/21/\n",
            "  inflating: test-set/face_detection/21/1607.05477v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/21/1607.05477v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/21/1607.05477v1.pdf  \n",
            "  inflating: test-set/face_detection/21/entities.txt  \n",
            "   creating: test-set/face_detection/21/info-units/\n",
            "  inflating: test-set/face_detection/21/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_detection/21/info-units/baselines.json  \n",
            "  inflating: test-set/face_detection/21/info-units/model.json  \n",
            "  inflating: test-set/face_detection/21/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/21/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/21/sentences.txt  \n",
            "   creating: test-set/face_detection/21/triples/\n",
            "  inflating: test-set/face_detection/21/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_detection/21/triples/baselines.txt  \n",
            "  inflating: test-set/face_detection/21/triples/model.txt  \n",
            "  inflating: test-set/face_detection/21/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/2/1809.02693v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/2/1809.02693v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/2/1809.02693v1.pdf  \n",
            "  inflating: test-set/face_detection/2/entities.txt  \n",
            "   creating: test-set/face_detection/2/info-units/\n",
            "  inflating: test-set/face_detection/2/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/2/info-units/model.json  \n",
            "  inflating: test-set/face_detection/2/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/2/info-units/results.json  \n",
            " extracting: test-set/face_detection/2/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/2/sentences.txt  \n",
            "   creating: test-set/face_detection/2/triples/\n",
            "  inflating: test-set/face_detection/2/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/2/triples/model.txt  \n",
            "  inflating: test-set/face_detection/2/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/2/triples/results.txt  \n",
            "   creating: test-set/face_detection/3/\n",
            "  inflating: test-set/face_detection/3/1408.1656v3-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/3/1408.1656v3-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/3/1408.1656v3.pdf  \n",
            "  inflating: test-set/face_detection/3/entities.txt  \n",
            "   creating: test-set/face_detection/3/info-units/\n",
            "  inflating: test-set/face_detection/3/info-units/code.json  \n",
            "  inflating: test-set/face_detection/3/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/3/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/3/info-units/model.json  \n",
            "  inflating: test-set/face_detection/3/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/3/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/3/sentences.txt  \n",
            "   creating: test-set/face_detection/3/triples/\n",
            "  inflating: test-set/face_detection/3/triples/code.txt  \n",
            "  inflating: test-set/face_detection/3/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/3/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/3/triples/model.txt  \n",
            "  inflating: test-set/face_detection/3/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/4/\n",
            "  inflating: test-set/face_detection/4/1904.10633v3-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/4/1904.10633v3-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/4/1904.10633v3.pdf  \n",
            "  inflating: test-set/face_detection/4/entities.txt  \n",
            "   creating: test-set/face_detection/4/info-units/\n",
            "  inflating: test-set/face_detection/4/info-units/baselines.json  \n",
            "  inflating: test-set/face_detection/4/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/4/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/4/info-units/model.json  \n",
            "  inflating: test-set/face_detection/4/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/4/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/4/sentences.txt  \n",
            "   creating: test-set/face_detection/4/triples/\n",
            "  inflating: test-set/face_detection/4/triples/baselines.txt  \n",
            "  inflating: test-set/face_detection/4/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/4/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/4/triples/model.txt  \n",
            " extracting: test-set/face_detection/4/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/5/\n",
            "  inflating: test-set/face_detection/5/1604.02878v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/5/1604.02878v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/5/1604.02878v1.pdf  \n",
            "  inflating: test-set/face_detection/5/entities.txt  \n",
            "   creating: test-set/face_detection/5/info-units/\n",
            "  inflating: test-set/face_detection/5/info-units/baselines.json  \n",
            "  inflating: test-set/face_detection/5/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/5/info-units/model.json  \n",
            "  inflating: test-set/face_detection/5/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/5/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/5/sentences.txt  \n",
            "   creating: test-set/face_detection/5/triples/\n",
            "  inflating: test-set/face_detection/5/triples/baselines.txt  \n",
            "  inflating: test-set/face_detection/5/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/5/triples/model.txt  \n",
            "  inflating: test-set/face_detection/5/triples/research-problem.txt  \n",
            "   creating: test-set/face_detection/6/\n",
            "  inflating: test-set/face_detection/6/1811.11662v1-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/6/1811.11662v1-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/6/1811.11662v1.pdf  \n",
            "  inflating: test-set/face_detection/6/entities.txt  \n",
            "   creating: test-set/face_detection/6/info-units/\n",
            "  inflating: test-set/face_detection/6/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_detection/6/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/6/info-units/model.json  \n",
            "  inflating: test-set/face_detection/6/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/6/info-units/results.json  \n",
            " extracting: test-set/face_detection/6/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/6/sentences.txt  \n",
            "   creating: test-set/face_detection/6/triples/\n",
            "  inflating: test-set/face_detection/6/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/face_detection/6/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/6/triples/model.txt  \n",
            "  inflating: test-set/face_detection/6/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/6/triples/results.txt  \n",
            "   creating: test-set/face_detection/7/\n",
            "  inflating: test-set/face_detection/7/1707.09531v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/7/1707.09531v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/7/1707.09531v2.pdf  \n",
            "  inflating: test-set/face_detection/7/entities.txt  \n",
            "   creating: test-set/face_detection/7/info-units/\n",
            "  inflating: test-set/face_detection/7/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/face_detection/7/info-units/code.json  \n",
            "  inflating: test-set/face_detection/7/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/7/info-units/model.json  \n",
            "  inflating: test-set/face_detection/7/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/7/info-units/results.json  \n",
            " extracting: test-set/face_detection/7/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/7/sentences.txt  \n",
            "   creating: test-set/face_detection/7/triples/\n",
            "  inflating: test-set/face_detection/7/triples/ablation-analysis.txt  \n",
            " extracting: test-set/face_detection/7/triples/code.txt  \n",
            "  inflating: test-set/face_detection/7/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/7/triples/model.txt  \n",
            "  inflating: test-set/face_detection/7/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/7/triples/results.txt  \n",
            "   creating: test-set/face_detection/8/\n",
            "  inflating: test-set/face_detection/8/1709.05256v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/8/1709.05256v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/8/1709.05256v2.pdf  \n",
            "  inflating: test-set/face_detection/8/entities.txt  \n",
            "   creating: test-set/face_detection/8/info-units/\n",
            "  inflating: test-set/face_detection/8/info-units/experimental-setup.json  \n",
            "  inflating: test-set/face_detection/8/info-units/model.json  \n",
            "  inflating: test-set/face_detection/8/info-units/research-problem.json  \n",
            "  inflating: test-set/face_detection/8/info-units/results.json  \n",
            " extracting: test-set/face_detection/8/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/8/sentences.txt  \n",
            "   creating: test-set/face_detection/8/triples/\n",
            "  inflating: test-set/face_detection/8/triples/experimental-setup.txt  \n",
            "  inflating: test-set/face_detection/8/triples/model.txt  \n",
            "  inflating: test-set/face_detection/8/triples/research-problem.txt  \n",
            "  inflating: test-set/face_detection/8/triples/results.txt  \n",
            "   creating: test-set/face_detection/9/\n",
            "  inflating: test-set/face_detection/9/1612.04402v2-Grobid-out.txt  \n",
            "  inflating: test-set/face_detection/9/1612.04402v2-Stanza-out.txt  \n",
            "  inflating: test-set/face_detection/9/1612.04402v2.pdf  \n",
            "  inflating: test-set/face_detection/9/entities.txt  \n",
            "   creating: test-set/face_detection/9/info-units/\n",
            "  inflating: test-set/face_detection/9/info-units/approach.json  \n",
            "  inflating: test-set/face_detection/9/info-units/experiments.json  \n",
            "  inflating: test-set/face_detection/9/info-units/research-problem.json  \n",
            " extracting: test-set/face_detection/9/section-line-nums.txt  \n",
            "  inflating: test-set/face_detection/9/sentences.txt  \n",
            "   creating: test-set/face_detection/9/triples/\n",
            "  inflating: test-set/face_detection/9/triples/approach.txt  \n",
            "  inflating: test-set/face_detection/9/triples/experiments.txt  \n",
            "  inflating: test-set/face_detection/9/triples/research-problem.txt  \n",
            "   creating: test-set/hypernym_discovery/\n",
            "   creating: test-set/hypernym_discovery/0/\n",
            "  inflating: test-set/hypernym_discovery/0/entities.txt  \n",
            "   creating: test-set/hypernym_discovery/0/info-units/\n",
            "  inflating: test-set/hypernym_discovery/0/info-units/approach.json  \n",
            "  inflating: test-set/hypernym_discovery/0/info-units/research-problem.json  \n",
            "  inflating: test-set/hypernym_discovery/0/info-units/results.json  \n",
            "  inflating: test-set/hypernym_discovery/0/S18-1151-Grobid-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/0/S18-1151-Stanza-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/0/S18-1151.pdf  \n",
            " extracting: test-set/hypernym_discovery/0/section-line-nums.txt  \n",
            " extracting: test-set/hypernym_discovery/0/sentences.txt  \n",
            "   creating: test-set/hypernym_discovery/0/triples/\n",
            "  inflating: test-set/hypernym_discovery/0/triples/approach.txt  \n",
            "  inflating: test-set/hypernym_discovery/0/triples/research-problem.txt  \n",
            "  inflating: test-set/hypernym_discovery/0/triples/results.txt  \n",
            "   creating: test-set/hypernym_discovery/1/\n",
            "  inflating: test-set/hypernym_discovery/1/entities.txt  \n",
            "   creating: test-set/hypernym_discovery/1/info-units/\n",
            "  inflating: test-set/hypernym_discovery/1/info-units/experimental-setup.json  \n",
            "  inflating: test-set/hypernym_discovery/1/info-units/model.json  \n",
            "  inflating: test-set/hypernym_discovery/1/info-units/research-problem.json  \n",
            "  inflating: test-set/hypernym_discovery/1/info-units/results.json  \n",
            "  inflating: test-set/hypernym_discovery/1/S18-1147-Grobid-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/1/S18-1147-Stanza-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/1/S18-1147.pdf  \n",
            " extracting: test-set/hypernym_discovery/1/section-line-nums.txt  \n",
            "  inflating: test-set/hypernym_discovery/1/sentences.txt  \n",
            "   creating: test-set/hypernym_discovery/1/triples/\n",
            "  inflating: test-set/hypernym_discovery/1/triples/experimental-setup.txt  \n",
            "  inflating: test-set/hypernym_discovery/1/triples/model.txt  \n",
            "  inflating: test-set/hypernym_discovery/1/triples/research-problem.txt  \n",
            "  inflating: test-set/hypernym_discovery/1/triples/results.txt  \n",
            "   creating: test-set/hypernym_discovery/2/\n",
            "  inflating: test-set/hypernym_discovery/2/1612.04460v2-Grobid-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/2/1612.04460v2-Stanza-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/2/1612.04460v2.pdf  \n",
            "  inflating: test-set/hypernym_discovery/2/entities.txt  \n",
            "   creating: test-set/hypernym_discovery/2/info-units/\n",
            "  inflating: test-set/hypernym_discovery/2/info-units/approach.json  \n",
            "  inflating: test-set/hypernym_discovery/2/info-units/experiments.json  \n",
            "  inflating: test-set/hypernym_discovery/2/info-units/research-problem.json  \n",
            " extracting: test-set/hypernym_discovery/2/section-line-nums.txt  \n",
            "  inflating: test-set/hypernym_discovery/2/sentences.txt  \n",
            "   creating: test-set/hypernym_discovery/2/triples/\n",
            "  inflating: test-set/hypernym_discovery/2/triples/approach.txt  \n",
            "  inflating: test-set/hypernym_discovery/2/triples/experiments.txt  \n",
            "  inflating: test-set/hypernym_discovery/2/triples/research-problem.txt  \n",
            "   creating: test-set/hypernym_discovery/3/\n",
            "  inflating: test-set/hypernym_discovery/3/D16-1041-Grobid-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/3/D16-1041-Stanza-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/3/D16-1041.pdf  \n",
            "  inflating: test-set/hypernym_discovery/3/entities.txt  \n",
            "   creating: test-set/hypernym_discovery/3/info-units/\n",
            "  inflating: test-set/hypernym_discovery/3/info-units/baselines.json  \n",
            "  inflating: test-set/hypernym_discovery/3/info-units/code.json  \n",
            "  inflating: test-set/hypernym_discovery/3/info-units/model.json  \n",
            "  inflating: test-set/hypernym_discovery/3/info-units/research-problem.json  \n",
            "  inflating: test-set/hypernym_discovery/3/info-units/results.json  \n",
            " extracting: test-set/hypernym_discovery/3/section-line-nums.txt  \n",
            "  inflating: test-set/hypernym_discovery/3/sentences.txt  \n",
            "   creating: test-set/hypernym_discovery/3/triples/\n",
            "  inflating: test-set/hypernym_discovery/3/triples/baselines.txt  \n",
            " extracting: test-set/hypernym_discovery/3/triples/code.txt  \n",
            "  inflating: test-set/hypernym_discovery/3/triples/model.txt  \n",
            "  inflating: test-set/hypernym_discovery/3/triples/research-problem.txt  \n",
            "  inflating: test-set/hypernym_discovery/3/triples/results.txt  \n",
            "   creating: test-set/hypernym_discovery/4/\n",
            "  inflating: test-set/hypernym_discovery/4/entities.txt  \n",
            "   creating: test-set/hypernym_discovery/4/info-units/\n",
            "  inflating: test-set/hypernym_discovery/4/info-units/hyperparameters.json  \n",
            "  inflating: test-set/hypernym_discovery/4/info-units/research-problem.json  \n",
            "  inflating: test-set/hypernym_discovery/4/info-units/results.json  \n",
            "  inflating: test-set/hypernym_discovery/4/S18-1148-Grobid-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/4/S18-1148-Stanza-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/4/S18-1148.pdf  \n",
            " extracting: test-set/hypernym_discovery/4/section-line-nums.txt  \n",
            "  inflating: test-set/hypernym_discovery/4/sentences.txt  \n",
            "   creating: test-set/hypernym_discovery/4/triples/\n",
            "  inflating: test-set/hypernym_discovery/4/triples/hyperparameters.txt  \n",
            "  inflating: test-set/hypernym_discovery/4/triples/research-problem.txt  \n",
            "  inflating: test-set/hypernym_discovery/4/triples/results.txt  \n",
            "   creating: test-set/hypernym_discovery/5/\n",
            "  inflating: test-set/hypernym_discovery/5/entities.txt  \n",
            "   creating: test-set/hypernym_discovery/5/info-units/\n",
            "  inflating: test-set/hypernym_discovery/5/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/hypernym_discovery/5/info-units/model.json  \n",
            "  inflating: test-set/hypernym_discovery/5/info-units/research-problem.json  \n",
            "  inflating: test-set/hypernym_discovery/5/info-units/results.json  \n",
            "  inflating: test-set/hypernym_discovery/5/S18-1116-Grobid-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/5/S18-1116-Stanza-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/5/S18-1116.pdf  \n",
            " extracting: test-set/hypernym_discovery/5/section-line-nums.txt  \n",
            "  inflating: test-set/hypernym_discovery/5/sentences.txt  \n",
            "   creating: test-set/hypernym_discovery/5/triples/\n",
            "  inflating: test-set/hypernym_discovery/5/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/hypernym_discovery/5/triples/model.txt  \n",
            " extracting: test-set/hypernym_discovery/5/triples/research-problem.txt  \n",
            "  inflating: test-set/hypernym_discovery/5/triples/results.txt  \n",
            "   creating: test-set/hypernym_discovery/6/\n",
            "  inflating: test-set/hypernym_discovery/6/entities.txt  \n",
            "   creating: test-set/hypernym_discovery/6/info-units/\n",
            "  inflating: test-set/hypernym_discovery/6/info-units/approach.json  \n",
            "  inflating: test-set/hypernym_discovery/6/info-units/research-problem.json  \n",
            "  inflating: test-set/hypernym_discovery/6/info-units/results.json  \n",
            "  inflating: test-set/hypernym_discovery/6/S18-1150-Grobid-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/6/S18-1150-Stanza-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/6/S18-1150.pdf  \n",
            " extracting: test-set/hypernym_discovery/6/section-line-nums.txt  \n",
            " extracting: test-set/hypernym_discovery/6/sentences.txt  \n",
            "   creating: test-set/hypernym_discovery/6/triples/\n",
            "  inflating: test-set/hypernym_discovery/6/triples/approach.txt  \n",
            " extracting: test-set/hypernym_discovery/6/triples/research-problem.txt  \n",
            "  inflating: test-set/hypernym_discovery/6/triples/results.txt  \n",
            "   creating: test-set/hypernym_discovery/7/\n",
            "  inflating: test-set/hypernym_discovery/7/entities.txt  \n",
            "   creating: test-set/hypernym_discovery/7/info-units/\n",
            "  inflating: test-set/hypernym_discovery/7/info-units/model.json  \n",
            "  inflating: test-set/hypernym_discovery/7/info-units/research-problem.json  \n",
            "  inflating: test-set/hypernym_discovery/7/info-units/results.json  \n",
            "  inflating: test-set/hypernym_discovery/7/S18-1152-Grobid-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/7/S18-1152-Stanza-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/7/S18-1152.pdf  \n",
            " extracting: test-set/hypernym_discovery/7/section-line-nums.txt  \n",
            " extracting: test-set/hypernym_discovery/7/sentences.txt  \n",
            "   creating: test-set/hypernym_discovery/7/triples/\n",
            "  inflating: test-set/hypernym_discovery/7/triples/model.txt  \n",
            " extracting: test-set/hypernym_discovery/7/triples/research-problem.txt  \n",
            "  inflating: test-set/hypernym_discovery/7/triples/results.txt  \n",
            "   creating: test-set/hypernym_discovery/8/\n",
            "  inflating: test-set/hypernym_discovery/8/entities.txt  \n",
            "   creating: test-set/hypernym_discovery/8/info-units/\n",
            "  inflating: test-set/hypernym_discovery/8/info-units/model.json  \n",
            "  inflating: test-set/hypernym_discovery/8/info-units/research-problem.json  \n",
            "  inflating: test-set/hypernym_discovery/8/info-units/results.json  \n",
            "  inflating: test-set/hypernym_discovery/8/S18-1146-Grobid-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/8/S18-1146-Stanza-out.txt  \n",
            "  inflating: test-set/hypernym_discovery/8/S18-1146.pdf  \n",
            " extracting: test-set/hypernym_discovery/8/section-line-nums.txt  \n",
            " extracting: test-set/hypernym_discovery/8/sentences.txt  \n",
            "   creating: test-set/hypernym_discovery/8/triples/\n",
            "  inflating: test-set/hypernym_discovery/8/triples/model.txt  \n",
            " extracting: test-set/hypernym_discovery/8/triples/research-problem.txt  \n",
            "  inflating: test-set/hypernym_discovery/8/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/\n",
            "   creating: test-set/natural_language_inference/0/\n",
            "  inflating: test-set/natural_language_inference/0/1804.05922v1-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/0/1804.05922v1-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/0/1804.05922v1.pdf  \n",
            "  inflating: test-set/natural_language_inference/0/entities.txt  \n",
            "   creating: test-set/natural_language_inference/0/info-units/\n",
            "  inflating: test-set/natural_language_inference/0/info-units/approach.json  \n",
            "  inflating: test-set/natural_language_inference/0/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/0/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/0/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/0/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/0/triples/\n",
            "  inflating: test-set/natural_language_inference/0/triples/approach.txt  \n",
            "  inflating: test-set/natural_language_inference/0/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/0/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/1/\n",
            "   creating: test-set/natural_language_inference/10/\n",
            "  inflating: test-set/natural_language_inference/10/entities.txt  \n",
            "   creating: test-set/natural_language_inference/10/info-units/\n",
            "  inflating: test-set/natural_language_inference/10/info-units/hyperparameters.json  \n",
            "  inflating: test-set/natural_language_inference/10/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/10/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/10/info-units/results.json  \n",
            "  inflating: test-set/natural_language_inference/10/N18-2015-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/10/N18-2015-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/10/N18-2015.pdf  \n",
            " extracting: test-set/natural_language_inference/10/section-line-nums.txt  \n",
            " extracting: test-set/natural_language_inference/10/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/10/triples/\n",
            "  inflating: test-set/natural_language_inference/10/triples/hyperparameters.txt  \n",
            "  inflating: test-set/natural_language_inference/10/triples/model.txt  \n",
            " extracting: test-set/natural_language_inference/10/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/10/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/11/\n",
            "  inflating: test-set/natural_language_inference/11/1611.01604v4-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/11/1611.01604v4-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/11/1611.01604v4.pdf  \n",
            "  inflating: test-set/natural_language_inference/11/entities.txt  \n",
            "   creating: test-set/natural_language_inference/11/info-units/\n",
            "  inflating: test-set/natural_language_inference/11/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/11/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/11/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/11/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/11/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/11/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/11/triples/\n",
            "  inflating: test-set/natural_language_inference/11/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/11/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/11/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/11/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/12/\n",
            "  inflating: test-set/natural_language_inference/12/1801.08459v2-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/12/1801.08459v2-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/12/1801.08459v2.pdf  \n",
            "  inflating: test-set/natural_language_inference/12/entities.txt  \n",
            "   creating: test-set/natural_language_inference/12/info-units/\n",
            "  inflating: test-set/natural_language_inference/12/info-units/code.json  \n",
            "  inflating: test-set/natural_language_inference/12/info-units/hyperparameters.json  \n",
            "  inflating: test-set/natural_language_inference/12/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/12/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/12/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/12/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/12/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/12/triples/\n",
            " extracting: test-set/natural_language_inference/12/triples/code.txt  \n",
            "  inflating: test-set/natural_language_inference/12/triples/hyperparameters.txt  \n",
            "  inflating: test-set/natural_language_inference/12/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/12/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/12/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/13/\n",
            "  inflating: test-set/natural_language_inference/13/1606.02270v2-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/13/1606.02270v2-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/13/1606.02270v2.pdf  \n",
            "  inflating: test-set/natural_language_inference/13/entities.txt  \n",
            "   creating: test-set/natural_language_inference/13/info-units/\n",
            "  inflating: test-set/natural_language_inference/13/info-units/approach.json  \n",
            "  inflating: test-set/natural_language_inference/13/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/13/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/13/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/13/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/13/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/13/triples/\n",
            "  inflating: test-set/natural_language_inference/13/triples/approach.txt  \n",
            "  inflating: test-set/natural_language_inference/13/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/13/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/13/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/14/\n",
            "  inflating: test-set/natural_language_inference/14/1503.08895v5-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/14/1503.08895v5-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/14/1503.08895v5.pdf  \n",
            "  inflating: test-set/natural_language_inference/14/entities.txt  \n",
            "   creating: test-set/natural_language_inference/14/info-units/\n",
            "  inflating: test-set/natural_language_inference/14/info-units/baselines.json  \n",
            "  inflating: test-set/natural_language_inference/14/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/14/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/14/info-units/research-problem.json  \n",
            " extracting: test-set/natural_language_inference/14/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/14/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/14/triples/\n",
            "  inflating: test-set/natural_language_inference/14/triples/baselines.txt  \n",
            "  inflating: test-set/natural_language_inference/14/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/14/triples/model.txt  \n",
            " extracting: test-set/natural_language_inference/14/triples/research-problem.txt  \n",
            "   creating: test-set/natural_language_inference/15/\n",
            "  inflating: test-set/natural_language_inference/15/1711.04289v3-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/15/1711.04289v3-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/15/1711.04289v3.pdf  \n",
            "  inflating: test-set/natural_language_inference/15/entities.txt  \n",
            "   creating: test-set/natural_language_inference/15/info-units/\n",
            "  inflating: test-set/natural_language_inference/15/info-units/hyperparameters.json  \n",
            "  inflating: test-set/natural_language_inference/15/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/15/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/15/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/15/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/15/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/15/triples/\n",
            "  inflating: test-set/natural_language_inference/15/triples/hyperparameters.txt  \n",
            "  inflating: test-set/natural_language_inference/15/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/15/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/15/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/16/\n",
            "  inflating: test-set/natural_language_inference/16/1603.01547v2-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/16/1603.01547v2-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/16/1603.01547v2.pdf  \n",
            "  inflating: test-set/natural_language_inference/16/entities.txt  \n",
            "   creating: test-set/natural_language_inference/16/info-units/\n",
            "  inflating: test-set/natural_language_inference/16/info-units/hyperparameters.json  \n",
            "  inflating: test-set/natural_language_inference/16/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/16/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/16/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/16/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/16/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/16/triples/\n",
            "  inflating: test-set/natural_language_inference/16/triples/hyperparameters.txt  \n",
            "  inflating: test-set/natural_language_inference/16/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/16/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/16/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/17/\n",
            "  inflating: test-set/natural_language_inference/17/1804.07461v3-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/17/1804.07461v3-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/17/1804.07461v3.pdf  \n",
            "  inflating: test-set/natural_language_inference/17/entities.txt  \n",
            "   creating: test-set/natural_language_inference/17/info-units/\n",
            "  inflating: test-set/natural_language_inference/17/info-units/baselines.json  \n",
            "  inflating: test-set/natural_language_inference/17/info-units/code.json  \n",
            "  inflating: test-set/natural_language_inference/17/info-units/dataset.json  \n",
            "  inflating: test-set/natural_language_inference/17/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/17/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/17/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/17/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/17/triples/\n",
            "  inflating: test-set/natural_language_inference/17/triples/baselines.txt  \n",
            "  inflating: test-set/natural_language_inference/17/triples/code.txt  \n",
            "  inflating: test-set/natural_language_inference/17/triples/dataset.txt  \n",
            "  inflating: test-set/natural_language_inference/17/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/17/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/18/\n",
            "  inflating: test-set/natural_language_inference/18/1812.01216v1-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/18/1812.01216v1-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/18/1812.01216v1.pdf  \n",
            "  inflating: test-set/natural_language_inference/18/entities.txt  \n",
            "   creating: test-set/natural_language_inference/18/info-units/\n",
            "  inflating: test-set/natural_language_inference/18/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/18/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/18/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/18/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/18/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/18/triples/\n",
            "  inflating: test-set/natural_language_inference/18/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/18/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/18/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/19/\n",
            "  inflating: test-set/natural_language_inference/19/1512.08422v3-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/19/1512.08422v3-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/19/1512.08422v3.pdf  \n",
            "  inflating: test-set/natural_language_inference/19/entities.txt  \n",
            "   creating: test-set/natural_language_inference/19/info-units/\n",
            "  inflating: test-set/natural_language_inference/19/info-units/hyperparameters.json  \n",
            "  inflating: test-set/natural_language_inference/19/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/19/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/19/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/19/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/19/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/19/triples/\n",
            "  inflating: test-set/natural_language_inference/19/triples/hyperparameters.txt  \n",
            "  inflating: test-set/natural_language_inference/19/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/19/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/19/triples/results.txt  \n",
            "  inflating: test-set/natural_language_inference/1/D18-1054-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/1/D18-1054-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/1/D18-1054.pdf  \n",
            "  inflating: test-set/natural_language_inference/1/entities.txt  \n",
            "   creating: test-set/natural_language_inference/1/info-units/\n",
            "  inflating: test-set/natural_language_inference/1/info-units/baselines.json  \n",
            "  inflating: test-set/natural_language_inference/1/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/1/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/1/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/1/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/1/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/1/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/1/triples/\n",
            "  inflating: test-set/natural_language_inference/1/triples/baselines.txt  \n",
            "  inflating: test-set/natural_language_inference/1/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/1/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/1/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/1/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/2/\n",
            "   creating: test-set/natural_language_inference/20/\n",
            "  inflating: test-set/natural_language_inference/20/1607.04492v2-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/20/1607.04492v2-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/20/1607.04492v2.pdf  \n",
            "  inflating: test-set/natural_language_inference/20/entities.txt  \n",
            "   creating: test-set/natural_language_inference/20/info-units/\n",
            "  inflating: test-set/natural_language_inference/20/info-units/experiments.json  \n",
            "  inflating: test-set/natural_language_inference/20/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/20/info-units/research-problem.json  \n",
            " extracting: test-set/natural_language_inference/20/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/20/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/20/triples/\n",
            "  inflating: test-set/natural_language_inference/20/triples/experiments.txt  \n",
            "  inflating: test-set/natural_language_inference/20/triples/model.txt  \n",
            " extracting: test-set/natural_language_inference/20/triples/research-problem.txt  \n",
            "   creating: test-set/natural_language_inference/21/\n",
            "  inflating: test-set/natural_language_inference/21/1607.04423v4-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/21/1607.04423v4-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/21/1607.04423v4.pdf  \n",
            "  inflating: test-set/natural_language_inference/21/entities.txt  \n",
            "   creating: test-set/natural_language_inference/21/info-units/\n",
            "  inflating: test-set/natural_language_inference/21/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/natural_language_inference/21/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/21/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/21/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/21/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/21/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/21/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/21/triples/\n",
            "  inflating: test-set/natural_language_inference/21/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/natural_language_inference/21/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/21/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/21/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/21/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/22/\n",
            "  inflating: test-set/natural_language_inference/22/1911.07405v1-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/22/1911.07405v1-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/22/1911.07405v1.pdf  \n",
            "  inflating: test-set/natural_language_inference/22/entities.txt  \n",
            "   creating: test-set/natural_language_inference/22/info-units/\n",
            "  inflating: test-set/natural_language_inference/22/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/natural_language_inference/22/info-units/baselines.json  \n",
            "  inflating: test-set/natural_language_inference/22/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/22/info-units/experiments.json  \n",
            "  inflating: test-set/natural_language_inference/22/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/22/info-units/research-problem.json  \n",
            " extracting: test-set/natural_language_inference/22/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/22/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/22/triples/\n",
            "  inflating: test-set/natural_language_inference/22/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/natural_language_inference/22/triples/baselines.txt  \n",
            "  inflating: test-set/natural_language_inference/22/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/22/triples/experiments.txt  \n",
            "  inflating: test-set/natural_language_inference/22/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/22/triples/research-problem.txt  \n",
            "   creating: test-set/natural_language_inference/23/\n",
            "  inflating: test-set/natural_language_inference/23/entities.txt  \n",
            "   creating: test-set/natural_language_inference/23/info-units/\n",
            "  inflating: test-set/natural_language_inference/23/info-units/baselines.json  \n",
            "  inflating: test-set/natural_language_inference/23/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/23/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/23/info-units/results.json  \n",
            "  inflating: test-set/natural_language_inference/23/P16-1098-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/23/P16-1098-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/23/P16-1098.pdf  \n",
            " extracting: test-set/natural_language_inference/23/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/23/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/23/triples/\n",
            "  inflating: test-set/natural_language_inference/23/triples/baselines.txt  \n",
            "  inflating: test-set/natural_language_inference/23/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/23/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/23/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/24/\n",
            "  inflating: test-set/natural_language_inference/24/1704.00051v2-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/24/1704.00051v2-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/24/1704.00051v2.pdf  \n",
            "  inflating: test-set/natural_language_inference/24/entities.txt  \n",
            "   creating: test-set/natural_language_inference/24/info-units/\n",
            "  inflating: test-set/natural_language_inference/24/info-units/hyperparameters.json  \n",
            "  inflating: test-set/natural_language_inference/24/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/24/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/24/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/24/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/24/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/24/triples/\n",
            "  inflating: test-set/natural_language_inference/24/triples/hyperparameters.txt  \n",
            "  inflating: test-set/natural_language_inference/24/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/24/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/24/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/25/\n",
            "  inflating: test-set/natural_language_inference/25/1811.11374v1-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/25/1811.11374v1-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/25/1811.11374v1.pdf  \n",
            "  inflating: test-set/natural_language_inference/25/entities.txt  \n",
            "   creating: test-set/natural_language_inference/25/info-units/\n",
            "  inflating: test-set/natural_language_inference/25/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/natural_language_inference/25/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/25/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/25/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/25/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/25/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/25/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/25/triples/\n",
            "  inflating: test-set/natural_language_inference/25/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/natural_language_inference/25/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/25/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/25/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/25/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/26/\n",
            "  inflating: test-set/natural_language_inference/26/1810.06638v1-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/26/1810.06638v1-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/26/1810.06638v1.pdf  \n",
            "  inflating: test-set/natural_language_inference/26/entities.txt  \n",
            "   creating: test-set/natural_language_inference/26/info-units/\n",
            "  inflating: test-set/natural_language_inference/26/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/natural_language_inference/26/info-units/hyperparameters.json  \n",
            "  inflating: test-set/natural_language_inference/26/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/26/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/26/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/26/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/26/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/26/triples/\n",
            "  inflating: test-set/natural_language_inference/26/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/natural_language_inference/26/triples/hyperparameters.txt  \n",
            "  inflating: test-set/natural_language_inference/26/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/26/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/26/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/27/\n",
            "  inflating: test-set/natural_language_inference/27/1812.03593v5-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/27/1812.03593v5-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/27/1812.03593v5.pdf  \n",
            "  inflating: test-set/natural_language_inference/27/entities.txt  \n",
            "   creating: test-set/natural_language_inference/27/info-units/\n",
            "  inflating: test-set/natural_language_inference/27/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/27/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/27/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/27/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/27/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/27/triples/\n",
            "  inflating: test-set/natural_language_inference/27/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/27/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/27/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/28/\n",
            "  inflating: test-set/natural_language_inference/28/1612.03969v3-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/28/1612.03969v3-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/28/1612.03969v3.pdf  \n",
            "  inflating: test-set/natural_language_inference/28/entities.txt  \n",
            "   creating: test-set/natural_language_inference/28/info-units/\n",
            "  inflating: test-set/natural_language_inference/28/info-units/experiments.json  \n",
            "  inflating: test-set/natural_language_inference/28/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/28/info-units/research-problem.json  \n",
            " extracting: test-set/natural_language_inference/28/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/28/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/28/triples/\n",
            "  inflating: test-set/natural_language_inference/28/triples/experiments.txt  \n",
            "  inflating: test-set/natural_language_inference/28/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/28/triples/research-problem.txt  \n",
            "   creating: test-set/natural_language_inference/29/\n",
            "  inflating: test-set/natural_language_inference/29/1710.10504v2-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/29/1710.10504v2-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/29/1710.10504v2.pdf  \n",
            "  inflating: test-set/natural_language_inference/29/entities.txt  \n",
            "   creating: test-set/natural_language_inference/29/info-units/\n",
            "  inflating: test-set/natural_language_inference/29/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/29/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/29/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/29/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/29/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/29/triples/\n",
            "  inflating: test-set/natural_language_inference/29/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/29/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/29/triples/results.txt  \n",
            "  inflating: test-set/natural_language_inference/2/1804.07888v2-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/2/1804.07888v2-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/2/1804.07888v2.pdf  \n",
            "  inflating: test-set/natural_language_inference/2/entities.txt  \n",
            "   creating: test-set/natural_language_inference/2/info-units/\n",
            "  inflating: test-set/natural_language_inference/2/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/2/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/2/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/2/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/2/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/2/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/2/triples/\n",
            "  inflating: test-set/natural_language_inference/2/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/2/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/2/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/2/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/3/\n",
            "   creating: test-set/natural_language_inference/30/\n",
            "  inflating: test-set/natural_language_inference/30/1503.03244v1-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/30/1503.03244v1-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/30/1503.03244v1.pdf  \n",
            "  inflating: test-set/natural_language_inference/30/entities.txt  \n",
            "   creating: test-set/natural_language_inference/30/info-units/\n",
            "  inflating: test-set/natural_language_inference/30/info-units/experiments.json  \n",
            "  inflating: test-set/natural_language_inference/30/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/30/info-units/research-problem.json  \n",
            " extracting: test-set/natural_language_inference/30/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/30/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/30/triples/\n",
            "  inflating: test-set/natural_language_inference/30/triples/experiments.txt  \n",
            "  inflating: test-set/natural_language_inference/30/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/30/triples/research-problem.txt  \n",
            "   creating: test-set/natural_language_inference/31/\n",
            "  inflating: test-set/natural_language_inference/31/1610.09027v1-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/31/1610.09027v1-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/31/1610.09027v1.pdf  \n",
            "  inflating: test-set/natural_language_inference/31/entities.txt  \n",
            "   creating: test-set/natural_language_inference/31/info-units/\n",
            "  inflating: test-set/natural_language_inference/31/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/31/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/31/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/31/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/31/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/31/triples/\n",
            "  inflating: test-set/natural_language_inference/31/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/31/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/31/triples/results.txt  \n",
            "  inflating: test-set/natural_language_inference/3/1703.04617v2-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/3/1703.04617v2-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/3/1703.04617v2.pdf  \n",
            "  inflating: test-set/natural_language_inference/3/entities.txt  \n",
            "   creating: test-set/natural_language_inference/3/info-units/\n",
            "  inflating: test-set/natural_language_inference/3/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/3/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/3/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/3/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/3/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/3/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/3/triples/\n",
            "  inflating: test-set/natural_language_inference/3/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/3/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/3/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/3/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/4/\n",
            "  inflating: test-set/natural_language_inference/4/D18-1237-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/4/D18-1237-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/4/D18-1237.pdf  \n",
            "  inflating: test-set/natural_language_inference/4/entities.txt  \n",
            "   creating: test-set/natural_language_inference/4/info-units/\n",
            "  inflating: test-set/natural_language_inference/4/info-units/ablation-analysis.json  \n",
            "  inflating: test-set/natural_language_inference/4/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/4/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/4/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/4/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/4/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/4/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/4/triples/\n",
            "  inflating: test-set/natural_language_inference/4/triples/ablation-analysis.txt  \n",
            "  inflating: test-set/natural_language_inference/4/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/4/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/4/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/4/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/5/\n",
            "  inflating: test-set/natural_language_inference/5/1602.07019v2-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/5/1602.07019v2-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/5/1602.07019v2.pdf  \n",
            "  inflating: test-set/natural_language_inference/5/entities.txt  \n",
            "   creating: test-set/natural_language_inference/5/info-units/\n",
            "  inflating: test-set/natural_language_inference/5/info-units/experiments.json  \n",
            "  inflating: test-set/natural_language_inference/5/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/5/info-units/research-problem.json  \n",
            " extracting: test-set/natural_language_inference/5/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/5/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/5/triples/\n",
            "  inflating: test-set/natural_language_inference/5/triples/experiments.txt  \n",
            "  inflating: test-set/natural_language_inference/5/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/5/triples/research-problem.txt  \n",
            "   creating: test-set/natural_language_inference/6/\n",
            "  inflating: test-set/natural_language_inference/6/1808.07383v1-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/6/1808.07383v1-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/6/1808.07383v1.pdf  \n",
            "  inflating: test-set/natural_language_inference/6/entities.txt  \n",
            "   creating: test-set/natural_language_inference/6/info-units/\n",
            "  inflating: test-set/natural_language_inference/6/info-units/experiments.json  \n",
            "  inflating: test-set/natural_language_inference/6/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/6/info-units/research-problem.json  \n",
            " extracting: test-set/natural_language_inference/6/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/6/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/6/triples/\n",
            "  inflating: test-set/natural_language_inference/6/triples/experiments.txt  \n",
            "  inflating: test-set/natural_language_inference/6/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/6/triples/research-problem.txt  \n",
            "   creating: test-set/natural_language_inference/7/\n",
            "  inflating: test-set/natural_language_inference/7/1506.03340v3-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/7/1506.03340v3-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/7/1506.03340v3.pdf  \n",
            "  inflating: test-set/natural_language_inference/7/entities.txt  \n",
            "   creating: test-set/natural_language_inference/7/info-units/\n",
            "  inflating: test-set/natural_language_inference/7/info-units/approach.json  \n",
            "  inflating: test-set/natural_language_inference/7/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/7/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/7/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/7/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/7/triples/\n",
            "  inflating: test-set/natural_language_inference/7/triples/approach.txt  \n",
            "  inflating: test-set/natural_language_inference/7/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/7/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/8/\n",
            "  inflating: test-set/natural_language_inference/8/1605.09090v1-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/8/1605.09090v1-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/8/1605.09090v1.pdf  \n",
            "  inflating: test-set/natural_language_inference/8/entities.txt  \n",
            "   creating: test-set/natural_language_inference/8/info-units/\n",
            "  inflating: test-set/natural_language_inference/8/info-units/hyperparameters.json  \n",
            "  inflating: test-set/natural_language_inference/8/info-units/model.json  \n",
            "  inflating: test-set/natural_language_inference/8/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/8/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/8/section-line-nums.txt  \n",
            "  inflating: test-set/natural_language_inference/8/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/8/triples/\n",
            "  inflating: test-set/natural_language_inference/8/triples/hyperparameters.txt  \n",
            "  inflating: test-set/natural_language_inference/8/triples/model.txt  \n",
            "  inflating: test-set/natural_language_inference/8/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/8/triples/results.txt  \n",
            "   creating: test-set/natural_language_inference/9/\n",
            "  inflating: test-set/natural_language_inference/9/1901.08634v3-Grobid-out.txt  \n",
            "  inflating: test-set/natural_language_inference/9/1901.08634v3-Stanza-out.txt  \n",
            "  inflating: test-set/natural_language_inference/9/1901.08634v3.pdf  \n",
            "  inflating: test-set/natural_language_inference/9/entities.txt  \n",
            "   creating: test-set/natural_language_inference/9/info-units/\n",
            "  inflating: test-set/natural_language_inference/9/info-units/approach.json  \n",
            "  inflating: test-set/natural_language_inference/9/info-units/experimental-setup.json  \n",
            "  inflating: test-set/natural_language_inference/9/info-units/research-problem.json  \n",
            "  inflating: test-set/natural_language_inference/9/info-units/results.json  \n",
            " extracting: test-set/natural_language_inference/9/section-line-nums.txt  \n",
            " extracting: test-set/natural_language_inference/9/sentences.txt  \n",
            "   creating: test-set/natural_language_inference/9/triples/\n",
            "  inflating: test-set/natural_language_inference/9/triples/approach.txt  \n",
            "  inflating: test-set/natural_language_inference/9/triples/experimental-setup.txt  \n",
            "  inflating: test-set/natural_language_inference/9/triples/research-problem.txt  \n",
            "  inflating: test-set/natural_language_inference/9/triples/results.txt  \n",
            "  inflating: test-set/README.md      \n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/record/4737071/files/test-set.zip\n",
        "!unzip test-set.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoHg4_uL2q2Z"
      },
      "outputs": [],
      "source": [
        "test_data_dir = '/content/test-set'\n",
        "test_articles, test_contributions = load_articles(test_data_dir)\n",
        "test_sentences, test_labels = article2sentence_and_labels(test_articles, test_contributions)\n",
        "\n",
        "test_encodings = tokenizer(test_sentences, truncation=True, padding=True, return_tensors='pt')\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels))\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMWaBNac2tuh",
        "outputId": "30809dd9-209e-4341-f32f-573d1d5496a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.469, Test Acc: 0.898\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "test_loss, test_acc = 0, 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {'input_ids': batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels': batch[2]}\n",
        "        outputs = model(**inputs)\n",
        "        loss = loss_fn(outputs.logits, inputs['labels'])\n",
        "        test_loss += loss.item()\n",
        "        test_acc += (outputs.logits.argmax(dim=1) == inputs['labels']).float().mean().item()\n",
        "\n",
        "test_loss /= len(test_dataloader)\n",
        "test_acc /= len(test_dataloader)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHW-mOFS1xbB"
      },
      "source": [
        "### **Save the Trained model weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAApPEZBeq0x"
      },
      "outputs": [],
      "source": [
        "!mkdir my_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jgvvcdg7lmo"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('my_model')\n",
        "tokenizer.save_pretrained('my_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Qj-7KH15Tv"
      },
      "source": [
        "### **Push the model to hugging face Repo**\n",
        "\n",
        "Model Instance URL - https://huggingface.co/GouthamVicky/ContributionSentClassification-bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "V6tOhvJA3U_i"
      },
      "outputs": [],
      "source": [
        "# Set up the Hugging Face API token\n",
        "os.environ['HUGGINGFACE_TOKEN'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "553099f872884c119be3d78d838b7e22",
            "4990e0c66dbc42de8542999c5dcb3e96",
            "354f3ef25169425598fac448ce4ef862",
            "830d8dbff63549919e7ff9943755756b",
            "3a8f7eeb38264d9db4513d2020e3cba3",
            "6d98c6e984514268bffc97f3da553df5",
            "be88d4bd2dd84b2494c86a1bcafed052",
            "86b0afc5e8344e2792b8c041219acb42",
            "d60d4b1ef5b74b0eaf4ecc87c4b9f7fa",
            "b2a479e9179b441c9bb25fe4e795b014",
            "d237c214399143bb92473ca4c3348ab8",
            "e6ff6d5e134a4d939d3b36b6430b50ac",
            "c79eb2b0defb44dfbe9781f9e68f8f7d",
            "c7abd4f20c574d3a90423963b503756e",
            "89b88db0ee76412ab19616e1530c49b9",
            "bf50fa68a0164e5f96a2b63b19793da9",
            "b7ff5f4d3c9d44c5a2585a968be15723",
            "5e3e801d13434d56a16707e741990902",
            "466ec1de0f4945be969d2dc6fdfdeb6f",
            "7204d29fe3c74097ba141968de856f11",
            "0028ec35398a454eb6d8fc04b50d2fba",
            "d947a546903b4aa5bcf564f03baf1073"
          ]
        },
        "id": "U8LAelod8Fg7",
        "outputId": "b75ad761-02bc-49a8-ff69-1f8ef48fae85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /tmp/tmp6e0cvyot/config.json\n",
            "Model weights saved in /tmp/tmp6e0cvyot/pytorch_model.bin\n",
            "Uploading the following files to Goutham-Vignesh/ContributionSentClassification-scibert: pytorch_model.bin,config.json\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "553099f872884c119be3d78d838b7e22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6ff6d5e134a4d939d3b36b6430b50ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer config file saved in /tmp/tmpmni0s0tq/tokenizer_config.json\n",
            "Special tokens file saved in /tmp/tmpmni0s0tq/special_tokens_map.json\n",
            "Uploading the following files to Goutham-Vignesh/ContributionSentClassification-scibert: vocab.txt,tokenizer_config.json,special_tokens_map.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Goutham-Vignesh/ContributionSentClassification-scibert/commit/cb78e8a79cb9844e62604285f6c5d4bf7a79b8fa', commit_message='Upload tokenizer', commit_description='', oid='cb78e8a79cb9844e62604285f6c5d4bf7a79b8fa', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Push the model and tokenizer to the model hub\n",
        "model_name = f'Goutham-Vignesh/ContributionSentClassification-scibert'\n",
        "model.push_to_hub(model_name, use_auth_token=os.getenv('HUGGINGFACE_TOKEN'))\n",
        "tokenizer.push_to_hub(model_name, use_auth_token=os.getenv('HUGGINGFACE_TOKEN'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0028ec35398a454eb6d8fc04b50d2fba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02b34604f7924972a6e103e888f5f5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6710d8fe39884237a23ace2a38e27996",
            "placeholder": "​",
            "style": "IPY_MODEL_bec2af17e55d4282b680843c1c0beda4",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "02d49eb81ff34a3ba425b57a9243eb83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "086146de732f4c68b5bae1b85f512de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f165db17d19d4030b42f17f40bb85ddc",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7aeac43c13343d49ac758cb2a6c69f8",
            "value": 385
          }
        },
        "1341181052444a6b8dde547abdf5993e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72e98287f02a41609d2ad822e898211d",
            "placeholder": "​",
            "style": "IPY_MODEL_e26b82242fad4bfd8614f89f25d50ab9",
            "value": " 228k/228k [00:00&lt;00:00, 2.69MB/s]"
          }
        },
        "13ec423978a644c78e78d7efe93b212e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb9991352a964efc86d1b9e82d86052a",
            "placeholder": "​",
            "style": "IPY_MODEL_33cf417a7ed448799e18d52717fa0472",
            "value": " 385/385 [00:00&lt;00:00, 18.9kB/s]"
          }
        },
        "156c9ed92752450fb5efec14aec65e32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24c774030bdb4cc9973e26f98c36f8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1f627608064424cac91992d671dee5e",
            "placeholder": "​",
            "style": "IPY_MODEL_4f629a11388e4d05a5f5a4191d1d64c9",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "33cf417a7ed448799e18d52717fa0472": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "354f3ef25169425598fac448ce4ef862": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86b0afc5e8344e2792b8c041219acb42",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d60d4b1ef5b74b0eaf4ecc87c4b9f7fa",
            "value": 1
          }
        },
        "3a8f7eeb38264d9db4513d2020e3cba3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c27a7757a2f4200ab5320e3b3f9705b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e1d2052d5040ea8bece28a9bd07bfc",
            "placeholder": "​",
            "style": "IPY_MODEL_65a66a19e24c4f34b6a719bc49478920",
            "value": " 442M/442M [00:02&lt;00:00, 202MB/s]"
          }
        },
        "466ec1de0f4945be969d2dc6fdfdeb6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47599f6779c8449884b5f93699c58aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24c774030bdb4cc9973e26f98c36f8e4",
              "IPY_MODEL_6c6eb4efced04678b51f736e56bfe831",
              "IPY_MODEL_1341181052444a6b8dde547abdf5993e"
            ],
            "layout": "IPY_MODEL_eadbf234d36c4b248b555ef8791f5ce2"
          }
        },
        "4990e0c66dbc42de8542999c5dcb3e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d98c6e984514268bffc97f3da553df5",
            "placeholder": "​",
            "style": "IPY_MODEL_be88d4bd2dd84b2494c86a1bcafed052",
            "value": "Upload 1 LFS files: 100%"
          }
        },
        "4f629a11388e4d05a5f5a4191d1d64c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52051f3ca8884b268159aa57dff99d55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "553099f872884c119be3d78d838b7e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4990e0c66dbc42de8542999c5dcb3e96",
              "IPY_MODEL_354f3ef25169425598fac448ce4ef862",
              "IPY_MODEL_830d8dbff63549919e7ff9943755756b"
            ],
            "layout": "IPY_MODEL_3a8f7eeb38264d9db4513d2020e3cba3"
          }
        },
        "5a42eb0b72084b5588cdc457ed01b760": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e3e801d13434d56a16707e741990902": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f3c612076744795a6fc71778f02f10d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a66a19e24c4f34b6a719bc49478920": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6710d8fe39884237a23ace2a38e27996": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69ff2732322b44b1a86c7b3e1b94a4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efd2da662e6a43fcb7772d7202294b2f",
              "IPY_MODEL_086146de732f4c68b5bae1b85f512de8",
              "IPY_MODEL_13ec423978a644c78e78d7efe93b212e"
            ],
            "layout": "IPY_MODEL_02d49eb81ff34a3ba425b57a9243eb83"
          }
        },
        "6c6eb4efced04678b51f736e56bfe831": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f3c612076744795a6fc71778f02f10d",
            "max": 227845,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa5737d0c18f41a49fd7bf15cfdc903a",
            "value": 227845
          }
        },
        "6d98c6e984514268bffc97f3da553df5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7204d29fe3c74097ba141968de856f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72e98287f02a41609d2ad822e898211d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "830d8dbff63549919e7ff9943755756b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2a479e9179b441c9bb25fe4e795b014",
            "placeholder": "​",
            "style": "IPY_MODEL_d237c214399143bb92473ca4c3348ab8",
            "value": " 1/1 [00:05&lt;00:00,  5.26s/it]"
          }
        },
        "8354e4e7c82a479e82d75a0d111677ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86b0afc5e8344e2792b8c041219acb42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b88db0ee76412ab19616e1530c49b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0028ec35398a454eb6d8fc04b50d2fba",
            "placeholder": "​",
            "style": "IPY_MODEL_d947a546903b4aa5bcf564f03baf1073",
            "value": " 440M/440M [00:05&lt;00:00, 95.6MB/s]"
          }
        },
        "aa5737d0c18f41a49fd7bf15cfdc903a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1f627608064424cac91992d671dee5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2a479e9179b441c9bb25fe4e795b014": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7ff5f4d3c9d44c5a2585a968be15723": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9991352a964efc86d1b9e82d86052a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be88d4bd2dd84b2494c86a1bcafed052": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bec2af17e55d4282b680843c1c0beda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf50fa68a0164e5f96a2b63b19793da9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c79eb2b0defb44dfbe9781f9e68f8f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7ff5f4d3c9d44c5a2585a968be15723",
            "placeholder": "​",
            "style": "IPY_MODEL_5e3e801d13434d56a16707e741990902",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "c7abd4f20c574d3a90423963b503756e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_466ec1de0f4945be969d2dc6fdfdeb6f",
            "max": 439752821,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7204d29fe3c74097ba141968de856f11",
            "value": 439752821
          }
        },
        "cb491e68d4ca4c739b94c96264a0e83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb7a0c92375242ffac709cfcfafff90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_156c9ed92752450fb5efec14aec65e32",
            "max": 442221694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8354e4e7c82a479e82d75a0d111677ab",
            "value": 442221694
          }
        },
        "d237c214399143bb92473ca4c3348ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d60d4b1ef5b74b0eaf4ecc87c4b9f7fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d947a546903b4aa5bcf564f03baf1073": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9e1d2052d5040ea8bece28a9bd07bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e26b82242fad4bfd8614f89f25d50ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6ff6d5e134a4d939d3b36b6430b50ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c79eb2b0defb44dfbe9781f9e68f8f7d",
              "IPY_MODEL_c7abd4f20c574d3a90423963b503756e",
              "IPY_MODEL_89b88db0ee76412ab19616e1530c49b9"
            ],
            "layout": "IPY_MODEL_bf50fa68a0164e5f96a2b63b19793da9"
          }
        },
        "e7aeac43c13343d49ac758cb2a6c69f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eadbf234d36c4b248b555ef8791f5ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaece8403cf341d2818deb9fa15f3121": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02b34604f7924972a6e103e888f5f5ff",
              "IPY_MODEL_cb7a0c92375242ffac709cfcfafff90c",
              "IPY_MODEL_3c27a7757a2f4200ab5320e3b3f9705b"
            ],
            "layout": "IPY_MODEL_52051f3ca8884b268159aa57dff99d55"
          }
        },
        "efd2da662e6a43fcb7772d7202294b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a42eb0b72084b5588cdc457ed01b760",
            "placeholder": "​",
            "style": "IPY_MODEL_cb491e68d4ca4c739b94c96264a0e83b",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "f165db17d19d4030b42f17f40bb85ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
